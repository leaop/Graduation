{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aula 15 - RNNs, LSTM e GRU (Continuação) - GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrutura das GRUs\n",
    "\n",
    "### Introdução às GRUs\n",
    "\n",
    "#### Conceito de Gated Recurrent Units (GRU) - Unidades recorrentes fechadas\n",
    "Gated Recurrent Units (GRUs) são um tipo de rede neural recorrente (RNN) projetada para resolver problemas comuns encontrados em RNNs tradicionais, como o desvanecimento e a explosão de gradientes. As GRUs utilizam mecanismos de gating (portões) para controlar o fluxo de informações através das células recorrentes, permitindo que a rede armazene e transporte informações ao longo de sequências temporais de forma mais eficiente.\n",
    "\n",
    "#### Histórico e desenvolvimento das GRUs\n",
    "As GRUs foram introduzidas por Kyunghyun Cho et al. em 2014 como uma alternativa simplificada às redes Long Short-Term Memory (LSTM). O principal objetivo era criar uma estrutura de rede recorrente que fosse menos complexa e mais rápida de treinar, mantendo, ao mesmo tempo, a capacidade de capturar dependências temporais de longo prazo. Desde sua introdução, as GRUs têm sido amplamente adotadas em várias aplicações de processamento de linguagem natural (PLN), reconhecimento de fala e previsão de séries temporais devido à sua eficiência e desempenho.\n",
    "\n",
    "### Componentes das GRUs\n",
    "\n",
    "#### Gates nas GRUs\n",
    "As GRUs utilizam dois tipos principais de gates para controlar o fluxo de informações: o Update Gate e o Reset Gate. Esses gates são essenciais para o funcionamento eficiente das GRUs, permitindo que a rede aprenda dependências temporais de longo prazo de maneira mais eficaz.\n",
    "\n",
    "##### Update Gate\n",
    "- **Função:** O Update Gate controla a quantidade de informação do estado anterior que deve ser transportada para o próximo estado.\n",
    "- **Importância:** Este gate determina quais informações são importantes o suficiente para serem mantidas ao longo das etapas de tempo, ajudando a mitigar o problema do desvanecimento de gradientes.\n",
    "\n",
    "##### Reset Gate\n",
    "- **Função:** O Reset Gate controla a quantidade de informação do estado anterior que deve ser esquecida.\n",
    "- **Importância:** Este gate permite que a rede decida quais informações antigas não são mais relevantes e devem ser descartadas, focando assim nas novas entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "\n",
    "# Definição do modelo GRU\n",
    "model = Sequential([\n",
    "    # Primeira camada GRU\n",
    "    # units: número de unidades (neurônios) na camada GRU\n",
    "    # return_sequences: se True, retorna a sequência completa de saídas\n",
    "    # input_shape: formato da entrada (timesteps, features)\n",
    "    # activation: função de ativação para as saídas\n",
    "    GRU(units=50, return_sequences=True, input_shape=(None, 1), activation='tanh'),\n",
    "\n",
    "    # Segunda camada GRU\n",
    "    # units: número de unidades (neurônios) na camada GRU\n",
    "    # return_sequences: se False, retorna apenas a última saída\n",
    "    GRU(units=50, return_sequences=False, activation='tanh'),\n",
    "\n",
    "    # Camada densa\n",
    "    # units: número de unidades na camada (aqui é 1, indicando saída unidimensional)\n",
    "    Dense(units=1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compilação do modelo\n",
    "# optimizer: algoritmo de otimização (Adam é um dos mais usados)\n",
    "# loss: função de perda (MSE é comum para problemas de regressão)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Resumo do modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcionamento das GRUs\n",
    "\n",
    "#### Cálculo do Update Gate (z)\n",
    "- **Fórmula Matemática:** \n",
    " \n",
    "  $( z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t]) )$\n",
    "- **Explicação dos Pesos e Ativações:**\n",
    "  - $( W_z )$ representa os pesos associados ao Update Gate.\n",
    "  - $( h_{t-1} )$ é o estado oculto anterior.\n",
    "  - $( x_t )$ é a entrada atual.\n",
    "  - $( \\sigma )$ é a função sigmoide que restringe os valores entre 0 e 1.\n",
    "  - O Update Gate decide a quantidade de informação anterior (\\( h_{t-1} \\)) a ser transportada para o próximo estado.\n",
    "\n",
    "#### Cálculo do Reset Gate (r)\n",
    "- **Fórmula Matemática:** \n",
    "  $( r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t]) )$\n",
    "- **Explicação dos Pesos e Ativações:**\n",
    "  - $( W_r )$ representa os pesos associados ao Reset Gate.\n",
    "  - $( h_{t-1} )$ é o estado oculto anterior.\n",
    "  - $( x_t )$ é a entrada atual.\n",
    "  - $( \\sigma )$ é a função sigmoide que restringe os valores entre 0 e 1.\n",
    "  - O Reset Gate decide a quantidade de informação anterior ($( h_{t-1} )$) a ser esquecida.\n",
    "\n",
    "#### Cálculo do Novo Estado do Candidato ($( \\tilde{h} )$)\n",
    "- **Fórmula Matemática:** \n",
    "  $( \\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t]) )$\n",
    "- **Explicação dos Pesos e Ativações:**\n",
    "  - $( W )$ representa os pesos associados ao novo estado do candidato.\n",
    "  - $( r_t )$ é o valor do Reset Gate.\n",
    "  - $( h_{t-1} )$ é o estado oculto anterior.\n",
    "  - $( x_t )$ é a entrada atual.\n",
    "  - $( \\odot )$ representa a multiplicação elemento a elemento (Hadamard product).\n",
    "  - $( \\tanh )$ é a função tangente hiperbólica que restringe os valores entre -1 e 1.\n",
    "  - O novo estado do candidato ($( \\tilde{h}_t )$) é calculado considerando a informação relevante filtrada pelo Reset Gate.\n",
    "\n",
    "#### Cálculo do Novo Estado Oculto (h)\n",
    "- **Fórmula Matemática:** \n",
    "  $( h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t )$\n",
    "- **Explicação da Combinação Linear de Estados:**\n",
    "  - $( z_t )$ é o valor do Update Gate.\n",
    "  - $( h_{t-1} )$ é o estado oculto anterior.\n",
    "  - $( \\tilde{h}_t )$ é o novo estado do candidato.\n",
    "  - $( \\odot )$ representa a multiplicação elemento a elemento (Hadamard product).\n",
    "  - O novo estado oculto ($( h_t )$) é uma combinação linear do estado anterior e do novo estado do candidato, ponderada pelos valores do Update Gate. Esta combinação permite que a rede decida quanta informação do passado deve ser transportada para o futuro.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Microsoft_Stock.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte a coluna Date para o tipo de dados de data\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Ordena os dados em ordem crescente pelos dado\n",
    "df = df.sort_values(by='Date')\n",
    "\n",
    "# Exclui a coluna Date e deixa a classe alvo como última coluna\n",
    "df = df[['Open', 'High', 'Low', 'Volume', 'Close']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_lookback_data(df, lookback):\n",
    "    \"\"\"\n",
    "    Função para criar dados de entrada e saída para uma LSTM usando uma janela de lookback.\n",
    "    \n",
    "    Parâmetros:\n",
    "    df (DataFrame): DataFrame contendo os dados. A última coluna é a coluna alvo.\n",
    "    lookback (int): Número de dias para a janela de lookback.\n",
    "    \n",
    "    Retorna:\n",
    "    X (numpy array): Dados de entrada para a LSTM, de formato (n_amostras, lookback, n_features).\n",
    "    y (numpy array): Dados de saída/targets, de formato (n_amostras,).\n",
    "    \"\"\"\n",
    "    data = df.values\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(lookback, len(data)):\n",
    "        X.append(data[i - lookback:i, :])  # Todas as colunas exceto a última (características)\n",
    "        y.append(data[i, -1])               # Apenas a última coluna (alvo)\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 20\n",
    "X, y = create_lookback_data(df, lookback)\n",
    "\n",
    "print(f\"Formato de X: {X.shape}\")\n",
    "print(f\"Formato de y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados de X.shape informam que X tem 1491 registros, dos quais cada um possui 20 linhas e 5 colunas (3 dimensões)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar os dados em treinamento e teste\n",
    "X_train = X[:int(len(X) * 0.8)]\n",
    "X_test = X[int(len(X) * 0.8):]\n",
    "\n",
    "y_train = y[:int(len(y) * 0.8)]\n",
    "y_test = y[int(len(y) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informa que tem 23840 registros com cada registro 5 colunas (valores) - 2 dimensões\n",
    "X_train.reshape(-1, X_train.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1192 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reshape(-1, X_train.shape[-1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalizando os dados de treinamento\n",
    "scaler_X = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler_y = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = scaler_X.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando a forma de X_train para duas dimensões antes da normalização\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "print(X_train_reshaped.shape)\n",
    "X_train_reshaped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza os valores\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_reshaped)\n",
    "print(X_train_scaled.shape)\n",
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando a forma de volta para três dimensões após a normalização\n",
    "X_train = X_train_scaled.reshape(X_train.shape)\n",
    "print(X_train.shape)\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler_y.fit_transform(y_train.reshape(-1, 1))\n",
    "\n",
    "# Normalizando os dados de teste\n",
    "X_test = scaler_X.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "y_test = scaler_y.transform(y_test.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação GRU vs. LSTM\n",
    "\n",
    "### Estrutura das LSTM\n",
    "\n",
    "#### Componentes das LSTM\n",
    "As LSTM são redes neurais recorrentes projetadas para modelar dependências de longo prazo, superando limitações de RNNs tradicionais.\n",
    "\n",
    "##### Input Gate\n",
    "- **Função:** Controla a quantidade de nova informação da entrada atual ($x_t$) a ser armazenada no estado da célula.\n",
    "- **Composição:**\n",
    "  - Pesos associados à entrada atual ($x_t$) e ao estado oculto anterior ($h_{t-1}$).\n",
    "  - Função de ativação sigmoide ($\\sigma$) que gera valores entre 0 e 1.\n",
    "- **Fórmula Matemática:**\n",
    "  - $i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$\n",
    "\n",
    "##### Forget Gate\n",
    "- **Função:** Controla a quantidade de informação anterior a ser esquecida do estado da célula.\n",
    "- **Composição:**\n",
    "  - Pesos associados à entrada atual ($x_t$) e ao estado oculto anterior ($h_{t-1}$).\n",
    "  - Função de ativação sigmoide ($\\sigma$) que gera valores entre 0 e 1.\n",
    "- **Fórmula Matemática:**\n",
    "  - $f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$\n",
    "\n",
    "##### Output Gate\n",
    "- **Função:** Controla a quantidade de informação do estado da célula a ser transferida para o estado oculto atual ($h_t$).\n",
    "- **Composição:**\n",
    "  - Pesos associados à entrada atual ($x_t$) e ao estado oculto anterior ($h_{t-1}$).\n",
    "  - Função de ativação sigmoide ($\\sigma$) que gera valores entre 0 e 1.\n",
    "  - Função tangente hiperbólica ($\\tanh$) que regula a amplitude da saída.\n",
    "- **Fórmula Matemática:**\n",
    "  - $o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$\n",
    "  - $h_t = o_t \\odot \\tanh(C_t)$\n",
    "\n",
    "##### Cell State\n",
    "- **Função:** Armazena informações a longo prazo, modificadas pelas gates de entrada e de esquecimento.\n",
    "- **Composição:**\n",
    "  - Combinação linear do estado anterior da célula e da nova informação ajustada pelas gates.\n",
    "- **Fórmula Matemática:**\n",
    "  - $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$\n",
    "  - $\\tilde{C}_t$ é o novo estado candidato, calculado como $ \\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$\n",
    "\n",
    "### Diferenças chave entre GRUs e LSTMs\n",
    "\n",
    "#### Número de Gates\n",
    "- **LSTMs:**\n",
    "  - **Input Gate:** Controla a quantidade de nova informação a ser armazenada no estado da célula.\n",
    "  - **Forget Gate:** Controla a quantidade de informação anterior a ser esquecida do estado da célula.\n",
    "  - **Output Gate:** Controla a quantidade de informação do estado da célula a ser transferida para o estado oculto atual.\n",
    "- **GRUs:**\n",
    "  - **Update Gate:** Controla a quantidade de informação anterior a ser transportada para a próxima etapa.\n",
    "  - **Reset Gate:** Controla a quantidade de informação anterior a ser esquecida.\n",
    "\n",
    "As LSTMs possuem três gates principais (input, forget, output), enquanto as GRUs possuem dois gates principais (update, reset). Esta diferença resulta em uma arquitetura mais simples para as GRUs.\n",
    "\n",
    "#### Complexidade Computacional\n",
    "- **LSTMs:**\n",
    "  - Devido à presença de três gates, as LSTMs são mais complexas e exigem mais cálculos em cada etapa de tempo.\n",
    "  - A complexidade adicional das LSTMs pode resultar em tempos de treinamento mais longos e maior uso de recursos computacionais.\n",
    "- **GRUs:**\n",
    "  - Com apenas dois gates, as GRUs são menos complexas e realizam menos cálculos em cada etapa de tempo.\n",
    "  - A arquitetura mais simples das GRUs permite treinos mais rápidos e uso eficiente dos recursos computacionais.\n",
    "\n",
    "As GRUs tendem a ser mais rápidas e menos complexas devido ao menor número de gates, tornando-as uma escolha eficiente para muitas aplicações práticas.\n",
    "\n",
    "#### Eficiência de Treinamento\n",
    "- **LSTMs:**\n",
    "  - Requerem uma quantidade maior de dados de treinamento para aprender eficazmente, devido à sua complexidade.\n",
    "  - A presença de múltiplos gates pode tornar o ajuste de hiperparâmetros mais desafiador.\n",
    "- **GRUs:**\n",
    "  - Geralmente requerem menos dados de treinamento para alcançar um desempenho comparável, devido à sua estrutura mais simples.\n",
    "  - A menor complexidade das GRUs facilita o processo de treinamento e ajuste de hiperparâmetros.\n",
    "\n",
    "GRUs geralmente requerem menos dados para alcançar desempenho comparável, facilitando o treinamento em cenários com dados limitados.\n",
    "\n",
    "#### Efetividade\n",
    "- **Desempenho em Tarefas Específicas:**\n",
    "  - **LSTMs:** Excelentes em tarefas que exigem a captura de dependências de longo prazo, como tradução automática e geração de texto.\n",
    "  - **GRUs:** Desempenham de forma similar às LSTMs em muitas tarefas, mas com vantagens em termos de velocidade e eficiência em cenários onde os dados são menos complexos ou de menor volume.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, LSTM, GRU, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Definindo o modelo RNN\n",
    "model_rnn = Sequential([\n",
    "    SimpleRNN(50, return_sequences=True, input_shape=(lookback, X_train.shape[-1]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    SimpleRNN(50, return_sequences=False, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model_rnn.compile(optimizer='adam', loss='mse')\n",
    "history_rnn = model_rnn.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Definindo o modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(lookback, X_train.shape[-1]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Definindo o modelo GRU\n",
    "model_gru = Sequential([\n",
    "    GRU(50, return_sequences=True, input_shape=(lookback, X_train.shape[-1]), activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    GRU(50, return_sequences=False, activation='tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "model_gru.compile(optimizer='adam', loss='mse')\n",
    "history_gru = model_gru.fit(X_train, y_train, epochs=50, validation_split=0.1, batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Perdas do modelo RNN\n",
    "plt.plot(history_rnn.history['loss'], label='Train Loss RNN')\n",
    "plt.plot(history_rnn.history['val_loss'], label='Val Loss RNN')\n",
    "\n",
    "# Perdas do modelo LSTM\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss LSTM')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Val Loss LSTM')\n",
    "\n",
    "# Perdas do modelo GRU\n",
    "plt.plot(history_gru.history['loss'], label='Train Loss GRU')\n",
    "plt.plot(history_gru.history['val_loss'], label='Val Loss GRU')\n",
    "\n",
    "plt.title('Training and Validation Loss for RNN, LSTM, and GRU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para calcular e imprimir métricas\n",
    "def evaluate_model(model, X_test, y_test, scaler_y):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_inverse = scaler_y.inverse_transform(y_pred)\n",
    "    y_test_inverse = scaler_y.inverse_transform(y_test)\n",
    "    mse = mean_squared_error(y_test_inverse, y_pred_inverse)\n",
    "    mae = mean_absolute_error(y_test_inverse, y_pred_inverse)\n",
    "    r2 = r2_score(y_test_inverse, y_pred_inverse)\n",
    "    return mse, mae, r2, y_test_inverse, y_pred_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo RNN\n",
    "mse_rnn, mae_rnn, r2_rnn, y_test_rnn, y_pred_rnn = evaluate_model(model_rnn, X_test, y_test, scaler_y)\n",
    "\n",
    "# Avaliando o modelo LSTM\n",
    "mse_lstm, mae_lstm, r2_lstm, y_test_lstm, y_pred_lstm = evaluate_model(model_lstm, X_test, y_test, scaler_y)\n",
    "\n",
    "# Avaliando o modelo GRU\n",
    "mse_gru, mae_gru, r2_gru, y_test_gru, y_pred_gru = evaluate_model(model_gru, X_test, y_test, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RNN - MSE: {mse_rnn}, MAE: {mae_rnn}, R2: {r2_rnn}')\n",
    "print(f'LSTM - MSE: {mse_lstm}, MAE: {mae_lstm}, R2: {r2_lstm}')\n",
    "print(f'GRU - MSE: {mse_gru}, MAE: {mae_gru}, R2: {r2_gru}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretação do Coeficiente de Determinação (R²)\n",
    "\n",
    "- **R² = 1**: O modelo explica perfeitamente a variação nos dados.\n",
    "- **R² entre 0 e 1**: O modelo explica parcialmente a variação nos dados.\n",
    "- **R² = 0**: O modelo não explica nenhuma variação nos dados.\n",
    "- **R² negativo**: O modelo está pior do que uma linha horizontal (o modelo de média), ou seja, a previsão média teria sido uma melhor estimativa do que o modelo preditivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios em um outro arquivo ;)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
