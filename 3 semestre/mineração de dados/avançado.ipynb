{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NuXUb0sAqvCE"},"outputs":[],"source":["from keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Dropout, Flatten, Dense\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n","from sklearn.model_selection import KFold\n","from keras.regularizers import l2\n","from sklearn.metrics import roc_auc_score\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n","\n","# TensorFlow e tf.keras\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.models import load_model\n","\n","# Bibliotecas Auxiliares\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn.metrics as sm\n","from collections import Counter\n","import gc\n","from IPython.display import clear_output\n","from keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiE3lJIoqvCF"},"outputs":[],"source":["len(Counter(y_train).keys())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VL3TGeJXqvCG"},"outputs":[],"source":["def split_data(X, y, n_splits, random_state=None):\n","    # Define a semente do gerador de números aleatórios\n","    np.random.seed(random_state)\n","\n","    # Cria uma lista de índices embaralhados\n","    indices = np.arange(X.shape[0])\n","    np.random.shuffle(indices)\n","\n","    # Aplica os índices embaralhados a X e y\n","    X = X[indices]\n","    y = y[indices]\n","\n","    # Calcula os tamanhos dos splits\n","    split_sizes = np.full(n_splits, len(X) // n_splits)\n","    split_sizes[:len(X) % n_splits] += 1\n","\n","    # Divide X e y de acordo com os tamanhos dos splits\n","    X_splits = np.split(X, np.cumsum(split_sizes[:-1]))\n","    y_splits = np.split(y, np.cumsum(split_sizes[:-1]))\n","\n","    # Retorna a lista de subconjuntos de X e y\n","    return [[X_i, y_i] for X_i, y_i in zip(X_splits, y_splits)]\n","\n","\n","def create_model(input_shape=(28, 28, 1), qtd_classes=10):\n","    model = Sequential([\n","        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape, kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","        Dropout(0.3),\n","\n","        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","        Dropout(0.5),\n","\n","        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        MaxPooling2D(pool_size=(2, 2)),\n","        Dropout(0.5),\n","\n","        Flatten(),\n","\n","        Dense(512, activation='relu', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        Dropout(0.5),\n","\n","        Dense(256, activation='relu', kernel_regularizer=l2(0.01)),\n","        BatchNormalization(),\n","        Dropout(0.5),\n","\n","        Dense(qtd_classes, activation='softmax')\n","    ])\n","    return model\n","\n","def train_model(model, X_train, y_train, epochs):\n","    model.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","    # Criar modelo\n","\n","    # Definir callbacks\n","    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=5, min_lr=0.001)\n","    early_stopping = EarlyStopping(monitor='val_accuracy', patience=20)\n","\n","    # Treinar modelo\n","    history = model.fit(\n","        X_train, y_train, \n","        epochs=epochs, \n","        validation_split=0.2, \n","        callbacks=[reduce_lr, early_stopping]\n","    )\n","    \n","    return model, history\n","\n","\n","\n","def evaluate_model(history, X_test, y_test, model, class_names):\n","    plt.plot(history.history['accuracy'])\n","    plt.plot(history.history['val_accuracy'])\n","    plt.title('Model accuracy')\n","    plt.ylabel('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper left')\n","    plt.show()\n","\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('Model loss')\n","    plt.ylabel('Loss')\n","    plt.xlabel('Epoch')\n","    plt.legend(['Train', 'Validation'], loc='upper right')\n","    plt.show()\n","\n","    score = model.evaluate(X_test, y_test, verbose=0)\n","    print('Test loss:', score[0])\n","    print('Test accuracy:', score[1])\n","\n","    y_pred = model.predict(X_test)\n","\n","    # Convertendo as previsões de matriz de classe binária para vetor de classificações de classe\n","    y_pred_classes = np.argmax(y_pred, axis=1)\n","\n","    print(classification_report(np.argmax(y_test, axis=1), y_pred_classes, target_names=class_names))\n","\n","    cm = confusion_matrix(np.argmax(y_test, axis=1), y_pred_classes)\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.set(font_scale=1.4)\n","    heatmap = sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n","\n","    heatmap.set_xticklabels(class_names, rotation=45)\n","    heatmap.set_yticklabels(class_names, rotation=45)\n","\n","    plt.title('Confusion Matrix')\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.show()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzk1D43-qvCH"},"outputs":[],"source":["# Baixar a base de dados\n","fashion_mnist = keras.datasets.fashion_mnist\n","(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n","\n","\n","# Convertendo o vetor de classes para matriz de classes binárias\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","# Definimos o nome das classes\n","class_names = ['Camiseta', 'Calça', 'Suéter', 'Vestido', 'Casaco',\n","               'Sandália', 'Camisa', 'Tênis', 'Bolsa', 'Botas']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxOUhp9bqvCH"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rV_otHqgqvCI"},"outputs":[],"source":["X_train.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sASw3YshqvCI"},"outputs":[],"source":["X_y = split_data(X_train, y_train, n_splits = 4, random_state=43)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sAXUcSsqvCJ"},"outputs":[],"source":["len(X_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-a2l6NtWqvCJ"},"outputs":[],"source":["X_y[0][0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7a_HpecqvCK"},"outputs":[],"source":["X_y[0][1].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xLbzoOyZqvCK"},"outputs":[],"source":["X_y[1][0].shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vMrlZNGDqvCK"},"outputs":[],"source":["X, y = X_y[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"478VYD7QqvCK"},"outputs":[],"source":["model = create_model()\n","X, y = X_y[0]\n","model, history = train_model(model, X, y, epochs = 50)\n","gc.collect()\n","clear_output(wait=True)\n","evaluate_model(history, X_test, y_test, model, class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhETR_soqvCK"},"outputs":[],"source":["# Para salvar modelo basta usar:\n","model.save('modelo_v1.h5')"]},{"cell_type":"markdown","metadata":{"id":"F3NTg85VqvCL"},"source":["E se tivermos mais dados para treinar o modelo? ou tivermos que continuar o treinamento após algum tempo?\n","\n","Podemos continuar o treinamento do modelo..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbKDkWpcqvCT"},"outputs":[],"source":["# Realizar a leitura do modelo\n","model = load_model('modelo_v1.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vc5ynQfwqvCU"},"outputs":[],"source":["# Conseguimos utilizar modelo carregado\n","evaluate_model(history, X_test, y_test, model, class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HYVMFjZrqvCU"},"outputs":[],"source":["X, y = X_y[1]\n","model, history = train_model(model, X, y, epochs = 50)\n","gc.collect()\n","clear_output(wait=True)\n","evaluate_model(history, X_test, y_test, model, class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Afw9d1zxqvCU"},"outputs":[],"source":["X, y = X_y[2]\n","model, history = train_model(model, X, y, epochs = 50)\n","gc.collect()\n","clear_output(wait=True)\n","evaluate_model(history, X_test, y_test, model, class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JFJjlF20qvCU"},"outputs":[],"source":["X, y = X_y[3]\n","model, history = train_model(model, X, y, epochs = 50)\n","gc.collect()\n","clear_output(wait=True)\n","evaluate_model(history, X_test, y_test, model, class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bSioun6ZqvCU"},"outputs":[],"source":["# Para salvar modelo basta usar:\n","model.save('modelo_v2.h5')"]}],"metadata":{"kernelspec":{"display_name":"tf","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}