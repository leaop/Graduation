{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leaop/Graduation/blob/main/Classifica%C3%A7%C3%A3o_e_Regress%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOkswhqOxHxf"
      },
      "source": [
        "# Unidade 1 - Tarefas de Mineração de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZF4_sy4xHxh"
      },
      "source": [
        "As tarefas de mineração de dados referem-se ao processo de explorar grandes conjuntos de dados para encontrar padrões, relações e informações úteis. A mineração de dados envolve uma variedade de técnicas, algoritmos e abordagens para identificar informações ocultas e valiosas nos dados, que podem ser utilizadas para tomar decisões informadas em uma variedade de áreas, como negócios, ciência, saúde, governo, entre outros.\n",
        "\n",
        "Algumas das principais tarefas de mineração de dados incluem:\n",
        "1.\t`Classificação`: é a tarefa de classificar objetos em categorias pré-definidas com base em suas características.\n",
        "2.\t`Regressão`: é a tarefa de prever um valor numérico com base em um conjunto de variáveis independentes.\n",
        "3.\t`Clusterização`: é a tarefa de agrupar objetos em grupos ou clusters com base em suas características.\n",
        "4.\t`Associação`: é a tarefa de identificar relações ou associações entre os diferentes atributos ou variáveis do conjunto de dados.\n",
        "5.\t`Análise de anomalias`: é a tarefa de identificar anomalias ou padrões incomuns nos dados, que podem ser indicativos de problemas ou oportunidades.\n",
        "6.\t`Sumarização`: é a tarefa de criar uma representação resumida dos dados para ajudar na sua compreensão e interpretação.\n",
        "7.\t`Previsão de séries temporais`: é a tarefa de prever valores futuros com base em padrões e tendências identificados nos dados históricos.\n",
        "\n",
        "Em resumo, as \"tarefas\" de Mineração de Dados realizam a fase de descoberta de conhecimento em Bases de Dados, em inglês, KDD - Knowledge Discovery in Databases. O KDD é formado por diversas etapas para transformar os dados brutos em informações relevantes (conhecimento útil).\n",
        "\n",
        "### Qual a importância da Classificação e Regressão em mineração de dados?\n",
        "\n",
        "A classificação e regressão são técnicas fundamentais em mineração de dados, que permitem, por exemplo, fazer previsões a partir de dados históricos.\n",
        "\n",
        "A classificação é usada para prever a classe ou categoria de um objeto ou evento com base em atributos conhecidos. Por exemplo, em uma empresa de telecomunicações, a classificação pode ser usada para prever se um cliente irá cancelar ou não seu serviço com base em fatores como tempo de contrato, taxa de uso e histórico de pagamentos. Isso permite que a empresa tome medidas para evitar o cancelamento de clientes, como oferecer promoções ou um serviço de melhor qualidade.\n",
        "\n",
        "Já a regressão é usada para prever um valor numérico a partir de um conjunto de variáveis conhecidas. Por exemplo, em uma empresa de seguros, a regressão pode ser usada para prever o valor de uma apólice com base em fatores como idade, histórico de sinistros e tipo de veículo. Isso ajuda a empresa a determinar o preço da apólice de forma mais precisa e justa.\n",
        "\n",
        "Ambas as técnicas são importantes porque permitem que as empresas tomem decisões mais informadas com base em dados históricos, tornando suas operações mais eficientes e reduzindo custos. Além disso, elas podem ser aplicadas em diversas ajudando as empresas a obter insights valiosos e tomar decisões mais precisas e estratégicas.\n",
        "\n",
        "### Algumas características importantes da classificação e regressão incluem:\n",
        "•   São técnicas de aprendizado supervisionado, ou seja, exigem um conjunto de dados com as classes conhecidas para treinar o modelo.\n",
        "\n",
        "•\tPodem ser aplicadas em diversas áreas, como finanças, saúde, marketing, segurança, entre outras.\n",
        "\n",
        "•\tRequerem o uso de algoritmos de aprendizado de máquina, que podem variar de acordo com o tipo de problema a ser resolvido.\n",
        "\n",
        "•\tPodem ser usadas em conjunto com outras técnicas de mineração de dados, como associação e agrupamento, para obter insights mais abrangentes sobre um conjunto de dados.\n",
        "\n",
        "### Alguns algoritmos e técnicas utilizadas em classificação e regressão\n",
        "Os algoritmos e técnicas utilizados em classificação e regressão podem variar de acordo com o tipo de problema a ser resolvido e as características do conjunto de dados. No entanto, existem algumas técnicas e algoritmos comumente utilizados em ambas as tarefas:\n",
        "\n",
        "1.\t`Árvores de decisão`: consiste em um modelo de árvore que divide o conjunto de dados em subconjuntos menores com base nas características ou atributos dos dados, até chegar a uma decisão final.\n",
        "\n",
        "2.\t`K-NN (K-Nearest Neighbors)`: é um algoritmo que calcula a distância entre cada objeto no conjunto de dados e seus vizinhos mais próximos, para determinar sua classe ou valor previsto.\n",
        "\n",
        "3.\t`Naive Bayes`: é um algoritmo baseado no Teorema de Bayes que utiliza a probabilidade condicional para determinar a classe ou valor previsto de um objeto no conjunto de dados.\n",
        "\n",
        "4.\t`Regressão Linear`: é uma técnica que utiliza uma equação linear para prever o valor numérico de uma variável dependente com base em uma ou mais variáveis independentes.\n",
        "\n",
        "5.\t`Redes Neurais Artificiais`: são modelos inspirados no funcionamento do cérebro humano, que utilizam camadas de neurônios artificiais interconectados para fazer previsões.\n",
        "\n",
        "6.\t`Máquinas de Vetores de Suporte (SVM)`: é um algoritmo que busca encontrar a melhor linha ou superfície de separação entre as classes do conjunto de dados.\n",
        "\n",
        "Esses são apenas alguns exemplos de algoritmos e técnicas utilizados em classificação e regressão na mineração de dados. A escolha do algoritmo mais adequado depende do tipo de problema a ser resolvido, da qualidade e tamanho do conjunto de dados, da complexidade do modelo e de outros fatores específicos do contexto da análise de dados.\n",
        "\n",
        "### Limitações e desafios das tarefas de classificação e regressão em mineração de dados\n",
        "Embora a classificação e regressão sejam técnicas poderosas da mineração de dados, elas também apresentam algumas limitações e desafios. Algumas das principais são:\n",
        "\n",
        "1.\t`Disponibilidade de dados`: as tarefas de classificação e regressão exigem uma quantidade significativa de dados de treinamento para construir modelos confiáveis e precisos. Se os dados disponíveis são limitados ou incompletos, pode ser difícil criar um modelo com alta acurácia.\n",
        "\n",
        "2.\t`Complexidade do modelo`: alguns conjuntos de dados podem ser tão complexos que é difícil encontrar um modelo que possa prever corretamente as classes ou valores alvo. Isso pode levar à criação de modelos excessivamente complexos, que podem ser difíceis de interpretar e entender.\n",
        "\n",
        "3.\t`Overfitting`: é o fenômeno em que o modelo é ajustado em excesso aos dados de treinamento, tornando-o menos preciso ao fazer previsões em dados desconhecidos. Isso pode ocorrer quando o modelo é muito específico para os dados de treinamento e não consegue capturar as variações dos dados novos e desconhecidos.\n",
        "\n",
        "4.\t`Desequilíbrio de classes`: algumas tarefas de classificação podem ter um desequilíbrio nas classes, o que significa que uma classe pode ter muito mais exemplos do que outra. Isso pode levar a um viés do modelo em favor da classe majoritária e a uma baixa precisão na previsão da classe minoritária.\n",
        "\n",
        "5.\t`Interpretabilidade do modelo`: alguns modelos de classificação e regressão podem ser difíceis de interpretar, especialmente quando são baseados em algoritmos complexos, como redes neurais. Isso pode dificultar a identificação das variáveis mais importantes ou a explicação dos resultados para pessoas mais leigas na área.\n",
        "\n",
        "Essas são apenas algumas das limitações e desafios que as tarefas de classificação e regressão podem enfrentar na mineração de dados. É importante levar em consideração esses aspectos ao selecionar a técnica e o algoritmo adequados para cada problema de análise de dados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHlCAfVgxHxj"
      },
      "source": [
        "# Classificação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz06JHrJxHxj"
      },
      "source": [
        "\n",
        "A classificação é uma das tarefas fundamentais na área de Mineração de Dados, que consiste em treinar um modelo a partir de um conjunto de dados rotulados (que já conhecemos as informações que os dados possuem), de forma que esse modelo possa ser usado para classificar novos dados não rotulados em categorias pré-definidas.\n",
        "\n",
        "Em outras palavras, a classificação é um processo de atribuição de rótulos ou classes a um conjunto de dados com base em seus atributos ou características. O objetivo é identificar um modelo ou padrão que possa ser usado para prever a classe de novos dados com base nas informações contidas nos dados de treinamento.\n",
        "\n",
        "Existem vários algoritmos de classificação, incluindo árvores de decisão, redes neurais, regressão logística, SVM (máquinas de vetor de suporte), Naive Bayes e K-Nearest Neighbors (KNN). Cada um desses algoritmos tem suas próprias vantagens e desvantagens e são mais adequados para diferentes tipos de problemas de classificação.\n",
        "\n",
        "### Exemplo de aplicação de Classificação:\n",
        "Um exemplo de classificação em Tarefas de Mineração de Dados pode ser a classificação de e-mails em spam e não-spam. O objetivo seria treinar um modelo que pudesse classificar novos e-mails em spam ou não-spam com base em suas características, como palavras-chave, remetente, assunto, quantidade de caracteres, etc.\n",
        "\n",
        "O conjunto de dados para este exemplo seria composto por uma lista de e-mails já classificados manualmente como spam ou não-spam. O modelo seria treinado usando algoritmos de aprendizado de máquina supervisionados, onde as características dos e-mails seriam usadas como variáveis independentes e a classe de spam ou não-spam seria a variável dependente.\n",
        "\n",
        "Após o treinamento, o modelo seria testado com novos e-mails, validado e, caso o modelo atinja uma elevada acurácia para prever se um e-mail é spam ou não, este modelo de classificação automática de e-mails poderia ajudar a economizar tempo e aumentar a eficiência de gerenciamento de e-mails.\n",
        "\n",
        "### Outro exemplo de aplicação de Classificação:\n",
        "A classificação de imagens em diferentes categorias. Por exemplo, suponha que temos um conjunto de imagens de animais e queremos classificá-las em categorias como cães, gatos, pássaros, entre outros.\n",
        "\n",
        "O conjunto de dados seria composto por imagens rotuladas de diferentes animais. O modelo seria treinado usando algoritmos de aprendizado de máquina supervisionados, onde as características das imagens (como a cor, a forma, o tamanho, entre outros) seriam usadas como variáveis independentes e a categoria do animal seria a variável dependente.\n",
        "\n",
        "Após o treinamento, teste e validação do modelo, poderíamos classificar novas imagens de animais em suas respectivas categorias com alta precisão. \n",
        "\n",
        "Este tipo de classificação automática de imagens pode ser útil em áreas como reconhecimento de objetos, análise de imagens médicas, vigilância e muitas outras. Atualmente, o estado da arte na visão computacional é o algoritmo YOLO (You Only Look Once), que possuí uma arquitetura de rede neural CNN (Convolutional Neural Netword). \n",
        "\n",
        "Caso se interesse por esta área de Visão Computacional, recomendo a leitura do artigo disponível em https://pjreddie.com/media/files/papers/yolo_1.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LvLkSMKxHxj"
      },
      "source": [
        "Iremos realizar a classificação de roupas com o TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HZKLKgUxHxj"
      },
      "outputs": [],
      "source": [
        "!pip install "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlCE6ki6xHxk"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKBJ96EoxHxk"
      },
      "outputs": [],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCCdSp5OxHxk"
      },
      "outputs": [],
      "source": [
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IilQrusgxHxl"
      },
      "outputs": [],
      "source": [
        "!pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BC4y5DkxHxl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsB4ME9JxHxl"
      },
      "outputs": [],
      "source": [
        "# TensorFlow e tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Bibliotecas Auxiliares\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as sm\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsDQDSNWxHxl"
      },
      "source": [
        "A base de dados que iremos utilizar esta disponível no próprio keras como um dataset de exemplo.\n",
        "\n",
        "O dataset é composto por 70.000 pequenas imagens de 28x28 pixels. Cada imagem é representada por um array numpy (uma matriz) de 28 linhas e 28 colunas, sendo cada pixel da imagem uma tupla da matriz.\n",
        "\n",
        "Na base de dados, cada imagem representa uma peça de roupa, mas para a maioria dos algoritmos de Machine Learning, não podemos usar textos, devendo assim, usar um valor numérico para representar cada classe de roupa. Desta forma, cada valor na base de dados possuí a seguinte representação: \n",
        "- 0 = Camisetas/Top (T-shirt/top)\n",
        "- 1 = Calça (Trouser)\n",
        "- 2 = Suéter (Pullover)\n",
        "- 3 = Vestidos (Dress)\n",
        "- 4 = Casaco (Coat)\n",
        "- 5 = Sandálias (Sandal)\n",
        "- 6 = Camisas (Shirt)\n",
        "- 7 = Tênis (Sneaker)\n",
        "- 8 = Bolsa (Bag)\n",
        "- 9 = Botas (Ankle boot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7ydwbzRxHxl"
      },
      "outputs": [],
      "source": [
        "# Baixar a base de dados\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# Definimos o nome das classes\n",
        "class_names = ['Camiseta', 'Calça', 'Suéter', 'Vestido', 'Casaco',\n",
        "               'Sandália', 'Camisa', 'Tênis', 'Bolsa', 'Botas']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAsYUcUXxHxl"
      },
      "source": [
        "As imagens (train_images e train_images) são as matrizes das imagens e as labels (train_labels e test_labels) são as classes de cada imagem.\n",
        "\n",
        "Breve exploração da base de dados\n",
        "\n",
        "Com o comando `shape` é possível ver qual o formato da base de dados.\n",
        "No caso da base de dados que iremos utilizar, o comando shape irá retornar 3 valores, respectivamente:\n",
        "- Quantidade de registros (no caso, quantidade de imagens)\n",
        "- Quantidade de Linhas de cada registro\n",
        "- Quantidade de \"colunas\" de cada registro\n",
        "\n",
        "Abaixo temos um array com 2 registros, sendo cada registro com apenas 1 linha e 3 colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrDxvmZ8xHxm"
      },
      "outputs": [],
      "source": [
        "np.array([\n",
        "    [['a','b','c']],\n",
        "    [['a','b','c']]]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN4pO6qmxHxm"
      },
      "source": [
        "Abaixo temos um array com 2 registros, sendo cada registro com 3 linhas e 4 colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsJv6j-4xHxm"
      },
      "outputs": [],
      "source": [
        "np.array([\n",
        "    [['a','b','c','d'],\n",
        "     ['a','b','c','d'],\n",
        "     ['a','b','c','d']],\n",
        "\n",
        "    [['a','b','c','d'],\n",
        "     ['a','b','c','d'],\n",
        "     ['a','b','c','d']]]).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEn5doLRxHxm"
      },
      "source": [
        "Base de dados de treinamento, 60.000 registros (imagens), tendo cada registro 28 linhas e 28 colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXcYhKT9xHxm"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNz0xVdixHxm"
      },
      "source": [
        "Base de dados de teste, 10.000 registros (imagens), tendo cada registro 28 linhas e 28 colunas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvRCJH5MxHxm"
      },
      "outputs": [],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tc3tmkcxHxm"
      },
      "source": [
        "Já nas labels (nosas classes), temos apenas 1 registro para cada matriz (imagem)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQEozqZ5xHxm"
      },
      "outputs": [],
      "source": [
        "train_labels.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPAmzLO3xHxn"
      },
      "outputs": [],
      "source": [
        "test_labels.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgN5HnNyxHxn"
      },
      "source": [
        "Entendendo a base de Dados na prática "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHtSwElbxHxn"
      },
      "outputs": [],
      "source": [
        "# Exibe a matriz da primeira imagem na base de treinamento\n",
        "train_images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwSiQE2ixHxn"
      },
      "outputs": [],
      "source": [
        "# Exibe a classe da primeira imagem na base de treinamento (Botas)\n",
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMYATvk_xHxn"
      },
      "outputs": [],
      "source": [
        "# Exibir as 25 primeiras imagens de treinamento\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5qLs_vpxHxn"
      },
      "source": [
        "### Tratar os dados\n",
        "Como tivemos uma breve contextualização sobre a normalização dos dados, nesta base de dados não será diferente.\n",
        "\n",
        "Cada pixel existente na base de dados possuí valores de 0 até 255, sendo 0 = branco e 255 = preto.\n",
        "\n",
        "A normalização de dados serve para que o modelo, no início do treinamento, considere todas as colunas igualmente importantes.\n",
        "\n",
        "Geralmente, a normalização dos dados resulta em valores de 0 até 1, ou de -1 até 1. Ou seja, o maior valor da coluna será 1, o menor será 0 e os demais valores escalonados entre 0 e 1.\n",
        "\n",
        "Aqui, como sabemos que o maior valor é 0 e o menor 255, podemos simplesmente dividir os dados por 255. desta forma, o maior valor (255) dividido por 255, se tornará 1, o menor se tornará 0 (0/255 = 0) e assim por diante (79/255 = 0.30980392156862746, 150/255 = 0.5882352941176471).\n",
        "\n",
        "Desta forma, não precisamos de calculos difíceis ou bibliotecas para isto, basta dividir as matrizes por 255."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sfTZwfaxHxn"
      },
      "outputs": [],
      "source": [
        "# Exemplo de divisão de matriz\n",
        "np.array([\n",
        "    [[0,10,50,100,150]],\n",
        "    [[0,10,50,100,150]]])/150"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_yX6G2AxHxn"
      },
      "outputs": [],
      "source": [
        "# Retorna o maior valor presente na matriz de teste antes de escalarmos os valores\n",
        "test_images.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBzPWUfUxHxn"
      },
      "outputs": [],
      "source": [
        "# train_images irá receber ela mesma dividida por 255\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "# test_images irá receber ela mesma dividida por 255\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxGnK9hXxHxn"
      },
      "outputs": [],
      "source": [
        "# Retorna o maior valor presente na matriz de teste após escalarmos os valores\n",
        "test_images.max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nnnAYQVxHxo"
      },
      "source": [
        "### Definição do modelo\n",
        "Iremos utilizar o keras que é uma biblioteca de rede neural muito famosa.\n",
        "\n",
        "Nela, iremos utilizar o model Sequencial (que possui a arquitetura de uma rede neural artificial).\n",
        "\n",
        "Ao criar um modelo de rede neural, precisamos definir:\n",
        "- input_shape (qual a forma dos dados de entrada);\n",
        "- as camadas de neurônios\n",
        "- quantos neurônios cada camada possui\n",
        "- função de ativação de cada camada\n",
        "\n",
        "Em aulas mais a frente iremos aprofundar mais nestes parâmetros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kaprWoRxHxo"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ytuTtobxHxo"
      },
      "source": [
        "Temos 60.000 imagens com o \"shape\" de 28 linhas e 28 colunas, então o input_shape do nosso modelo será (28, 28)\n",
        "\n",
        "A primeira camada utiliza o formato Flatten (achatar), o que transforma a entrada de 28x28 pixels (um array de 2 dimensões) em um array de 1 dimensão (28 * 28 = 784 pixels).\n",
        "\n",
        "Iremos utilizar 2 camadas ocultas (Dense). A primeira irá possuir 128 neurônios e a ativação relu. \n",
        "\n",
        "A segunda camada também será a camada de saída e, quando temos uma rede neural para classificação, devemos ter 1 neurônio para cada classe existente na base de dados. No caso, temos 10 classes (classes de 0 até 10), então teremos 10 neurônios na camada de saída.\n",
        "A função de ativação utilizada para classificação de várias classes sempre é uma função sigmoidal, neste caso, iremos utilizar a softmax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzHy7_VdxHxo"
      },
      "outputs": [],
      "source": [
        "# Criamos a estrutura do modelo\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ha5JPY2xHxo"
      },
      "source": [
        "Antes de treinar o modelo, precisamos passar mais algumas configurações, que são adicionadas na \"compilação\".\n",
        "\n",
        "- loss - Calcula quão preciso o modelo está sendo durante o treinamento. quanto menor o erro, melhor o modelo;\n",
        "\n",
        "- optimizer - Como o modelo atualiza os pesos baseado no loss;\n",
        "\n",
        "- metrics - Qual a métrica para monitorar o treinamento e teste. Neste caso, a acurácia (porcentagem de acertos que o modelo faz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USJZAmAUxHxo"
      },
      "outputs": [],
      "source": [
        "# Compilamos o modelo\n",
        "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
        "initial_learning_rate=1e-2,\n",
        "decay_steps=10000,\n",
        "decay_rate=0.9)\n",
        "opt = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "model.compile(optimizer=opt,\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yN-mAZixHxo"
      },
      "source": [
        "### Treinamento do Modelo\n",
        "\n",
        "Iremos usar as imagens e labels de treinamento, com 10 épocas e iremos utilizar 20 dos dados para validar o modelo, ou seja, dos dados de treinamento, somente 80% será utilizado no treinamento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmEDeiZqxHxo"
      },
      "outputs": [],
      "source": [
        "history_callback = model.fit(train_images, train_labels, epochs=60, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDJxMVgLxHxo"
      },
      "outputs": [],
      "source": [
        "# Aqui podemos salvar os valores de loss e acurácia, de treinamento e validação\n",
        "loss_history = history_callback.history['loss']\n",
        "val_loss_history = history_callback.history['val_loss'] \n",
        "accuracy_history = history_callback.history['accuracy']\n",
        "val_accuracy_history = history_callback.history['val_accuracy']  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WA8r1WRlxHxo"
      },
      "outputs": [],
      "source": [
        "# Definimos os valores do eixo x (as épocas)\n",
        "x = [y+1 for y in range(0,len(loss_history))]\n",
        "\n",
        "plt.plot(x, loss_history, label = \"Loss\")\n",
        "plt.plot(x, val_loss_history, label = \"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5X_Yo07xHxp"
      },
      "outputs": [],
      "source": [
        "plt.plot(x, accuracy_history, label = \"Accuracy\")\n",
        "plt.plot(x, val_accuracy_history, label = \"Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi4NiHMwxHxp"
      },
      "source": [
        "### Avaliar a acurácia do modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTcSIc98xHxp"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "\n",
        "print('\\nTest accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJj-GbMAxHxp"
      },
      "source": [
        "### Realizar predições com o modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTZcrHB5xHxp"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjcGDAMFxHxp"
      },
      "source": [
        "O nosso modelo possui 10 neurônios na camada de saída, sendo 1 para cada classe, então apredição dele resulta em 10 valores, sendo cada valor a porcentagem de certeza de qual classe o valor de teste pertence.\n",
        "\n",
        "De acordo com a camada de ativação softmax, a soma de todos os valores resulta em 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KlKa2wwxHxp"
      },
      "outputs": [],
      "source": [
        "# Exibe os valores de cada neurônio para a predição do primeiro item de teste\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vgM8NH8xHxp"
      },
      "outputs": [],
      "source": [
        "# Soma das predições\n",
        "sum(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N1PSG1qxHxp"
      },
      "source": [
        "Para obter qual a classe da predição, podemos usar a função argmax do numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkpOUlkPxHxp"
      },
      "outputs": [],
      "source": [
        "np.argmax(predictions[0])\n",
        "# O modelo treinado previu que a classe do primeiro elemento de teste é 9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49odSa74xHxp"
      },
      "outputs": [],
      "source": [
        "# Exibe a classe do primeiro elemento de teste\n",
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhGElAmAxHxp"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.imshow(test_images[0]*255, cmap=plt.cm.binary)\n",
        "plt.xlabel(class_names[test_labels[0]])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVNVPxK5xHxp"
      },
      "source": [
        "Aqui temos uma função para calcular as métricas para avaliar o modelo. Hoje não aprofundarei em detalhes sobre estas métricas pois abordaremos isto em aulas mais a frente.\n",
        "\n",
        "Mas uma breve explicação, as 4 primeiras métricas, quanto menor o valor, melhor o modelo. \n",
        "\n",
        "Já as 3 últimas, quanto maior o valor, melhor o modelo (valor máximo é 100). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQ3o1KwcxHxq"
      },
      "outputs": [],
      "source": [
        "def metrics(X_test, y_test, model):\n",
        "    y_predict = model.predict(X_test)\n",
        "    y_predict = [np.argmax(x) for x in y_predict]\n",
        "    y_test = y_test\n",
        "    k = X_test.shape[1]\n",
        "    n = len(X_test)\n",
        "    r2 = sm.r2_score(y_test, y_predict)\n",
        "    adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)\n",
        "    print('Root Mean Square Error: ',round(np.sqrt(np.mean(np.array(y_predict) - np.array(y_test))**2),2))\n",
        "    print('Mean Square Error:', round(sm.mean_squared_error(y_test, y_predict ),2))\n",
        "    print('Mean Absolut Error:', round(sm.mean_absolute_error(y_test, y_predict ),2))\n",
        "    print('Median Absolut Error:', round(sm.median_absolute_error(y_test, y_predict ),2))\n",
        "    print('Explain Variance Score:', round(sm.explained_variance_score(y_test, y_predict)*100,2))\n",
        "    print('R2 score:', round(sm.r2_score(y_test, y_predict)*100,2))\n",
        "    print('Adjusted R2 =', round(adj_r2*100,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Er3u-4qaxHxq"
      },
      "outputs": [],
      "source": [
        "metrics(test_images, test_labels, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-UJ4cWOxHxq"
      },
      "source": [
        "# Regressão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JINqoofAxHxq"
      },
      "source": [
        "A regressão é outra tarefa fundamental na área de Mineração de Dados, que tem como objetivo prever um valor numérico contínuo a partir de um conjunto de dados com variáveis independentes. É usada para modelar a relação entre uma variável dependente (ou resposta) e uma ou mais variáveis independentes (ou preditoras).\n",
        "\n",
        "Em outras palavras, a regressão é uma técnica estatística que ajuda a identificar e quantificar a relação entre uma variável dependente e uma ou mais variáveis independentes, com o objetivo de fazer previsões precisas para novos dados. A regressão é frequentemente usada em previsão financeira, análise de tendências de mercado, análise de séries temporais, análise de dados científicos e muitas outras áreas.\n",
        "\n",
        "Existem vários algoritmos de regressão, incluindo regressão linear, regressão logística, regressão polinomial, regressão de séries temporais, redes neurais, entre outros. Cada algoritmo tem suas próprias vantagens e desvantagens e é mais adequado para diferentes tipos de problemas de regressão.\n",
        "\n",
        "\n",
        "### Exemplo de aplicação de Regressão:\n",
        "Um exemplo de Regressão em Tarefas de Mineração de Dados é a previsão do preço de imóveis com base em suas características, como a área construída, o número de quartos, o número de banheiros, a localização, entre outros.\n",
        "\n",
        "O conjunto de dados para este exemplo seria composto por informações de imóveis já vendidos, incluindo suas características e preços de venda. O modelo seria treinado usando algoritmos de aprendizado de máquina supervisionados, onde as características dos imóveis seriam usadas como variáveis independentes e o preço de venda seria a variável dependente.\n",
        "\n",
        "Após o treinamento, o modelo seria capaz de prever o preço de novos imóveis com base em suas características. Essa previsão de preços de imóveis pode ser útil para corretores de imóveis, compradores e vendedores de imóveis, ajudando-os a tomar decisões informadas sobre preços de imóveis.\n",
        "\n",
        "### Outro exemplo de aplicação de Regressão:\n",
        "Outro exemplo de Regressão em Tarefas de Mineração de Dados é a previsão da demanda de um produto em determinado período de tempo com base em informações históricas de vendas e outras variáveis relevantes.\n",
        "\n",
        "O conjunto de dados seria composto por informações de vendas anteriores do produto, incluindo a quantidade vendida, o preço, a região de venda, o período de venda, entre outros. O modelo seria treinado usando algoritmos de aprendizado de máquina supervisionados, onde as variáveis independentes seriam as características das vendas (como preço, região, período, entre outros) e a variável dependente seria a quantidade vendida.\n",
        "\n",
        "Após o treinamento, o modelo seria capaz de prever a demanda futura do produto com base nas variáveis independentes. Essa previsão pode ser útil para ajudar empresas a planejar suas estratégias de produção e marketing com antecedência, aumentando a eficiência e reduzindo custos desnecessários.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_EdSDe38xHxq"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "# Importamos a biblioteca necessária para dividir a base de dados\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-07Og_9xHxq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('preco_casas.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93F0GX66xHxq"
      },
      "outputs": [],
      "source": [
        "df = df.values\n",
        "# Separa os dados em Variáveis Independentes (X) \n",
        "# E variável dependente (Y), que é o que queremos prever\n",
        "X = df[:,0:13]\n",
        "Y = df[:,13]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gautFcdkxHxq"
      },
      "outputs": [],
      "source": [
        "# Separamos X e y em treino e teste, com 15% para treinamento (e os 85% restantes para treinamento)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcI7xlfAxHxq"
      },
      "source": [
        "Nesta base de dados, temos apenas uma dimenção por registro, logo, o input_shape será de apenas 13."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfbK89CmxHxq"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8nUPOeSxHxq"
      },
      "source": [
        "Já a nossa camada de saida possui somente 1 neurônio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYazOGPqxHxr"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(13, input_shape=(13,), kernel_initializer='normal', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "# Compile model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2lZYZJa9xHxr"
      },
      "outputs": [],
      "source": [
        "history_callback = model.fit(x_train, y_train, epochs=1600, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lkHiZqJxHxr"
      },
      "outputs": [],
      "source": [
        "# Aqui podemos salvar os valores de loss e acurácia, de treinamento e validação\n",
        "loss_history = history_callback.history['loss']\n",
        "val_loss_history = history_callback.history['val_loss'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99bACNOzxHxr"
      },
      "outputs": [],
      "source": [
        "# Definimos os valores do eixo x (as épocas)\n",
        "x = [y+1 for y in range(0,len(loss_history))]\n",
        "plt.plot(x, loss_history, label = \"Loss\")\n",
        "plt.plot(x, val_loss_history, label = \"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjsyhEMOxHxr"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxU1-KdoxHxr"
      },
      "source": [
        "Diferente do outro modelo que tínhamos vários neurônios de saída, este só possui 1, logo, existe apenas 1 saída para cada entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nohg8u0cxHxr"
      },
      "outputs": [],
      "source": [
        "# Valor Predito\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fD08QgsSxHxr"
      },
      "outputs": [],
      "source": [
        "# Valor Real\n",
        "y_test[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upu-ouipxHxr"
      },
      "outputs": [],
      "source": [
        "# Cria um dataframe para comparar os resultados\n",
        "predictions = [x[0] for x in predictions]\n",
        "resultados = pd.DataFrame(list(zip(y_test, predictions)), columns = ['Real', 'Predito'])\n",
        "resultados = resultados.sort_values(by=(['Real']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hld_Us5VxHxr"
      },
      "outputs": [],
      "source": [
        "# Definimos os valores do eixo x (as épocas)\n",
        "x = [y+1 for y in range(0,len(resultados))]\n",
        "plt.plot(x, resultados.Real, label = \"Real\")\n",
        "plt.plot(x, resultados.Predito, label = \"Predito\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHA_fvj0xHxr"
      },
      "source": [
        "Em alguns casos, as métricas não funcionam muito bem..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFzaooDexHxr"
      },
      "outputs": [],
      "source": [
        "metrics(x_test, y_test, model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}