{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leaop/Graduation/blob/main/Algoritmos_para_Minera%C3%A7%C3%A3o_de_Dados.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERSgeZalzrpV"
      },
      "source": [
        "# 1 - Análise da base de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON9zBACFzrpX"
      },
      "source": [
        "Vamos entender a base de dados:\n",
        "\n",
        "Base de dados sobre preço de casas em regiões de Boston.\n",
        "\n",
        "A base é composta por 12 atributos (colunas previsoras - variáveis independentes) e uma coluna para ser o target, sendo o preço das casas.\n",
        "\n",
        "Abaixo segue axplicação do que cada coluna representa:\n",
        "\n",
        "* `crim` - A taxa de crimes per capita por cidade;\n",
        "* `terrenos` - A proporção de terrenos residenciais zoneados para lotes maiores que 25.000 pés quadrados (cerca de 2.322,6 metros quadrados) por cidade;\n",
        "* `comercio` - A proporção de acres (cerca de 4046m²) comerciais não varejistas por cidade;\n",
        "* `rio` - Variável que indica se o Rio Charles delimita ou não o imóvel em questão;\n",
        "* `oxido` - Concentração de óxidos nítricos (partes por 10 milhões) na região do imóvel;\n",
        "* `quartos` - Número médio de quartos na região do imóvel;\n",
        "* `1940` - Proporção de imóveis contruídos antes de 1940;\n",
        "* `empregos` - Distância ponderada da região para os 5 principais centros de emprego de Boston;\n",
        "* `rodovias` - Índice de acessibilidade às principais rodovias da cidade;\n",
        "* `imposto` - Imposto pago por cada U$10.000 do valor da propriedade;\n",
        "* `educacao` - Proporção de quantidade de alunos/professores nas regiões;\n",
        "* `status` - Porcentagem da população da região com status social abaixo da média americana;\n",
        "* `valor` - Valor médio das casas da região"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w21JaANNzrpX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('casas_boston.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6g8kogMzrpY"
      },
      "outputs": [],
      "source": [
        "# Exibir as duas primeiras linhas do dataframe\n",
        "df.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwOH-XK6zrpY"
      },
      "outputs": [],
      "source": [
        "# Quantidade de valores nulos por coluna\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6hkY6m-zrpY"
      },
      "source": [
        "Como não temos valores nulos nas colunas, não faz sentido analisar a porcentagem de valores nulos por coluna, mas o comando seria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nOytuCb2zrpZ"
      },
      "outputs": [],
      "source": [
        "df.isna().sum() * 100 / len(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9RGH_LFzrpZ"
      },
      "source": [
        "Aqui o mais interessante é avaliar os tipos de dados das colunas, onde, no final da análise podemos observar que temos 10 colunas do tipo float (números com casas decimais) e 3 colunas int (números inteiros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBsitL88zrpZ"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0jvxjR3zrpZ"
      },
      "source": [
        "Outra forma de analisar os tipos de dados das colunas seria usando o comando dtypes (abreviação de data types - Tipos de dados)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBrRR2kIzrpa"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npoqDpKGzrpa"
      },
      "source": [
        "Nas informações descritivas da base de dados não precisamos verificar as colunas não numéricas (do tipo object) pois a base de dados não possui coluna com este tipo de dados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEZ-nhbGzrpa"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kae0yr-0zrpa"
      },
      "source": [
        "Em uma base de dados com muitas colunas esta análise pode ficar meio ruim de interpretar devido à \"falta de espaço\" na tela. Para solucionar, podemos fazer a Matriz Transposta (inverte as linhas e colunas) da análise apenas colocando um `.T` no final do comando.\n",
        "\n",
        "Entendendo as informações:\n",
        "\n",
        "* `Count` - Quantos valores não nulos a coluna possuí;\n",
        "* `mean` - A média dos valores da coluna (a soma de todos os valores dividida pela quantidade de valores)\n",
        "* `std` - standard deviation (desvio padrão) é uma medida de dispersão que informa quão longe os valores estão da média da coluna;\n",
        "* `min` - Qual o valor mínimo que aparece na coluna;\n",
        "* `25%` - Qual valor que 25% dos registros são menores ou iguais;\n",
        "* `50%` - Qual valor que 50% dos registros são menores ou iguais;\n",
        "* `75%` - Qual valor que 75% dos registros são menores ou iguais;\n",
        "* `max` - Qual o valor máximo que aparece na coluna;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TDweL5nzrpa"
      },
      "outputs": [],
      "source": [
        "df.describe().T "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6o5LSNQzrpa"
      },
      "source": [
        "## 1.1 - Desvio padrão"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3fXos6bzrpa"
      },
      "source": [
        "O desvio padrão é utilizado para analisar a dispersão de um conjunto de dados. Um valor alto indica que o os valores do conjunto de dados tendem a estar distantes da média, ou seja, a distribuição é mais “espalhada”. Se o valor for pequeno os dados tendem a estar mais concentrados em torno da média.\n",
        "\n",
        "\n",
        "O desvio padrão é a raiz quadrada da variância, logo, precisamos calcular a variância que é a soma dos quadrados das diferenças entre cada valor e a média / número de valores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5zd-FlDzrpb"
      },
      "outputs": [],
      "source": [
        "# Retorna quantos valores não nulos a coluna comercio possui\n",
        "df[['comercio']].count()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HOJ4XB0zrpb"
      },
      "outputs": [],
      "source": [
        "# Retorna a soma de todos os valores da coluna comercio\n",
        "df[['comercio']].sum()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyQw6ELCzrpb"
      },
      "outputs": [],
      "source": [
        "# Calcular a média da coluna comercio \n",
        "qtd = df[['comercio']].count()[0]\n",
        "soma = df[['comercio']].sum()[0]\n",
        "media = soma/qtd\n",
        "media"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0Gb5U8-zrpb"
      },
      "outputs": [],
      "source": [
        "# Exibe as 5 primeiras linhas da coluna comercio\n",
        "df[['comercio']].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Mv07t9fzrpb"
      },
      "outputs": [],
      "source": [
        "# Exibe as 5 primeiras linhas da coluna comercio - 1 \n",
        "df[['comercio']].head()-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AlgKcAAzrpb"
      },
      "outputs": [],
      "source": [
        "# Exibe os valors da coluna comercio menos a média da coluna comercio\n",
        "df[['comercio']] - media"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "501lcVmyzrpb"
      },
      "outputs": [],
      "source": [
        "# Exibe os valors da coluna comercio menos a média da coluna comercio, tudo isto elevado ao quadrado\n",
        "(df[['comercio']] - media) ** 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9YjCuhxzrpc"
      },
      "outputs": [],
      "source": [
        "# a soma de: ((os valors da coluna comercio menos a média da coluna comercio) elevado ao quadrado)\n",
        "((df[['comercio']] - media) ** 2).sum()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZS-MJywCzrpc"
      },
      "outputs": [],
      "source": [
        "# O cálculo anterior dividido pela quantidade de registros não nulos\n",
        "variancia = (((df[['comercio']] - media) ** 2).sum() / qtd)[0]\n",
        "variancia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALNqFK9Ozrpc"
      },
      "source": [
        "Para calcular a raiz quadrada no python podemos elevar o valor a 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0jzrOQwzrpc"
      },
      "outputs": [],
      "source": [
        "variancia ** 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4j2hZd5zrpc"
      },
      "source": [
        "Agora que temos a teoria do desvio padrão, podemos utilizar o numpy para calcular isto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZD-RAfZzrpc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "desvio_padrao = np.std(df[['comercio']].values)\n",
        "desvio_padrao"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIb0oLAczrpc"
      },
      "source": [
        "### 1.1.1 - Gráfico de Desvio padrão"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTTabsRZzrpc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import math\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "# Calcula a média dos valores da lista (Coluna)\n",
        "def media(lista):\n",
        "    # Número de itens na lista\n",
        "    n = len(lista)\n",
        "    # Média dos dados da lista\n",
        "    mean = sum(lista) / n\n",
        "    return mean\n",
        "\n",
        "# Calcula a variância dos valores da lista (Coluna)\n",
        "def variancia(lista):\n",
        "    # Número de itens na lista\n",
        "    n = len(lista)\n",
        "    #Recebe a média da lista\n",
        "    mean = media(lista)\n",
        "    # Desvios quadrados\n",
        "    desvio = [(x - mean) ** 2 for x in lista]\n",
        "    # Variância\n",
        "    variancia = sum(desvio) / n\n",
        "    return variancia\n",
        "\n",
        "# Calcula o desvio padrão da lista (Coluna)\n",
        "def desv_pad(lista):\n",
        "    # Recebe a variância dos valores da lista\n",
        "    var = variancia(lista)\n",
        "    # Calcula o desvio padrão\n",
        "    desv_padrao = math.sqrt(var)\n",
        "    return desv_padrao\n",
        "\n",
        "\n",
        "# Calcula Função densidade de probabilidade (PDF - Probability Density Function) da lista (Coluna)\n",
        "def dens_prob(lista):\n",
        "    pdf = stats.norm.pdf(lista, media(lista), desv_pad(lista))\n",
        "    return pdf\n",
        "\n",
        "# Lista que irá receber os valores de todas as colunas (x_values) (Cada posição na lista é uma outra lista com os valores ordenados de uma coluna)\n",
        "x_val = []\n",
        "# Lista que irá receber a Função densidade de probabilidade de todas as colunas\n",
        "pdf_val = []\n",
        "# Lista que irá receber o nome do gráfico\n",
        "graph_name = []\n",
        "\n",
        "# Loop que irá percorrer o nome das colunas do dataframe\n",
        "for x in df.columns:\n",
        "        # Recebe os valores não nulos da coluna\n",
        "        x_values = list(df[x].dropna())\n",
        "        # Ordena os valores em ordem crescente\n",
        "        x_values.sort()\n",
        "        # Salva os valores de x_value na lista x_val\n",
        "        x_val.append(x_values)\n",
        "        \n",
        "        # Salva os valores da Função densidade de probabilidade na lista pdf_val\n",
        "        pdf_val.append(dens_prob(x_values))\n",
        "\n",
        "        # Salva o nome do gráfico na lista graph_name\n",
        "        graph_name.append('Desvio Padrão de '+ x)\n",
        "\n",
        "def grafico_desvio_padrao(x_val, pdf_val, graph_name, posicao):\n",
        "    # Define o tamanho da gráfico\n",
        "    plt.figure(figsize=(20,10))\n",
        "\n",
        "    # Define o estilo do gráfico\n",
        "    plt.style.use('seaborn-whitegrid')\n",
        "\n",
        "\n",
        "    # Define o gráfico\n",
        "    plt.plot(x_val[posicao], pdf_val[posicao]) \n",
        "    # Define o nome do gráfico\n",
        "    plt.title(graph_name[posicao],size = 20)\n",
        "\n",
        "\n",
        "    # Define o nome do eixo x\n",
        "    plt.xlabel('Valor',size = 20)\n",
        "    # Define o nome do eixo y\n",
        "    plt.ylabel('Quantidade',size = 20)\n",
        "\n",
        "    # Define o tamanho dos valores no eixo x\n",
        "    plt.xticks(size = 20)\n",
        "    # Define o tamanho dos valores no eixo y\n",
        "    plt.yticks(size = 20)\n",
        "\n",
        "    # Recebe o valor de x no ponto y máximo\n",
        "    xmax = x_val[posicao][np.argmax(pdf_val[posicao])]\n",
        "    # Recebe o maior valor de y\n",
        "    ymax = pdf_val[posicao].max()\n",
        "\n",
        "\n",
        "    # Define a linha horizontal cortando o ponto y máximo\n",
        "    plt.axhline(ymax, ls=':', c='k')\n",
        "    # Define a linha vertical cortando o ponto y máximo\n",
        "    plt.axvline(xmax, ls=':', c='k')\n",
        "\n",
        "\n",
        "    # Plota os valores de x e y no ponto y máximo\n",
        "    plt.text(xmax*1.003, ymax*1.015, f'({round(xmax,4)}, {round(ymax)})')\n",
        "\n",
        "\n",
        "    # Define o ponto verde no y máximo\n",
        "    plt.plot(xmax,ymax,'s', color = 'g')\n",
        "\n",
        "    # Define o tamanho da fonte\n",
        "    plt.rcParams['font.size'] = '15'\n",
        "\n",
        "    # Para salvar a figura, descomente a linha abaixo -------------------------\n",
        "    #plt.savefig('fig.png')\n",
        "\n",
        "    plt.show()\n",
        "    %matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GkDVZIAazrpc"
      },
      "outputs": [],
      "source": [
        "grafico_desvio_padrao(x_val, pdf_val, graph_name, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU6pw5Tjzrpd"
      },
      "outputs": [],
      "source": [
        "grafico_desvio_padrao(x_val, pdf_val, graph_name, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eebSD13bzrpd"
      },
      "source": [
        "# 2 - Algorítmos de Mineração de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5O4lw2tzrpd"
      },
      "source": [
        "Como já vimos anteriormente, a Mineração de Dados nus ajuda a extrair informações valiosos em conjuntos de dados. Existem vários algoritmos de mineração de dados disponíveis dos quais temos que selecionar o(s) mais adequados para cada base de dados e problema que queremos solucionar.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR67UpdZzrpd"
      },
      "source": [
        "## 2.1 - Regressão Linear:\n",
        "A regressão linear é um algoritmo de aprendizado supervisionado que tenta modelar a relação entre uma variável dependente (nosso target) contínua (valores numéricos como valor, peso, comprimento, tempo, área...) e uma ou mais variáveis independentes. \n",
        "\n",
        "Como a regressão linear é um algoritmo supervisionado (que sabemos qual classe queremos prever), precisamos realizar os seguintes passos antes de aplicar os dados no algoritmo:\n",
        "* 1 - Separar os dados em X e y, sendo X o valor das colunas idependentes e y a nossa variável independente (o target, alvo que desejamos prever)\n",
        "* 2 - Normalizar os dados de X e y\n",
        "* 3 - Separar X e y em treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xpQiuGpzrpd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUfj5Jrgzrpd"
      },
      "source": [
        "### 2.1.1 - Separar dados em X e y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27gD0rmwzrpd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('casas_boston.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wp8OG9zmzrpd"
      },
      "outputs": [],
      "source": [
        "# Retorno uma lista com as colunas da base de dados\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4UUPPjHzrpd"
      },
      "outputs": [],
      "source": [
        "# Retorno uma lista com as colunas da base de dados sem a última coluna\n",
        "df.columns[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPJ9OZB3zrpd"
      },
      "outputs": [],
      "source": [
        "X = df[df.columns[:-1]]\n",
        "\n",
        "# Exibe a base de dados sem a última coluna\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlWJviUtzrpe"
      },
      "outputs": [],
      "source": [
        "y = df[['valor']]\n",
        "\n",
        "# Exibe a base de dados sem a última coluna\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMt65UaJzrpe"
      },
      "source": [
        "### 2.1.2 - Normalizar os dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bV1rqLJDzrpe"
      },
      "outputs": [],
      "source": [
        "# Cria uma instância do MinMaxScaler para normalizar as features (atributos, variáveis independentes)\n",
        "scaler_features =  MinMaxScaler()\n",
        "\n",
        "# Define quais oa valores o scaler_features irá utilizar para normalizar cada coluna\n",
        "scaler_features.fit(X)\n",
        "\n",
        "# Normaliza os dados de X\n",
        "X = scaler_features.transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S_FT_gyzrpe"
      },
      "outputs": [],
      "source": [
        "# Cria uma instância do MinMaxScaler para normalizar o nosso alvo (target, variáveis dependente)\n",
        "scaler_target =  MinMaxScaler()\n",
        "\n",
        "# Define quais oa valores o scaler_target irá utilizar para normalizar a coluna\n",
        "scaler_target.fit(y)\n",
        "\n",
        "# Normaliza os dados de y\n",
        "y = scaler_target.transform(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kHX89MJzrpe"
      },
      "source": [
        "Repare que antes da normalização estávamos utilizando dataframes em X e y, após normalizar o próprio MinMaxScaler do Sklearn já converte os valores para uma matriz numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ylJ3vydzrpe"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mW3PtMRvzrpe"
      },
      "outputs": [],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKizNeHCzrpe"
      },
      "source": [
        "### 2.1.3 - Separar os dados em treinamento e teste"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qZAPWWjzrpe"
      },
      "outputs": [],
      "source": [
        "# Dividir o conjunto de dados em dados de treinamento e teste, com 20% para teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TvZIW8Zzrpe"
      },
      "source": [
        "### 2.1.4 - Treinar o modelo de Regressão Linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNVwntsZzrpe"
      },
      "outputs": [],
      "source": [
        "# Criar uma instância do modelo de regressão linear\n",
        "model_lr = LinearRegression()\n",
        "model_lr.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqsduH7Lzrpf"
      },
      "source": [
        "### 2.1.5 - testar/avaliar o modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXIiEGpOzrpf"
      },
      "outputs": [],
      "source": [
        "# Avaliar o desempenho do modelo nos dados de teste\n",
        "predict = model_lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRqKy6sizrpf"
      },
      "source": [
        "Como é uma regressão linear, não temos como calcular a acurácia (A acurácia mede porcentagem de acertos quando queremos prever uma classe) pois não temos uma classe (por exemplo a base de dados de roupas que tinham as classes bota, blusa, camiseta, vestido...)\n",
        "\n",
        "Então uma métrica para utilizar é o MSE de Mean Square Error ou Erro Médio Quadrático, que verifica quanto ele está errando em relação aos valores reais. Quanto menor o valor, melhor o modelo.\n",
        "\n",
        "Outra forma de avaliar o resultado de uma regressão linear, é utilizando um gráfico para comparar os valores reais e os valores preditos.\n",
        "\n",
        "Mas, antes de fazermos estas análises, é importante \"desnormalizar\" os dados de y e os dados que o modelo previu. Para isto iremos usar o próprio scaler_target com a função inverse_transform.\n",
        "\n",
        "\n",
        "`Plot Twist` - Vamos treinar um algoritmo de rede neural e comparar com os resultados do algoritmo de regressão linear "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT7pHY27zrpf"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, InputLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1beGUQKzrpf"
      },
      "outputs": [],
      "source": [
        "X_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHWpOi4lzrpf"
      },
      "outputs": [],
      "source": [
        "# Criamos uma instância do modelo de rede neural\n",
        "model_rn = Sequential()\n",
        "model_rn.add(InputLayer(input_shape=(X_train.shape[1],)))\n",
        "model_rn.add(Dense(50, activation='relu'))\n",
        "model_rn.add(Dense(50, activation='relu'))\n",
        "model_rn.add(Dense(50, activation='relu'))\n",
        "model_rn.add(Dense(50, activation='relu'))\n",
        "model_rn.add(Dense(50, activation='relu'))\n",
        "model_rn.add(Dense(1, activation='linear'))\n",
        "\n",
        "# compila o modelo\n",
        "model_rn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "\n",
        "# treina o modelo\n",
        "model_rn.fit(X_train, y_train, epochs=200, batch_size=10, verbose = 0)\n",
        "\n",
        "predict_rn = model_rn.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PgjD9DWzrpf"
      },
      "outputs": [],
      "source": [
        "# Desnormaliza o y_test na variável y_desnormalizado\n",
        "y_test_desnormalizado = scaler_target.inverse_transform(y_test)\n",
        "# Desnormaliza o predict na variável predict_desnormalizado\n",
        "predict_desnormalizado = scaler_target.inverse_transform(predict)\n",
        "# Desnormaliza o predict na variável predict_desnormalizado\n",
        "predict_rn_desnormalizado = scaler_target.inverse_transform(predict_rn)\n",
        "\n",
        "# Calcula o mse com a biblioteca do sklearn\n",
        "mse = mean_squared_error(y_test_desnormalizado, predict_desnormalizado)\n",
        "print(\"Mean squared error Regressão Linear: \", mse)\n",
        "\n",
        "mse_rn = mean_squared_error(y_test_desnormalizado, predict_rn_desnormalizado)\n",
        "print(\"Mean squared error Rede Neural: \", mse_rn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17kbAocyzrpf"
      },
      "outputs": [],
      "source": [
        "# Exibe o erro médio quadrático do y_test e predict sem a desnormalização\n",
        "mse = mean_squared_error(y_test, predict)\n",
        "print(\"Mean squared error: \", mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53G5rrUmzrpf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.plot(range(0,len(y_test_desnormalizado)), y_test_desnormalizado, label = 'Real')\n",
        "plt.plot(range(0,len(predict_desnormalizado)), predict_desnormalizado, label = 'Predict RL')\n",
        "plt.plot(range(0,len(predict_rn_desnormalizado)), predict_rn_desnormalizado, label = 'Predict RN')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZWsDdhuzrpf"
      },
      "source": [
        "Repare que desta forma pode ter ficado um pouco ruim de visualizar o real com o valor predito. Para melhorar esta vizualização, podemos criar um dataframe com o valor real e o valor predito e ordenar os valores reais de forma crescente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnupGyUhzrpg"
      },
      "outputs": [],
      "source": [
        "# Cria um dataframe para ordenar os valores\n",
        "df_grafico_predict = pd.DataFrame()\n",
        "\n",
        "# Acrescenta o valor de y_test_desnormalizado na coluna real\n",
        "df_grafico_predict['real'] = y_test_desnormalizado.flatten()\n",
        "\n",
        "# Acrescenta o valor de predict_desnormalizado na coluna predict\n",
        "df_grafico_predict['predict_rl'] = predict_desnormalizado.flatten()\n",
        "\n",
        "# Acrescenta o valor de predict_desnormalizado na coluna predict\n",
        "df_grafico_predict['predict_rn'] = predict_rn_desnormalizado.flatten()\n",
        "\n",
        "# Ordena os dados pela coluna real de forma crescente\n",
        "df_grafico_predict.sort_values(by='real', inplace = True)\n",
        "\n",
        "plt.plot(range(0,len(y_test_desnormalizado)), df_grafico_predict['real'], label = 'Real')\n",
        "plt.plot(range(0,len(predict_desnormalizado)), df_grafico_predict['predict_rl'], label = 'Predict RL')\n",
        "plt.plot(range(0,len(predict_desnormalizado)), df_grafico_predict['predict_rn'], label = 'Predict RN')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a3QMB4Jzrpg"
      },
      "source": [
        "## 2.2 - Ávore de Decisão e Floresta Aleatória \n",
        "\n",
        "A árvore de decisão é um algoritmo de aprendizado supervisionado que cria um modelo de decisão em forma de árvore. Cada nó interno da árvore representa uma decisão baseada em um valor de atributo, e cada folha representa o valor de saída ou a decisão final.\n",
        "\n",
        "Podemos pensar na Floresta aleatória como um conjuntos de ávores com diferentes características.\n",
        "\n",
        "Tando a ávore como a floresta são algoritmos que determinam uma classe. Então, diferente do exemplo que vimos, estes algoritmos não conseguiriam prever os valores das casas. Para seguirmos com a mesma base de dados, podemos classificar os valores em faixa de valores da seguinte forma: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cd-b9V_Ozrpg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, InputLayer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('casas_boston.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S6yxJOMzrpg"
      },
      "outputs": [],
      "source": [
        "df.valor.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhpGruCzzrpg"
      },
      "source": [
        "Podemos ver que o valor mínimo dos preçõs é 5000 e o máximo 50.000.\n",
        "\n",
        "Sendo assim, que tal dividir os valores em 10 classes, sendo:\n",
        "\n",
        "1 = casas com valores de U$5.000 a valores menores que U$10.000\n",
        "\n",
        "2 = casas com valores de U$10.000 a valores menores que U$20.000\n",
        "\n",
        "3 = casas com valores de U$20.000 a valores menores que U$30.000\n",
        "\n",
        "4 = casas com valores de U$30.000 a valores menores que U$40.000\n",
        "\n",
        "5 = casas com valores a partir de U$40.000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTV5P82azrpg"
      },
      "outputs": [],
      "source": [
        "# Acrescenta as categorias de cada valor\n",
        "df.loc[(df['valor']>=5000) & (df['valor']<10000), 'categoria_valor'] = 0\n",
        "df.loc[(df['valor']>=10000) & (df['valor']<20000), 'categoria_valor'] = 1\n",
        "df.loc[(df['valor']>=20000) & (df['valor']<30000), 'categoria_valor'] = 2\n",
        "df.loc[(df['valor']>=30000) & (df['valor']<40000), 'categoria_valor'] = 3\n",
        "df.loc[df['valor']>=40000, 'categoria_valor'] = 4\n",
        "\n",
        "# Remove a coluna valor\n",
        "df.drop('valor', axis = 1, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3E91ra6-zrpg"
      },
      "outputs": [],
      "source": [
        "df.categoria_valor.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iOCw7Czzrpg"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKxpEQM0zrpg"
      },
      "source": [
        "### 2.2.1 - Tratar dados para aplicar ao modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSkvuMW8zrpg"
      },
      "outputs": [],
      "source": [
        "X = df[df.columns[:-1]]\n",
        "y = df[['categoria_valor']].values.flatten()\n",
        "\n",
        "# Cria uma instância do MinMaxScaler para normalizar as features (atributos, variáveis independentes)\n",
        "scaler_features =  MinMaxScaler()\n",
        "\n",
        "# Define quais oa valores o scaler_features irá utilizar para normalizar cada coluna\n",
        "scaler_features.fit(X)\n",
        "\n",
        "# Normaliza os dados de X\n",
        "X = scaler_features.transform(X)\n",
        "\n",
        "# Dividir o conjunto de dados em dados de treinamento e teste, com 20% para teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVM6lZ0Fzrpg"
      },
      "source": [
        "Quando utilizamos uma classificação, geralmente não normalizamos o alvo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu6lg5eVzrph"
      },
      "outputs": [],
      "source": [
        "# Árvore de decisão\n",
        "dtc = DecisionTreeClassifier()\n",
        "dtc.fit(X_train, y_train)\n",
        "y_pred_dtc = dtc.predict(X_test)\n",
        "acc_dtc = accuracy_score(y_test, y_pred_dtc)\n",
        "print(\"Acurácia Árvore de Decisão: \", acc_dtc)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_dtc)\n",
        "# Exibindo a matriz de confusão em forma de gráfico\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Classe Predita')\n",
        "plt.ylabel('Classe Real')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILs1GIXDzrph"
      },
      "outputs": [],
      "source": [
        "# Floresta aleatória\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(X_train, y_train)\n",
        "y_pred_rfc = rfc.predict(X_test)\n",
        "acc_rfc = accuracy_score(y_test, y_pred_rfc)\n",
        "print(\"Acurácia Floresta Aleatória: \", acc_rfc)\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_rfc)\n",
        "# Exibindo a matriz de confusão em forma de gráfico\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Classe Predita')\n",
        "plt.ylabel('Classe Real')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6zY9m4Lzrph"
      },
      "outputs": [],
      "source": [
        "df[['categoria_valor']].nunique()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKOChz95zrph"
      },
      "outputs": [],
      "source": [
        "# Criamos uma instância do modelo de rede neural\n",
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(X_train.shape[1],)))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "# O que iremos alterar é a função de ativação e quantos neurônios na camada de saída\n",
        "model.add(Dense(df[['categoria_valor']].nunique()[0], activation='softmax'))\n",
        "\n",
        "# compila o modelo, também alteramos o loss para categórico\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# treina o modelo\n",
        "model.fit(X_train, pd.get_dummies(y_train), epochs=200, batch_size=10, verbose = 0)\n",
        "\n",
        "y_pred_rn = model.predict(X_test)\n",
        "\n",
        "_, acc_nn = model.evaluate(X_test, pd.get_dummies(y_test), verbose=0)\n",
        "print(\"Acurácia Rede Neural: \", acc_nn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VbRlrYozrph"
      },
      "source": [
        "Entendendo a função de ativação SoftMax:\n",
        "\n",
        "Esta função de ativação retorna uma lista para cada previsão.\n",
        "\n",
        "Esta lista é composta pela quantidade de classes que existem, no caso, temos 5 classes, então será retornado 1 lista com 5 valores onde cada valor representa a porcentagem de chance dos valores de X (atributos) pertencerem a cada classe. A soma dos 5 valores é 1.\n",
        "\n",
        "Exemplo de como é o resultado do predict:\n",
        "\n",
        "[0.001, 0.022, 0.957, 0.01, 0.01]\n",
        "\n",
        "onde cada posição na lista, representa, respectivamente, a seguinte categoria:\n",
        "\n",
        "[1, 2, 3, 4 , 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1l0GuAqzrph"
      },
      "outputs": [],
      "source": [
        "0.001 + 0.022 + 0.957 + 0.01 + 0.01"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NU7Ydpfnzrph"
      },
      "outputs": [],
      "source": [
        "y_pred_rn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSQWt2eBzrph"
      },
      "outputs": [],
      "source": [
        "# Com este comando, obtemos qual a categoria que possui maior porcentagem para cada previsão\n",
        "y_pred_rn = np.argmax(y_pred_rn,axis=1)\n",
        "y_pred_rn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SipuCAvQzrph"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_test, y_pred_rn)\n",
        "# Exibindo a matriz de confusão em forma de gráfico\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n",
        "plt.xlabel('Classe Predita')\n",
        "plt.ylabel('Classe Real')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKwzd0iizrph"
      },
      "source": [
        "## 2.3 - Clusterização com kmeans\n",
        "\n",
        "O K-Means é um algoritmo de agrupamento que agrupa dados em K grupos diferentes. O algoritmo tenta minimizar a distância entre os pontos de dados em cada grupo e o centro do grupo.\n",
        "\n",
        "Este algoritmo é não supervisionado, ou seja, não sabemos o que queremos prever (não temos a classe algo no final da tabela...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_cialWxDzrph"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output \n",
        "\n",
        "df = pd.read_csv('casas_boston.csv')\n",
        "\n",
        "X = df[df.columns[:-1]].values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX7BQn8Yzrpi"
      },
      "source": [
        "Ao utilizar a mesma base de dados, não devemos utilizar a classe alvo (o nosso y). Iremos utilizar apenas os atributos (classes previsoras ou features).\n",
        "\n",
        "Em geral, a decisão de normalizar ou não os dados antes da clusterização dependerá das características dos dados em questão e dos objetivos específicos da análise. Em certas situações, a normalização pode não ser necessária ou até mesmo prejudicar a análise. Por exemplo, se as variáveis ​​já estiverem na mesma escala ou se você quiser que a clusterização seja influenciada por variáveis ​​com valores maiores, é possível que a normalização não seja necessária.\n",
        "\n",
        "Aqui não irei normalizar os dados, visando encontrar grupos mais relacionados às variáveis com maires valores (terreno, 1940 e imposto)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOAxRPNvzrpi"
      },
      "source": [
        "### 2.3.1 - Definir quantidade de clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuqvDEOTzrpi"
      },
      "outputs": [],
      "source": [
        "# Defina o intervalo de valores k\n",
        "k_range = range(1, 10)\n",
        "\n",
        "# Execute o algoritmo de clustering para cada valor k e armazene a soma dos erros quadráticos em uma lista\n",
        "sse = []\n",
        "for k in k_range:\n",
        "    km = KMeans(n_clusters=k)\n",
        "    km.fit(X)\n",
        "    sse.append(km.inertia_)\n",
        "# Limpa as mensagens de saída\n",
        "clear_output(wait=True)\n",
        "\n",
        "# Trace a curva de cotovelo\n",
        "plt.plot(k_range, sse)\n",
        "plt.xlabel('Número de Clusters (k)')\n",
        "plt.ylabel('soma dos erros quadráticos ')\n",
        "plt.title('Método do Cotovelo')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77MyqiNHzrpi"
      },
      "source": [
        "### 2.3.2 - Treinar o modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVvD71mzzrpi"
      },
      "outputs": [],
      "source": [
        "# Cria o modelo KMeans com 3 clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n",
        "\n",
        "# Aplica o modelo aos dados\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Obtém os rótulos dos clusters para cada amostra\n",
        "labels = kmeans.labels_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCO2bVgyzrpi"
      },
      "source": [
        "### 2.3.3 - Gráficos\n",
        "Vamos entender os gráficos que conseguimos gerar na clusterização:\n",
        "\n",
        "Quando vamos definir o nosso gráfico para analisar os clusters que foram gerados, temos que selecionar 2 colunas, uma para formar os valores do eixo x e outra para o eixo y do gráfico.\n",
        "\n",
        "Desta forma, podemos cruzar várias colunas a fim de identificar algum padrão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVRzsuXNzrpi"
      },
      "outputs": [],
      "source": [
        "df.head(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kD-AQPH9zrpi"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['imposto'], df['crim'],c=labels)\n",
        "plt.xlabel('Imposto por U$10.000')\n",
        "plt.ylabel('Criminalidade')\n",
        "plt.title('Criminalidade x Imposto por U$10.000')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zf4p12YLzrpi"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['imposto'], df['educacao'], c=labels)\n",
        "plt.xlabel('Imposto por U$10.000')\n",
        "plt.ylabel('Educação')\n",
        "plt.title('Educação x Imposto por U$10.000')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP6Qi8GEzrpi"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['imposto'], df['comercio'], c=labels)\n",
        "plt.xlabel('Imposto por U$10.000')\n",
        "plt.ylabel('Quantidade de Comércio')\n",
        "plt.title('Quantidade de Comércio x Imposto por U$10.000')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bwbowSQzrpi"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['oxido'], df['comercio'], c=labels)\n",
        "plt.xlabel('Poluição Óxido')\n",
        "plt.ylabel('Quantidade de Comércio')\n",
        "plt.title('Poluição Óxido x Quantidade de Comércio')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puYV-jRazrpj"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['oxido'], df['status'], c=labels)\n",
        "plt.xlabel('Poluição Óxido')\n",
        "plt.ylabel('Status Social Abaixo da Média')\n",
        "plt.title('Poluição Óxido x Status Social Abaixo da Média')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfk3qWiNzrpj"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['terrenos'], df['quartos'], c=labels)\n",
        "plt.xlabel('Terrenos Grandes')\n",
        "plt.ylabel('Quantidade de Quartos')\n",
        "plt.title('Terrenos Grandes x Quantidade de Quartos')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CacKrEBCzrpj"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['comercio'], df['empregos'], c=labels)\n",
        "plt.xlabel('Comercio')\n",
        "plt.ylabel('Proximidade dos Polos de Emprego')\n",
        "plt.title('Comercio x Emprego')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daKXZBdMzrpj"
      },
      "outputs": [],
      "source": [
        "# Plota os clusters\n",
        "plt.scatter(df['quartos'], df['empregos'], c=labels)\n",
        "plt.xlabel('Quantidade Quartos')\n",
        "plt.ylabel('Proximidade dos Polos de Emprego')\n",
        "plt.title('Quantidade Quartos x Emprego')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_gIfAOizrpj"
      },
      "source": [
        "## 2.4 - Redução de dimensionalidade com PCA\n",
        "\n",
        "O PCA (Principal Component Analysis), Análise de Componentes Principais em portugês é um algoritmo de redução de dimensionalidade que tenta encontrar a melhor representação dos dados em um espaço de menor dimensão.\n",
        "\n",
        "O objetivo do PCA é identificar padrões nos dados, como variáveis altamente correlacionadas, e transformar esses padrões em um conjunto menor de variáveis chamadas de \"componentes principais\".\n",
        "\n",
        "Em termos simples, o PCA encontra uma nova base de dados que é uma combinação linear das variáveis originais, de tal forma que a primeira componente principal captura a maior quantidade possível de variação dos dados, a segunda componente principal captura a maior quantidade possível de variação restante e assim por diante. Essas componentes são ordenadas de acordo com a quantidade de variação que elas capturam e são ortogonais entre si, o que significa que elas não estão correlacionadas.\n",
        "\n",
        "Quando aplicamos o PCA a um conjunto de dados, podemos escolher o número de componentes principais que desejamos manter. Isso nos permite reduzir a dimensionalidade do conjunto de dados, eliminando variáveis redundantes ou pouco informativas e, assim, simplificar o modelo sem perder muita informação. Além disso, o PCA pode ajudar a visualizar os padrões nos dados em um espaço de menor dimensão.\n",
        "\n",
        "``Para que utilizar o PCA?``\n",
        "\n",
        "Imagine uma base de dados com 20M de Linhas e 500 colunas. Para treinar um algoritmo de machine learning para fazer previsões será necessário um poder computacional elevado e talvez algumas horas... \n",
        "\n",
        "Então entra o PCA, quem sabe reduzir as 500 colunas para 20? desta forma seriam 20M de linhas e 20 colunas. ainda iríamos precisar de um tempo para treinar o modelo, mas certamente levaria menos tempo.\n",
        "\n",
        "No exemplo abaixo, aplicamos o PCA à base de dados casas_boston e reduzimos o número de dimensões de 12 para 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGwU1N2azrpj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "df = pd.read_csv('casas_boston.csv')\n",
        "df.head()\n",
        "\n",
        "X = df[df.columns[:-1]]\n",
        "y = df[[df.columns[-1]]]\n",
        "\n",
        "# Normaliza os dados das features com StandardScale\n",
        "scaler_features = StandardScaler()\n",
        "X = scaler_features.fit_transform(X)\n",
        "\n",
        "# Normaliza os dados do alvo com StandardScale\n",
        "scaler_target = StandardScaler()\n",
        "y = scaler_target.fit_transform(y)\n",
        "\n",
        "# Define uma instância do PCA para reduziar a dimensionalidade para 2 colunas\n",
        "pca = PCA(n_components=2)\n",
        "# Aplica os dados de X no pca e retorna uma lista com duas listas (Uma para cada coluna nova)\n",
        "df_pca = pca.fit_transform(X)\n",
        "\n",
        "# Cria um dataframe do PCA\n",
        "df_pca = pd.DataFrame(data=df_pca, columns=['PC1', 'PC2'])\n",
        "df_pca = pd.concat([df_pca, pd.DataFrame(y, columns=['target'])], axis=1)\n",
        "\n",
        "X_pca = df_pca[df_pca.columns[:-1]]\n",
        "# Exibe o dataframe do PCA\n",
        "df_pca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nani5f9qzrpj"
      },
      "source": [
        "Vamos comparar os resultados da base real com a base do PCA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr-VKEOozrpj"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, InputLayer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def return_compiled_model(X_train):\n",
        "    # Criamos uma instância do modelo de rede neural\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(input_shape=(X_train.shape[1],)))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    # compila o modelo\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
        "    return model\n",
        "    \n",
        "# Dividir o conjunto de dados em dados de treinamento e teste, com 20% para teste\n",
        "X_normal_train, X_normal_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=73)\n",
        "\n",
        "# Dividir o conjunto de dados em dados de treinamento e teste, com 20% para teste\n",
        "X_pca_train, X_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=73)\n",
        "\n",
        "\n",
        "\n",
        "# Define o modelo para os dados normais\n",
        "model_normal = return_compiled_model(X_normal_train)\n",
        "\n",
        "# Define o modelo para os dados do PCA\n",
        "model_pca = return_compiled_model(X_pca_train)\n",
        "\n",
        "\n",
        "\n",
        "# treina o modelo com os dados normais\n",
        "model_normal.fit(X_normal_train, y_train, epochs=200, batch_size=10, verbose = 0)\n",
        "\n",
        "# treina o modelo com os dados do PCA\n",
        "model_pca.fit(X_pca_train, y_train, epochs=200, batch_size=10, verbose = 0)\n",
        "\n",
        "\n",
        "\n",
        "# Faz o predict com o modelo normal\n",
        "predict_normal = model_normal.predict(X_normal_test)\n",
        "\n",
        "# Faz o predict com o modelo normal\n",
        "predict_pca = model_pca.predict(X_pca_test)\n",
        "\n",
        "\n",
        "# Desnormaliza o y_test na variável y_desnormalizado\n",
        "y_test_desnormalizado = scaler_target.inverse_transform(y_test)\n",
        "# Desnormaliza o predict na variável predict_desnormalizado\n",
        "predict_normal = scaler_target.inverse_transform(predict_normal)\n",
        "# Desnormaliza o predict na variável predict_desnormalizado\n",
        "predict_pca = scaler_target.inverse_transform(predict_pca)\n",
        "\n",
        "# Calcula o mse com a biblioteca do sklearn\n",
        "mse_normal = mean_squared_error(y_test_desnormalizado, predict_normal)\n",
        "print(\"Mean squared error Normal: \", mse_normal)\n",
        "\n",
        "mse_pca = mean_squared_error(y_test_desnormalizado, predict_pca)\n",
        "print(\"Mean squared error PCA: \", mse_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ssc-JLZzrpj"
      },
      "outputs": [],
      "source": [
        "# Cria um dataframe para ordenar os valores\n",
        "df_grafico_predict = pd.DataFrame()\n",
        "\n",
        "# Acrescenta o valor de y_test_desnormalizado na coluna real\n",
        "df_grafico_predict['real'] = y_test_desnormalizado.flatten()\n",
        "\n",
        "# Acrescenta o valor de predict_desnormalizado na coluna predict\n",
        "df_grafico_predict['predict_normal'] = predict_normal.flatten()\n",
        "\n",
        "# Acrescenta o valor de predict_desnormalizado na coluna predict\n",
        "df_grafico_predict['predict_pca'] = predict_pca.flatten()\n",
        "\n",
        "# Ordena os dados pela coluna real de forma crescente\n",
        "df_grafico_predict.sort_values(by='real', inplace = True)\n",
        "\n",
        "plt.plot(range(0,len(y_test_desnormalizado)), df_grafico_predict['real'], label = 'Real')\n",
        "plt.plot(range(0,len(predict_desnormalizado)), df_grafico_predict['predict_normal'], label = 'Predict Normal')\n",
        "plt.plot(range(0,len(predict_desnormalizado)), df_grafico_predict['predict_pca'], label = 'Predict PCA')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
