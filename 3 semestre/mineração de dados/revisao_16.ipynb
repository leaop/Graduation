{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Existem 2 tipos de Algoritmos de Machine Learning para Mineração de Dados:\n",
    "\n",
    "## 1.1 - Algoritmos Supervisionados:\n",
    "\n",
    "- Possuem uma classe alvo.\n",
    "- Servem para criar um modelo de previsão (regressão) ou classificação a partir de dados de entrada e saída conhecidos.\n",
    "- Baseiam-se em dados de treinamento rotulados para fazer previsões precisas sobre dados futuros ou não vistos.\n",
    "- Exemplos de algoritmos supervisionados incluem: regressão linear, regressão logística, máquinas de vetores de suporte (SVM), árvores de decisão, florestas aleatórias e redes neurais.\n",
    "\n",
    "## 1.2 - Algoritmos Não Supervisionados:\n",
    "\n",
    "- Não possuem uma classe alvo.\n",
    "- Servem para encontrar padrões e relacionamentos ocultos em dados sem rótulos.\n",
    "- Usam técnicas como agrupamento (clustering) e detecção de anomalias para entender a estrutura e distribuição de dados.\n",
    "- Exemplos de algoritmos não supervisionados incluem: K-means, hierarchical clustering, DBSCAN, autoencoders e ``detecção de anomalias baseada em densidade``.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 - Algoritmos Supervisionados para Classificação e Previsão (Regressão):\n",
    "\n",
    "## Classificação:\n",
    "\n",
    "- A classificação é uma tarefa de aprendizado supervisionado que usa um algoritmo para identificar a categoria a que uma nova observação pertence, com base em um conjunto de treinamento de dados que contém observações cujas categorias são conhecidas.\n",
    "- O objetivo da classificação é prever rótulos categóricos, por exemplo, se um e-mail é 'spam' ou 'não spam'.\n",
    "- Na classificação, o resultado é uma variável discreta (categórica). Exemplos incluem a identificação de espécies de flores a partir de medidas de pétalas ou a previsão se um tumor é benigno ou maligno.\n",
    "\n",
    "### Exemplos de algoritmos de classificação:\n",
    "- Logistic Regression\n",
    "- Naive Bayes\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Gradient Boosting algorithms (like XGBoost and LightGBM)\n",
    "- Support Vector Machines (SVM)\n",
    "- Nearest Neighbors\n",
    "- Neural Networks (Redes Neurais)\n",
    "\n",
    "## Previsão (Regressão):\n",
    "\n",
    "- A regressão é outro tipo de aprendizado supervisionado que prevê uma saída contínua. Isso significa que os algoritmos de regressão são usados para prever um valor numérico, como o preço de uma casa baseado em suas características, ou a provável progressão de uma doença baseada em dados de saúde do paciente.\n",
    "- O objetivo da regressão é prever valores numéricos contínuos, como o preço de uma casa.\n",
    "- Na regressão, o resultado é uma variável contínua. Exemplos incluem a previsão do tempo que levará para um software ser concluído ou a previsão do preço futuro de um produto.\n",
    "\n",
    "### Exemplos de algoritmos de regressão:\n",
    "- Linear Regression\n",
    "- Polynomial Regression\n",
    "- Support Vector Regression\n",
    "- Decision Trees\n",
    "- Random Forest\n",
    "- Gradient Boosting algorithms (like XGBoost and LightGBM)\n",
    "- Ridge Regression\n",
    "- Lasso Regression\n",
    "- Neural Networks (Redes Neurais)\n",
    "\n",
    "\n",
    "### Limitações e desafios das tarefas de classificação e regressão em mineração de dados\n",
    "Embora a classificação e regressão sejam técnicas poderosas da mineração de dados, elas também apresentam algumas limitações e desafios. Algumas das principais são:\n",
    "\n",
    "1.\t`Disponibilidade de dados`: as tarefas de classificação e regressão exigem uma quantidade significativa de dados de treinamento para construir modelos confiáveis e precisos. Se os dados disponíveis são limitados ou incompletos, pode ser difícil criar um modelo com alta acurácia.\n",
    "\n",
    "2.\t`Overfitting`: é o fenômeno em que o modelo é ajustado em excesso aos dados de treinamento, tornando-o menos preciso ao fazer previsões em dados desconhecidos. Isso pode ocorrer quando o modelo é muito específico para os dados de treinamento e não consegue capturar as variações dos dados novos e desconhecidos.\n",
    "\n",
    "3.\t`Desequilíbrio de classes`: algumas tarefas de classificação podem ter um desequilíbrio nas classes, o que significa que uma classe pode ter muito mais exemplos do que outra. Isso pode levar a um viés do modelo em favor da classe majoritária e a uma baixa precisão na previsão da classe minoritária.\n",
    "\n",
    "# Importância da Normalização\n",
    "\n",
    "## Classificação:\n",
    "\n",
    "- **Escalas Uniformes**: Os recursos podem ter diferentes escalas. Por exemplo, a idade varia de 0 a 100, enquanto os salários podem variar de mil a milhões. Os algoritmos de Machine Learning geralmente não funcionam bem com recursos de escalas variadas.\n",
    "\n",
    "- **Velocidade de Convergência**: Muitos algoritmos de Machine Learning usam métodos de gradiente para encontrar o mínimo da função de perda. Se os recursos não forem normalizados, alguns gradientes podem desaparecer e o algoritmo levará mais tempo para convergir.\n",
    "\n",
    "- **Interpretabilidade**: Quando as variáveis estão na mesma escala, podemos comparar diretamente a importância dos coeficientes para cada recurso na predição do resultado.\n",
    "\n",
    "## Regressão:\n",
    "\n",
    "- **Os mesmos pontos mencionados acima se aplicam à regressão**. Além disso, em modelos de regressão, normalmente normalizamos/escalamos o alvo (y) além das características (X) por razões adicionais:\n",
    "\n",
    "  - **Preservação de relações estatísticas**: Ao normalizar as variáveis, preservamos as relações estatísticas (como a correlação) entre as variáveis, o que pode ser crucial para certos tipos de modelos de regressão.\n",
    "\n",
    "  - **Evitar problemas numéricos**: Para modelos que usam otimização baseada em gradiente (como redes neurais), ter um alvo de escala muito grande pode levar a problemas numéricos durante o processo de otimização.\n",
    "\n",
    "  - **Garantir coerência**: Como na classificação, ter características e alvos na mesma escala pode tornar o processo de treinamento mais eficiente e os resultados mais fáceis de interpretar.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O arquivo NPZ:\n",
    "\n",
    "- É um formato de arquivo usado pelo NumPy, uma biblioteca popular em Python para computação numérica.\n",
    "- 'NPZ' significa 'NumPy Zipped'.\n",
    "- É uma maneira eficiente de armazenar múltiplos arrays NumPy em um único arquivo.\n",
    "- Pode ser usado para armazenar grandes conjuntos de dados de maneira compacta, mas acessível, para análises de dados.\n",
    "- Quando carregados, os arrays NPZ são armazenados em um objeto de tipo dicionário.\n",
    "- Permite o acesso direto a qualquer array contido no arquivo sem a necessidade de carregar o arquivo inteiro na memória, o que é útil para trabalhar com conjuntos de dados grandes.\n",
    "- O arquivo NPZ é útil para cenários onde os dados precisam ser salvos e recuperados da memória em disco.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('roupas.npz')\n",
    "\n",
    "# Podemos acessar os conjuntos de dados com:\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_val = data['X_val']\n",
    "y_val = data['y_val']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nossos dados já estão normalizados\n",
    "X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Os valores de pixel da nossa primeira imagem\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nosso input shape\n",
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna a classificação da primeira imagem de treinamento\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retorna o total de classes existentes (Valor do nosso neurônio de saída)\n",
    "len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicialização do modelo sequencial.\n",
    "model = Sequential()\n",
    "\n",
    "# Primeira camada: Converte a matriz de entrada 2D (28, 28) em um vetor 1D (784).\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "\n",
    "# Segunda camada: Camada Densa (Fully-Connected) com 128 neurônios e ativação ReLU.\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Do terceiro ao nono nível: Camadas Densas com 64 neurônios e ativação ReLU.\n",
    "for _ in range(7):\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "\n",
    "# Décima camada: Camada Densa de saída com 10 neurônios (um para cada classe) e ativação Softmax.\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Configuração da compilação do modelo, com otimizador Adam, perda de entropia cruzada categórica \n",
    "# (adequado para problemas de classificação multiclasse) e métrica de precisão.\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Configuração do EarlyStopping para monitorar a perda de validação e parar o treinamento quando \n",
    "# ela não melhora após 5 épocas (patience=5), evitando assim o overfitting.\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo nos dados de treinamento, com validação nos dados de validação, para 50 épocas, \n",
    "# em lotes de 128 amostras e usando o callback de EarlyStopping.\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=50, \n",
    "                    batch_size=128, \n",
    "                    callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotando a acurácia\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plotando a perda\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsões do modelo para o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "class_names = ['Camiseta', 'Calça', 'Suéter', 'Vestido', 'Casaco',\n",
    "               'Sandália', 'Camisa', 'Tênis', 'Bolsa', 'Botas']\n",
    "\n",
    "# Converte os rótulos one-hot de volta para o formato de classe\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Calcula a matriz de confusão\n",
    "conf_mat = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.heatmap(conf_mat, annot=True,  xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test_classes, y_pred_classes, target_names=class_names))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('notebooks.csv')\n",
    "# Exibe a quantidade de valores núlos \n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista as colunas de valores textuais\n",
    "colunas_textuais = df.select_dtypes(include='object').columns.tolist()\n",
    "colunas_textuais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop que percorre todas colunas textuais e transforma em valores numéricos\n",
    "le = LabelEncoder()\n",
    "for col in colunas_textuais:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select_dtypes(include='object').columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliza X e y usando MinMaxScaler\n",
    "X = df.drop('valor', axis = 1)\n",
    "y = df[['valor']]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# Separa X e y em conjuntos de treinamento (70%), validação (15%) e teste (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa um modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Adiciona a camada de entrada\n",
    "# A camada de entrada requer a forma de entrada (input_shape) dos dados. Neste caso, é (28, 28).\n",
    "model.add(Dense(128, activation='relu', input_shape=(15, )))\n",
    "\n",
    "# Adiciona camadas ocultas\n",
    "# Estamos usando a função de ativação 'relu', que é comum em redes neurais porque ajuda a mitigar o problema do desaparecimento do gradiente.\n",
    "for _ in range(8):\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Adiciona a camada de saída\n",
    "# Como estamos resolvendo um problema de regressão, usamos a função de ativação 'linear'.\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compila o modelo\n",
    "# O otimizador Adam é usado por causa de sua eficiência.\n",
    "# Estamos usando o erro quadrático médio (mean squared error) como função de perda, que é comum para problemas de regressão.\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Configuração de Early Stopping\n",
    "# A paciência é o número de épocas para esperar para ver se a perda de validação melhora. \n",
    "# Estamos usando 'val_loss' como a quantidade a ser monitorada, portanto, o treinamento parará se não houver melhoria na perda de validação após 3 épocas consecutivas.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina o modelo\n",
    "# O número de épocas é o número de vezes que o algoritmo de aprendizado verá todo o conjunto de dados.\n",
    "# Ajustamos o tamanho do batch para 32, o que significa que o modelo atualiza os pesos após ver 32 exemplos.\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotando a acurácia\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desnormalização de Dados\n",
    "\n",
    "Ao fazer previsões com um modelo de regressão, normalmente desejamos interpretar os resultados no mesmo domínio que os dados originais. Por exemplo, se estivéssemos prevendo preços de casas, gostaríamos de ver os preços em dólares, não como um número entre 0 e 1.\n",
    "\n",
    "Se normalizamos nossos dados antes do treinamento (o que é comum e muitas vezes benéfico), as previsões que recebemos do modelo também estarão no domínio normalizado. Para interpretar essas previsões, precisamos \"desnormalizá-las\" ou convertê-las de volta ao domínio original.\n",
    "\n",
    "## Por que desnormalizar?\n",
    "\n",
    "Desnormalizar os dados é uma etapa importante para interpretar os resultados, especialmente se você estiver comunicando seus resultados para uma audiência que não está familiarizada com o conceito de normalização. Também é útil se você estiver comparando os resultados com outras previsões ou dados não normalizados.\n",
    "\n",
    "É importante lembrar que a desnormalização não altera o desempenho do modelo; é apenas para facilitar a interpretação dos resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Fazendo previsões com o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Desnormalizando as previsões e os valores verdadeiros\n",
    "y_pred_orig = scaler.inverse_transform(y_pred)\n",
    "y_test_orig = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculando as métricas\n",
    "mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "rmse = mean_squared_error(y_test_orig, y_pred_orig, squared=False)\n",
    "r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('R² Score:', r2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas de Avaliação para Regressão\n",
    "---\n",
    "1. **Erro Quadrático Médio (Mean Squared Error - MSE)**: É a média dos erros ao quadrado. O MSE é calculado tomando a média das diferenças quadráticas entre a previsão e o valor verdadeiro. É útil para destacar grandes erros (porque eles são elevados ao quadrado). No entanto, seu valor pode ser difícil de interpretar porque ele está em unidades quadradas do rótulo.\n",
    "- **Valores bons e ruins**: Como o MSE é um erro, valores mais baixos são melhores. Um MSE de 0 indica que as previsões do modelo são perfeitas. Quanto maior o MSE, pior o modelo. O que é considerado um \"bom\" MSE pode variar dependendo do contexto.\n",
    "---\n",
    "2. **Erro Absoluto Médio (Mean Absolute Error - MAE)**: É a média dos valores absolutos dos erros. O MAE é menos sensível a outliers em comparação com o MSE porque ele não eleva os erros ao quadrado. Ele mede a magnitude média dos erros em um conjunto de previsões, sem considerar sua direção.\n",
    "- **Valores bons e ruins**: Assim como o MSE, valores mais baixos de MAE são melhores e um MAE de 0 indica previsões perfeitas. O que é considerado um \"bom\" MAE pode variar dependendo do contexto.\n",
    "---\n",
    "3. **Raiz do Erro Quadrático Médio (Root Mean Squared Error - RMSE)**: É a raiz quadrada da média dos erros ao quadrado. É uma medida de erro mais popular porque o erro é expresso nas mesmas unidades que a variável de resposta.\n",
    "- **Valores bons e ruins**: RMSE segue a mesma regra que o MSE e MAE, onde valores mais baixos são melhores e um valor de 0 é o ideal. O RMSE é especialmente útil porque ele tem a mesma unidade que a variável dependente, tornando-o mais fácil de interpretar do que o MSE.\n",
    "---\n",
    "4. **Coeficiente de Determinação (R² Score)**: É uma estatística que fornece alguma informação sobre a qualidade do ajuste de um modelo. Em regressão, o R² coeficiente de determinação é uma medida estatística que indica a proporção da variação na variável dependente que é previsível a partir da(s) variável(eis) independente(s).\n",
    "- **Valores bons e ruins**: Ao contrário das métricas de erro acima, para o R², valores mais altos são melhores. Um R² de 1 indica que as variáveis independentes explicam toda a variação na variável dependente, enquanto um R² de 0 indica que as variáveis independentes não explicam nada. Em geral, um R² maior indica um modelo melhor, mas mesmo um modelo com um R² alto pode não ser bom se estiver superajustando os dados.\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-processamento de Dados\n",
    "\n",
    "## Codificação de Variáveis Categóricas\n",
    "\n",
    "Muitos algoritmos de aprendizado de máquina requerem que as entradas sejam numéricas. Isto significa que os valores categóricos, geralmente armazenados como texto, devem ser convertidos em números antes de poderem ser usados como entrada para um modelo. Aqui estão algumas razões para isso:\n",
    "\n",
    "- **Compreensão do modelo**: Os algoritmos de aprendizado de máquina entendem os números e realizam cálculos matemáticos. Textos ou categorias não podem ser diretamente utilizados em equações matemáticas.\n",
    "- **Eficiência e desempenho**: O processamento de texto é computacionalmente mais caro do que o processamento de números. Portanto, a conversão de categorias em números pode aumentar a velocidade e eficiência do algoritmo.\n",
    "\n",
    "Para converter variáveis categóricas em números, usamos codificadores como o `LabelEncoder` do `sklearn`, que associa a cada categoria única um número inteiro.\n",
    "\n",
    "## Tratamento de Valores Nulos\n",
    "\n",
    "Os valores nulos em um conjunto de dados podem levar a resultados imprecisosos ou enganosos ao treinar modelos de aprendizado de máquina. Aqui estão algumas estratégias para lidar com eles:\n",
    "\n",
    "- **Excluir linhas ou colunas**: Se uma coluna tem mais de 50% de valores nulos, pode ser melhor excluí-la, pois ele pode não ter informações suficientes para contribuir para a previsão. O mesmo vale para linhas com menos de 1% de valores nulos.\n",
    "\n",
    "- **Preencher com a média, mediana ou moda**:\n",
    "  - **Média**: É o valor médio do conjunto de dados. É uma opção boa e rápida, mas é sensível a outliers.\n",
    "  - **Mediana**: É o valor do meio do conjunto de dados quando ordenado. Menos sensível a outliers do que a média, por isso é uma opção melhor se os dados estão muito distorcidos.\n",
    "  - **Moda**: É o valor que ocorre mais frequentemente no conjunto de dados. Pode ser usada para variáveis categóricas ou numéricas discretas, ou quando a média e a mediana são inapropriadas (por exemplo, com variáveis binárias).\n",
    "\n",
    "- **Preencher com um valor constante**: Em alguns casos, você pode querer preencher os valores nulos com um valor constante, como 0, se isso fizer sentido no contexto do problema.\n",
    "\n",
    "- **Usar um algoritmo de imputação**: Existem várias técnicas mais avançadas que podem prever os valores nulos com base nos outros dados disponíveis, como a imputação KNN ou a imputação por regressão.\n",
    "\n",
    "Cada situação é única, e a estratégia de tratamento de valores nulos deve ser decidida com base no entendimento do conjunto de dados e do problema específico que você está tentando resolver.\n",
    "\n",
    "`Podemos usar o df.describe para nos auxiliar na decisão`\n",
    "\n",
    "**Média vs Mediana**: A média é fortemente afetada por valores extremos (outliers). Portanto, se houver uma grande diferença entre a média e a mediana (50º percentil) e/ou um grande desvio padrão (std), isso pode indicar a presença de outliers. Neste caso, a mediana pode ser uma escolha melhor para a imputação, pois ela é mais resistente a outliers.\n",
    "\n",
    "**Moda**: A moda é mais comumente usada para variáveis categóricas ou discretas. Não podemos inferir diretamente do df.describe() se a moda seria uma boa escolha para a imputação. Entretanto, se o conjunto de dados for do tipo discreto e apresentar uma moda muito frequente, esta poderá ser usada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('notebooks_nulos.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primeiro tratamos os valores núlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() * 100 / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apaga a coluna com mais de 50% de valores núlos\n",
    "df.drop(['entradas_usb'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() * 100 / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista as colunas de valores textuais\n",
    "colunas_textuais = df.select_dtypes(include='object').columns.tolist()\n",
    "# Loop que irá percorrer todas as colunas textuais e substituir os valores núlos pela moda\n",
    "for col in colunas_textuais:\n",
    "    df.loc[df[col].isna(), col] = df[col].mode()[0]\n",
    "# Só temos as colunas marca e segmento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() * 100 / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ano', 'ram', 'duracao_bateria', 'armazenamento_hdd']].describe().loc[['std', 'mean', '50%']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ano e Ram possuem um \"pequeno\" desvio padrão e a média e a mediana estão próximas, logo, podemos usar a média para substituir os valores núlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['ano'].isna(), 'ano'] = df['ano'].mean()\n",
    "df.loc[df['ram'].isna(), 'ram'] = df['ram'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "duracao_bateria e armazenamento_hdd possuem um desvio padrão um pouco maior e a média e mediana um tanto quanto distantes, logo, iremos usar a mediana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['duracao_bateria'].isna(), 'duracao_bateria'] = df['duracao_bateria'].median()\n",
    "df.loc[df['armazenamento_hdd'].isna(), 'armazenamento_hdd'] = df['armazenamento_hdd'].median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agora convertemos valores textuais para numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "for col in colunas_textuais:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como estamos querendo prever o valor, significa que temos uma classe alvo, logo, temos um algoritmo supervisionado, com X sendo os nossos atributos (colunas previsoras) e y sendo nossa classe alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('valor', axis = 1)\n",
    "y = df[['valor']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ser uma regressão, iremos normalizar o X e o y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Separa X e y em conjuntos de treinamento (70%), validação (15%) e teste (15%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa um modelo sequencial\n",
    "model = Sequential()\n",
    "\n",
    "# Adiciona a camada de entrada\n",
    "# A camada de entrada requer a forma de entrada (input_shape) dos dados. Neste caso, é (28, 28).\n",
    "model.add(Dense(128, activation='relu', input_shape=(14, )))\n",
    "\n",
    "# Adiciona camadas ocultas\n",
    "# Estamos usando a função de ativação 'relu', que é comum em redes neurais porque ajuda a mitigar o problema do desaparecimento do gradiente.\n",
    "for _ in range(8):\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Adiciona a camada de saída\n",
    "# Como estamos resolvendo um problema de regressão, usamos a função de ativação 'linear'.\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "# Compila o modelo\n",
    "# O otimizador Adam é usado por causa de sua eficiência.\n",
    "# Estamos usando o erro quadrático médio (mean squared error) como função de perda, que é comum para problemas de regressão.\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Configuração de Early Stopping\n",
    "# A paciência é o número de épocas para esperar para ver se a perda de validação melhora. \n",
    "# Estamos usando 'val_loss' como a quantidade a ser monitorada, portanto, o treinamento parará se não houver melhoria na perda de validação após 3 épocas consecutivas.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina o modelo\n",
    "# O número de épocas é o número de vezes que o algoritmo de aprendizado verá todo o conjunto de dados.\n",
    "# Ajustamos o tamanho do batch para 32, o que significa que o modelo atualiza os pesos após ver 32 exemplos.\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=100, \n",
    "                    batch_size=32, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotando a acurácia\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Fazendo previsões com o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# Desnormalizando as previsões e os valores verdadeiros\n",
    "y_pred_orig = scaler.inverse_transform(y_pred)\n",
    "y_test_orig = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculando as métricas\n",
    "mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "mae = mean_absolute_error(y_test_orig, y_pred_orig)\n",
    "rmse = mean_squared_error(y_test_orig, y_pred_orig, squared=False)\n",
    "r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('R² Score:', r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
