{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção 1: Introdução à Redução de Dimensionalidade\n",
    "\n",
    "### Definição de Redução de Dimensionalidade\n",
    "- **Redução de Dimensionalidade** refere-se ao processo de reduzir o número de variáveis aleatórias sob consideração, obtendo um conjunto de variáveis principais. Em machine learning, isso é frequentemente realizado para simplificar modelos e reduzir a complexidade computacional.\n",
    "\n",
    "### Por que é Importante\n",
    "1. **Simplificação de Modelos**: Modelos com menos variáveis são mais fáceis de interpretar e menos propensos a overfitting.\n",
    "2. **Melhoria de Performance**: Menor dimensionalidade pode levar a tempos de treinamento e inferência mais rápidos, bem como a uma melhor generalização do modelo.\n",
    "3. **Visualização de Dados**: Reduzir a dimensionalidade dos dados para duas ou três dimensões permite a visualização em gráficos, facilitando a compreensão das relações entre as variáveis e identificação de padrões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção 2: Principal Component Analysis (PCA)\n",
    "\n",
    "### Teoria Básica do PCA (Análise de Componentes Principais)\n",
    "\n",
    "- **O que é PCA?** \n",
    "  - Imagine que você tem muitos dados, como uma pilha de livros. O PCA ajuda a reorganizar essa pilha de forma que os livros mais importantes fiquem no topo. Aqui, cada 'livro' é uma informação sobre seus dados, e os 'mais importantes' são aqueles que contêm a maior parte da informação.\n",
    "- **Componentes Principais**: \n",
    "  - São como os resumos dos seus livros. O primeiro resumo (componente principal) tenta capturar a maior parte da história contida na sua pilha de livros. O segundo resumo tenta capturar a maior parte da história restante, mas sem repetir nada do que já foi dito no primeiro.\n",
    "- **Variação e Ortogonalidade**: \n",
    "  - Variação é o quanto de 'novidade' cada resumo (componente) traz sobre a história total. Ortogonalidade é um termo técnico que, neste caso, significa que cada resumo fala de partes diferentes da história, sem sobreposição.\n",
    "\n",
    "Essa é a ideia básica do PCA: reduzir a complexidade dos seus dados, mas tentando manter a maior parte da informação original, organizando-a de forma inteligente.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretação Geométrica e Estatística do PCA\n",
    "\n",
    "- **Interpretação Geométrica**:\n",
    "  - Imagine seus dados como um enxame de abelhas voando no espaço. Cada abelha é um ponto de dado. O PCA ajuda a encontrar a melhor lente de câmera (um novo ponto de vista) para fotografar esse enxame de modo que você possa ver o máximo possível das abelhas em uma única foto. Essa 'foto' é o novo espaço com os componentes principais como eixos.\n",
    "- **Interpretação Estatística**:\n",
    "  - Se cada abelha no enxame estivesse falando (representando uma variável), o PCA seria como um microfone direcional que tenta captar as conversas mais altas e distintas. Isso se relaciona com a análise de quão relacionadas (correlacionadas) estão as 'vozes' das abelhas (variáveis) e quão alto cada grupo de abelhas está falando (covariância).\n",
    "\n",
    "Essas interpretações ajudam a entender o PCA como uma técnica para capturar a essência dos seus dados, seja visualizando-os de uma nova perspectiva ou ouvindo as partes mais importantes da história que eles contam.\n",
    "\n",
    "---\n",
    "\n",
    "### Variação Explicada e Seleção do Número de Componentes no PCA\n",
    "\n",
    "- **Variação Explicada**:\n",
    "  - Pense na 'variação explicada' como a quantidade de tesouro que cada componente principal consegue encontrar. Em outras palavras, é quanto cada componente ajuda a entender os dados. Se um componente encontra muito tesouro (informação), ele explica muito sobre os dados.\n",
    "- **Seleção do Número de Componentes**:\n",
    "  - Escolher quantos componentes principais manter é como decidir quantos detetives você quer para encontrar o tesouro nos seus dados. Você pode usar um gráfico chamado 'Scree Plot', que mostra o quanto de tesouro (informação) cada detetive (componente) encontra. Normalmente, você mantém os detetives que encontram mais tesouro e dispensa os que encontram menos.\n",
    "\n",
    "Usar a variação explicada e o Scree Plot ajuda a tomar decisões informadas sobre quantos componentes principais você deve manter para capturar a maior parte da informação nos seus dados, sem manter partes desnecessárias.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# Carregando um dataset exemplo com alta dimensionalidade\n",
    "# Neste exemplo, estamos usando o dataset 'digits', que é um conjunto de imagens de dígitos escritos à mão\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Função para exibir os valores de X\n",
    "def plot_handwritten_digit_from_array(digit_array, digit_value):\n",
    "    \"\"\"\n",
    "    Plota a imagem de um dígito escrito à mão a partir de um array de pixels.\n",
    "\n",
    "    Parâmetros:\n",
    "    digit_array (array): Array de 64 elementos representando um dígito escrito à mão.\n",
    "\n",
    "    Retorna:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Verificando se o array tem o tamanho correto\n",
    "    if len(digit_array) != 64:\n",
    "        print(\"O array deve ter 64 elementos.\")\n",
    "        return\n",
    "\n",
    "    # Redimensionando o array para 8x8 para plotagem\n",
    "    digit_image = digit_array.reshape(8, 8)\n",
    "\n",
    "    # Plotando a imagem\n",
    "    plt.figure(figsize=(2, 2))\n",
    "    plt.imshow(digit_image, cmap='gray')\n",
    "    plt.title(f'Dígito \"{digit_value}\" Escrito à Mão')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_handwritten_digit_from_array(X[4],y[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando PCA\n",
    "# Neste exemplo, vamos reduzir os dados para 2 dimensões para fins de visualização\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explicações dos Gráficos Gerados pelo PCA\n",
    "\n",
    "### 1. Visualização dos Componentes Principais\n",
    "- **Objetivo do Gráfico**:\n",
    "  - Este gráfico de dispersão mostra os dados do dataset 'digits' após a aplicação do PCA, que os reduziu para 2 dimensões.\n",
    "- **Detalhes do Gráfico**:\n",
    "  - Cada ponto no gráfico representa um dígito escrito à mão.\n",
    "  - O eixo X representa o primeiro componente principal, e o eixo Y representa o segundo componente principal.\n",
    "  - A cor de cada ponto indica o dígito real (0 a 9) que o ponto representa.\n",
    "  - Este gráfico ajuda a visualizar como os dígitos diferentes estão distribuídos no espaço de 2 dimensões.\n",
    "\n",
    "### 2. Variação Explicada por Cada Componente\n",
    "- **Objetivo do Gráfico**:\n",
    "  - O gráfico de barras mostra quanta informação (variação) cada um dos componentes principais retém do dataset original.\n",
    "- **Detalhes do Gráfico**:\n",
    "  - As barras representam a proporção da variação total do dataset que cada componente principal captura.\n",
    "  - O primeiro componente principal geralmente captura a maior parte da variação, enquanto o segundo captura uma quantidade menor.\n",
    "  - Este gráfico é útil para entender a eficácia do PCA em reduzir a dimensionalidade dos dados preservando ao mesmo tempo a maior parte da informação.\n",
    "\n",
    "---\n",
    "\n",
    "Esses gráficos são ferramentas essenciais para entender os resultados do PCA, oferecendo uma visão clara de como os dados foram transformados e quanta informação cada componente principal conseguiu reter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando os componentes principais\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.xlabel('Primeiro componente principal')\n",
    "plt.ylabel('Segundo componente principal')\n",
    "plt.colorbar()\n",
    "plt.title('Visualização dos Dados do PCA')\n",
    "plt.show()\n",
    "\n",
    "# Visualizando a variação explicada por cada componente\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(range(1, pca.n_components_ + 1), pca.explained_variance_ratio_, color='blue', label='Variação individual explicada')\n",
    "plt.ylabel('Proporção da Variação Explicada')\n",
    "plt.xlabel('Componente Principal')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Variação Explicada por Cada Componente do PCA')\n",
    "plt.show()\n",
    "\n",
    "# Notas:\n",
    "# 1. Este código aplica PCA ao dataset 'digits' e o reduz para 2 componentes principais.\n",
    "# 2. Em seguida, visualiza os dados transformados em um gráfico de dispersão, colorido por dígitos.\n",
    "# 3. Por fim, exibe um gráfico de barras mostrando a quantidade de variação explicada por cada componente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção 3: Linear Discriminant Analysis (LDA)\n",
    "\n",
    "\n",
    "### Diferenças Fundamentais entre PCA e LDA\n",
    "- **PCA (Principal Component Analysis)** é uma técnica não-supervisionada que busca a direção de máxima variação nos dados, independentemente das classes.\n",
    "- **LDA (Linear Discriminant Analysis)**, por outro lado, é uma técnica supervisionada que busca maximizar a separação entre diferentes classes. \n",
    "- Enquanto o PCA busca direções que maximizam a variação total, o LDA foca em encontrar um espaço que maximize a separação entre classes.\n",
    "\n",
    "## Teoria do LDA: Maximizando a Diferença Entre Grupos\n",
    "\n",
    "- **O que é LDA (Linear Discriminant Analysis)?**\n",
    "  - Imagine que você está organizando uma série de fotos de diferentes grupos de pessoas (classes). O LDA é como um mágico da fotografia que arranja as pessoas de modo que os grupos fiquem o mais distintos possível uns dos outros.\n",
    "\n",
    "- **Maximizando a Distinção Entre Grupos**:\n",
    "  - O LDA busca a melhor forma de arranjar as fotos para que cada grupo se destaque claramente. Ele faz isso garantindo que os grupos estejam o mais separados possível (maximizando a variância entre classes) e que as pessoas dentro de cada grupo estejam o mais próximas possível umas das outras (minimizando a variância dentro das classes).\n",
    "\n",
    "- **Encontrando os Melhores Eixos**:\n",
    "  - Matematicamente, o LDA procura linhas imaginárias (eixos) que passam pelos grupos, de forma a maximizar a distância entre os centros de cada grupo (médias das classes) e, ao mesmo tempo, manter cada grupo o mais unido possível.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Esta seção introduz o LDA como uma poderosa técnica de redução de dimensionalidade, especialmente útil em contextos onde a classificação é um objetivo chave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import load_wine\n",
    "\n",
    "# Carregando um dataset exemplo para classificação\n",
    "# Neste exemplo, estamos usando o dataset 'wine', que é um conjunto de dados de classificação\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo as Limitações de Componentes no LDA\n",
    "\n",
    "A Análise Discriminante Linear (LDA) é uma técnica poderosa de redução de dimensionalidade em Machine Learning, mas vem com uma limitação específica quanto ao número de componentes que pode extrair. Vamos entender essa limitação.\n",
    "\n",
    "## LDA e o Número Máximo de Componentes\n",
    "\n",
    "- **Limitação Fundamental**:\n",
    "  - O número máximo de componentes que o LDA pode extrair é determinado pelo menor valor entre:\n",
    "    - O número de características (features) do dataset.\n",
    "    - O número de classes menos um (`n_classes - 1`).\n",
    "\n",
    "- **Por que essa Limitação Existe?**:\n",
    "  - O LDA funciona maximizando a separação entre as classes. Matematicamente, isso é feito encontrando eixos ou direções que maximizam a distância entre os centros das classes.\n",
    "  - O número de eixos úteis para essa separação é limitado pelo número de classes que você tem menos um. Isso ocorre porque, em um espaço de `N` dimensões, você pode ter no máximo `N-1` direções que separam efetivamente `N` pontos (ou classes).\n",
    "\n",
    "## Aplicação Prática\n",
    "\n",
    "- **Exemplo com 3 Classes**:\n",
    "  - Se o dataset tem 3 classes, o número máximo de componentes que o LDA pode usar é `3 - 1 = 2`.\n",
    "  - Tentar usar um `n_components` maior do que 2 resultará em um erro, já que não é possível criar mais de 2 eixos discriminantes para 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicando PCA para comparação\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "X_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para plotar os resultados\n",
    "def plot_scatter(X, y, title, ax):\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], wine.target_names):\n",
    "        ax.scatter(X[y == i, 0], X[y == i, 1], alpha=0.8, color=color, label=target_name)\n",
    "    ax.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "    ax.title.set_text(title)\n",
    "\n",
    "# Visualizando os resultados\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "plot_scatter(X_lda, y, 'LDA: Wine Dataset', ax[0])\n",
    "plot_scatter(X_pca, y, 'PCA: Wine Dataset', ax[1])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Notas:\n",
    "# 1. Este código aplica LDA e PCA ao dataset 'wine' e os reduz para 2 componentes.\n",
    "# 2. Em seguida, visualiza os dados transformados usando LDA e PCA em gráficos de dispersão separados.\n",
    "# 3. A comparação entre LDA e PCA ajuda a entender as diferenças em como eles tratam a separabilidade das classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparando com os exemplos dos dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# Aplicando LDA\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "# Aplicando PCA para comparação\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Configurando o layout dos gráficos\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Gráfico PCA\n",
    "scatter = axs[0].scatter(X_pca[:, 0], X_pca[:, 1], c=y, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "axs[0].set_xlabel('Primeiro componente principal')\n",
    "axs[0].set_ylabel('Segundo componente principal')\n",
    "axs[0].set_title('Visualização dos Dados do PCA')\n",
    "\n",
    "# Gráfico LDA\n",
    "scatter_lda = axs[1].scatter(X_lda[:, 0], X_lda[:, 1], c=y, edgecolor='none', alpha=0.5, cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "axs[1].set_xlabel('Primeiro Discriminante Linear')\n",
    "axs[1].set_ylabel('Segundo Discriminante Linear')\n",
    "axs[1].set_title('Visualização dos Dados do LDA')\n",
    "\n",
    "# Adicionando barra de cores\n",
    "fig.colorbar(scatter, ax=axs[0], orientation='vertical')\n",
    "fig.colorbar(scatter_lda, ax=axs[1], orientation='vertical')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção 4: Considerações Finais e Melhores Práticas em Redução de Dimensionalidade\n",
    "\n",
    "## Escolhendo Entre PCA e LDA\n",
    "\n",
    "### PCA (Principal Component Analysis)\n",
    "- **Descrição**: Técnica não-supervisionada para transformar dados em componentes principais ortogonais.\n",
    "- **Uso Ideal**: Para visualização de dados, análise exploratória e casos onde as classes não são o foco.\n",
    "- **Aplicação em Machine Learning**:\n",
    "  - **Classificação e Regressão**: Útil para ambos, especialmente para reduzir a dimensionalidade e evitar overfitting.\n",
    "  - **Independência de Classes**: Não considera a separação de classes, o que pode ser limitante em certos casos de classificação.\n",
    "\n",
    "### LDA (Linear Discriminant Analysis)\n",
    "- **Descrição**: Técnica supervisionada focada em maximizar a separabilidade entre classes conhecidas.\n",
    "- **Uso Ideal**: Para tarefas de classificação onde a distinção clara entre grupos é necessária.\n",
    "- **Aplicação em Machine Learning**:\n",
    "  - **Classificação**: Altamente eficaz em classificação devido à sua natureza supervisionada e foco em separabilidade de classes.\n",
    "  - **Limitações**: Pressupõe distribuições gaussianas com covariâncias iguais entre as classes, o que pode não ser verdade em todos os datasets.\n",
    "  - **Regressão**: Pode ser adaptado para regressão ao categorizar variáveis dependentes contínuas.\n",
    "\n",
    "## Quando Escolher Cada Um?\n",
    "- **PCA**: Escolha o PCA quando a redução de dimensionalidade para visualização ou análise exploratória é o principal objetivo, independentemente das classes.\n",
    "- **LDA**: Prefira o LDA quando o foco está na maximização da separabilidade entre classes conhecidas, especialmente em problemas de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seção 5: Técnicas Famosas de Redução de Dimensionalidade em Machine Learning\n",
    "\n",
    "A redução de dimensionalidade é uma técnica crucial em Machine Learning para simplificar os dados sem perder informações essenciais. Vamos explorar algumas das técnicas mais famosas, suas aplicações e em que tipo de problemas de aprendizado elas são mais utilizadas.\n",
    "\n",
    "## PCA (Principal Component Analysis)\n",
    "- **Descrição**: Reduz a dimensionalidade transformando os dados em componentes principais.\n",
    "- **Indicação de Utilização**: Ideal para visualização de dados e pré-processamento em problemas complexos.\n",
    "- **Aplicação**: Útil tanto em problemas supervisionados (classificação e regressão) quanto em não supervisionados.\n",
    "\n",
    "## LDA (Linear Discriminant Analysis)\n",
    "- **Descrição**: Foca na maximização da separabilidade entre classes conhecidas.\n",
    "- **Indicação de Utilização**: Mais eficaz em problemas de classificação supervisionada.\n",
    "- **Aplicação**: Principalmente usada em problemas de classificação supervisionada.\n",
    "\n",
    "## t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "- **Descrição**: Técnica não-linear para redução de dimensionalidade, mantendo a estrutura local dos dados.\n",
    "- **Indicação de Utilização**: Excelente para visualização de dados de alta dimensionalidade.\n",
    "- **Aplicação**: Comumente usada em problemas não supervisionados, mas pode ser útil em etapas exploratórias de problemas supervisionados.\n",
    "\n",
    "## UMAP (Uniform Manifold Approximation and Projection)\n",
    "- **Descrição**: Semelhante ao t-SNE, mas mais rápido e escalável.\n",
    "- **Indicação de Utilização**: Boa para visualização e exploração de estruturas complexas.\n",
    "- **Aplicação**: Usada principalmente em análises não supervisionadas.\n",
    "\n",
    "## Autoencoders\n",
    "- **Descrição**: Redes neurais para aprender representações codificadas dos dados.\n",
    "- **Indicação de Utilização**: Usado em redução de dimensionalidade e aprendizado de características.\n",
    "- **Aplicação**: Principalmente em problemas não supervisionados, mas também pode ser útil como pré-processamento em problemas supervisionados.\n",
    "\n",
    "## Feature Selection Techniques\n",
    "- **Descrição**: Métodos para selecionar um subconjunto relevante de variáveis.\n",
    "- **Indicação de Utilização**: Para reduzir a dimensionalidade preservando variáveis importantes.\n",
    "- **Aplicação**: Aplicável tanto em problemas supervisionados (classificação e regressão) quanto não supervisionados.\n",
    "\n",
    "## Isomap (Isometric Mapping)\n",
    "- **Descrição**: Técnica não-linear que preserva distâncias geodésicas entre pontos.\n",
    "- **Indicação de Utilização**: Útil para dados com estrutura não-linear complexa.\n",
    "- **Aplicação**: Mais comumente utilizada em análises não supervisionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('saude_fetal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicação de PCA ou LDA em Processos de Classificação em Machine Learning\n",
    "\n",
    "O uso de técnicas como PCA (Principal Component Analysis) e LDA (Linear Discriminant Analysis) é uma etapa fundamental em muitos fluxos de trabalho de Machine Learning, especialmente em tarefas de classificação. Vamos explorar em que momento do processo elas devem ser aplicadas e por que.\n",
    "\n",
    "## Processo de Classificação: Passos Iniciais\n",
    "1. **Separação de X e y**: A primeira etapa envolve separar os recursos (X) dos rótulos ou classes (y) no seu conjunto de dados.\n",
    "2. **Divisão em Treinamento, Validação e Teste**: Em seguida, divide-se o dataset em conjuntos de treinamento, validação e teste para garantir que o modelo possa ser treinado, validado e testado de forma eficaz.\n",
    "\n",
    "## Aplicação de PCA ou LDA\n",
    "- **Momento Ideal para Aplicação**:\n",
    "  - PCA ou LDA devem ser aplicados **após a divisão dos dados** em conjuntos de treinamento, validação e teste, mas **antes da normalização dos dados**.\n",
    "- **Por que Após a Divisão?**:\n",
    "  - Essencialmente, a redução de dimensionalidade deve ser feita de maneira a evitar o vazamento de informações do conjunto de teste para o modelo durante o treinamento.\n",
    "  - Ao aplicar PCA ou LDA após a divisão, você garante que a transformação dos dados seja realizada de forma independente nos conjuntos de treinamento, validação e teste.\n",
    "\n",
    "## Processo de Aplicação\n",
    "- **PCA ou LDA no Conjunto de Treinamento**:\n",
    "  - Use `fit_transform` no conjunto de treinamento. Isso garante que o PCA ou LDA aprenda e aplique a redução de dimensionalidade com base apenas nos dados de treinamento.\n",
    "- **PCA ou LDA nos Conjuntos de Validação e Teste**:\n",
    "  - Aplique `transform` nos conjuntos de validação e teste. Isso utiliza os parâmetros aprendidos com o conjunto de treinamento para aplicar a mesma transformação, sem reaprender ou ajustar com base nesses conjuntos.\n",
    "\n",
    "## Justificativa\n",
    "- **Evitar Vazamento de Dados**: Aplicando a redução de dimensionalidade após a divisão e usando `fit_transform` apenas no treinamento, evita-se o risco de vazamento de informações do conjunto de teste.\n",
    "- **Consistência nas Transformações**: Ao usar `transform` nos conjuntos de validação e teste, garante-se que as transformações sejam consistentes e comparáveis entre os diferentes conjuntos de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('saude_fetal', axis = 1).values\n",
    "y = df[['saude_fetal']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separa 70% dos dados para treinamento\n",
    "X_train, X_temp , y_train, y_temp = train_test_split(X, y, test_size= 0.3)\n",
    "\n",
    "# Separa igualmente 50% de 30% (15% do total) para validação e teste\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica LDA nos dados\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train, y_train)\n",
    "X_test_lda = lda.transform(X_test)\n",
    "X_val_lda = lda.transform(X_val)\n",
    "\n",
    "\n",
    "# Aplica PCA nos dados\n",
    "pca = PCA(n_components=6)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_val_pca = pca.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converter os valores de y para categóricos\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Conversão dos rótulos para o formato one-hot\n",
    "y_train = to_categorical(y_train, num_classes=3)\n",
    "y_test = to_categorical(y_test, num_classes=3)\n",
    "y_val = to_categorical(y_val, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_X_lda = MinMaxScaler()\n",
    "scaler_X_pca = MinMaxScaler()\n",
    "\n",
    "X_train = scaler_X.fit_transform(X_train) \n",
    "X_val = scaler_X.transform(X_val) \n",
    "X_test = scaler_X.transform(X_test) \n",
    "\n",
    "X_train_lda  = scaler_X_lda .fit_transform(X_train_lda ) \n",
    "X_val_lda  = scaler_X_lda .transform(X_val_lda ) \n",
    "X_test_lda  = scaler_X_lda .transform(X_test_lda ) \n",
    "\n",
    "X_train_pca = scaler_X_pca.fit_transform(X_train_pca) \n",
    "X_val_pca = scaler_X_pca.transform(X_val_pca) \n",
    "X_test_pca = scaler_X_pca.transform(X_test_pca) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import clear_output\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def criar_modelo(X_train, n_classes):\n",
    "    \"\"\"\n",
    "    Cria e compila um modelo de Machine Learning com 5 camadas ocultas.\n",
    "\n",
    "    Parâmetros:\n",
    "    X_train (array): Dados de treinamento.\n",
    "    y_train (array): Rótulos de treinamento.\n",
    "\n",
    "    Retorna:\n",
    "    model: Modelo de Machine Learning compilado.\n",
    "    \"\"\"\n",
    "    n_features = X_train.shape[1]  # Número de características\n",
    "\n",
    "    # Criando o modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, activation='relu', input_shape=(n_features,)))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(n_classes, activation='softmax'))  # Camada de saída\n",
    "\n",
    "    # Compilando o modelo\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def treinar_modelo(model, X_train, y_train, X_val, y_val, epochs = 300, early_stopping = 50):\n",
    "    \"\"\"\n",
    "    Treina um modelo de Machine Learning e exibe gráficos de desempenho.\n",
    "\n",
    "    Parâmetros:\n",
    "    model: Modelo de Machine Learning.\n",
    "    X_train (array): Dados de treinamento.\n",
    "    y_train (array): Rótulos de treinamento.\n",
    "    X_val (array): Dados de validação.\n",
    "    y_val (array): Rótulos de validação.\n",
    "    epochs (int): Número de épocas para treinamento.\n",
    "    early_stopping (bool): Se True, aplica Early Stopping.\n",
    "\n",
    "    Retorna:\n",
    "    model: Modelo de Machine Learning treinado.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    \n",
    "    early_stopping_callback = EarlyStopping(monitor='val_loss', patience=early_stopping, restore_best_weights=True)\n",
    "    callbacks = [early_stopping_callback]\n",
    "\n",
    "    # Treinando o modelo\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, verbose=1, callbacks=callbacks)\n",
    "    clear_output(wait=False)\n",
    "\n",
    "    # Plotando gráficos de loss e acurácia\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Gráfico de Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Loss de Treinamento')\n",
    "    plt.plot(history.history['val_loss'], label='Loss de Validação')\n",
    "    plt.title('Gráfico de Loss')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Gráfico de Acurácia\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Acurácia de Treinamento')\n",
    "    plt.plot(history.history['val_accuracy'], label='Acurácia de Validação')\n",
    "    plt.title('Gráfico de Acurácia')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Acurácia')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modelo para os dados normais\n",
    "modelo = criar_modelo(X_train, 3)\n",
    "\n",
    "# Define o modelo para os dados de LDA\n",
    "modelo_lda = criar_modelo(X_train_lda, 3)\n",
    "\n",
    "# Define o modelo para os dados de PCA\n",
    "modelo_pca = criar_modelo(X_train_pca, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quantidade de épocas\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define modelo para os dados normais\n",
    "modelo = treinar_modelo(modelo, X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o modelo para os dados de LDA\n",
    "modelo_lda = treinar_modelo(modelo_lda, X_train_lda, y_train, X_val_lda, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o modelo para os dados de PCA\n",
    "modelo_pca = treinar_modelo(modelo_pca, X_train_pca, y_train, X_val_pca, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def metricas_classificacao(y_real_, y_pred_, nome):\n",
    "    y_real = [np.argmax(x) for x in y_real_]\n",
    "    y_pred = [np.argmax(x) for x in y_pred_]\n",
    "\n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        \"Acurácia\": accuracy_score(y_real, y_pred),\n",
    "        \"Precisão (macro)\": precision_score(y_real, y_pred, average='macro', zero_division=0),\n",
    "        \"Recall (macro)\": recall_score(y_real, y_pred, average='macro', zero_division=0),\n",
    "        \"F1-Score (macro)\": f1_score(y_real, y_pred, average='macro', zero_division=0)\n",
    "    }\n",
    "    clear_output(wait=False)\n",
    "    print(f\"{nome}\\n\")\n",
    "    # Printar métricas\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Calcular a Matriz de Confusão\n",
    "    confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "    # Printar Matriz de Confusão\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    sns.heatmap(confusion_mat, annot=True, cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel('Previsto')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = modelo.predict(X_test)\n",
    "metricas_classificacao(y_test, y_pred, 'Normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_pca = modelo_pca.predict(X_test_pca)\n",
    "metricas_classificacao(y_test, y_pred_pca, 'PCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lda = modelo_lda.predict(X_test_lda)\n",
    "metricas_classificacao(y_test, y_pred_lda, 'LDA')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
