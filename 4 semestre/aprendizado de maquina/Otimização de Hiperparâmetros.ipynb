{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução à Otimização de Hiperparâmetros"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos em Aprendizado de Máquina\n",
    "No mundo do aprendizado de máquina, um modelo é uma representação de um sistema. Ele é treinado usando um conjunto de dados e, com base nesse treinamento, faz previsões ou toma decisões sem ser explicitamente programado para realizar uma determinada tarefa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Criando um modelo de Regressão Logística\n",
    "modelo = LogisticRegression()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parâmetros vs Hiperparâmetros em Aprendizado de Máquina\n",
    "Em aprendizado de máquina, o processo de treinamento envolve ajustar os parâmetros do modelo para melhor se adequar aos dados. \n",
    "\n",
    "Em contraste, os hiperparâmetros são configurações que influenciam como esse treinamento é realizado."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetros:\n",
    "O que são? São componentes do modelo que são aprendidos diretamente dos dados durante o treinamento.\n",
    "\n",
    "Exemplo: Nos modelos de redes neurais, os parâmetros são os pesos e biases (viés, um termo que adiciona um valor constante para ajudar o modelo a melhor se ajustar aos dados) que são ajustados através do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando dataset do sklearn\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Carregando o dataset de dígitos\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando uma rede neural simples\n",
    "model = Sequential()\n",
    "model.add(Dense(4, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(2, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Compilando e treinando o modelo\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando alguns parâmetros (pesos) da primeira camada\n",
    "weights_first_layer = model.layers[0].get_weights()[0]\n",
    "print(\"Pesos da primeira camada:\\n\", weights_first_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_first_layer.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hiperparâmetros:\n",
    "O que são? São configurações que definem aspectos do treinamento e da estrutura do modelo. Não são aprendidos nos dados, mas são configurados antes do treinamento.\n",
    "\n",
    "Exemplo: Em redes neurais, a taxa de aprendizado, o número de camadas, o número de neurônios em uma camada, e a função de ativação são todos hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo hiperparâmetros para uma rede neural\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "number_of_neurons_layer1 = 128\n",
    "number_of_neurons_layer2 = 64\n",
    "number_of_neurons_layer3 = 128\n",
    "activation_function = 'relu'\n",
    "\n",
    "# Criando um modelo usando os hiperparâmetros\n",
    "model = Sequential()\n",
    "model.add(Dense(number_of_neurons_layer1, input_dim=X_train.shape[1], activation=activation_function))\n",
    "model.add(Dense(number_of_neurons_layer2, activation=activation_function))\n",
    "model.add(Dense(number_of_neurons_layer3, activation=activation_function))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Treinando o modelo usando os hiperparâmetros\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`learning_rate` = 0.001: Taxa de aprendizado é um hiperparâmetro que define o tamanho do passo que será usado no ajuste dos pesos durante o treinamento. Embora seja definido aqui, note que ele não está sendo usado diretamente no código fornecido, pois o otimizador 'adam' tem sua própria taxa de aprendizado adaptativa.\n",
    "\n",
    "`batch_size` = 32: Tamanho do lote (ou batch) é a quantidade de amostras de dados que serão usadas para atualizar os pesos de uma única vez. Esse é um hiperparâmetro importante quando você está usando um método de otimização estocástico ou mini-batch.\n",
    "\n",
    "`epochs` = 50: Número de épocas indica quantas vezes o algoritmo verá o conjunto de treinamento completo. Se você tem, digamos, 1000 amostras de treinamento e o batch_size é 100, então uma época terá 10 atualizações (passos) para o modelo.\n",
    "\n",
    "`number_of_neurons_layer1` = 128 e `number_of_neurons_layer2` = 64 e `number_of_neurons_layer3` = 128: Estes definem o número de neurônios nas camadas ocultas da rede neural. A arquitetura e a profundidade da rede (número de camadas e neurônios em cada camada) são hiperparâmetros.\n",
    "\n",
    "`activation_function` = 'relu': Função de ativação que é usada nos neurônios da rede. Dependendo da função escolhida, a rede pode aprender diferentes tipos de representações. O 'relu' (Rectified Linear Unit) é um dos mais populares atualmente.\n",
    "\n",
    "`loss` = 'sparse_categorical_crossentropy': Define a função de perda, que é a métrica que a rede tentará minimizar durante o treinamento.\n",
    "\n",
    "`optimizer` = 'adam': Define o otimizador usado, que é o algoritmo de otimização. Existem vários otimizadores e cada um tem seus próprios hiperparâmetros internos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão:\n",
    "\n",
    "``Parâmetros`` são intrínsecos ao modelo e são ajustados durante o treinamento.\n",
    "\n",
    "``Hiperparâmetros`` definem como o treinamento é realizado e a estrutura do modelo, sendo definidos externamente e não ajustados automaticamente pelo modelo durante o treinamento."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Crucialidade da Otimização de Hiperparâmetros\n",
    "Hiperparâmetros adequados podem fazer a diferença entre um modelo médio e um modelo altamente eficiente. A otimização de hiperparâmetros:\n",
    "\n",
    "- Melhora o desempenho do modelo no conjunto de dados.\n",
    "- Pode ajudar a prevenir overfitting ou underfitting.\n",
    "- Pode acelerar o treinamento do modelo ou torná-lo mais eficaz.\n",
    "\n",
    "Encontrar os hiperparâmetros ideais muitas vezes é um desafio. Se pegarmos, por exemplo, uma rede neural, a taxa de aprendizado, o tamanho do batch, a quantidade de camadas e unidades em cada camada, entre outros, são hiperparâmetros que precisamos definir antes do treinamento. A combinação certa desses hiperparâmetros pode variar amplamente de um conjunto de dados para outro, e a otimização automática desses hiperparâmetros é onde podemos extrair o máximo do potencial de um modelo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de Otimização de Hiperparâmetros\n",
    "Após a introdução sobre a importância dos hiperparâmetros, agora nos deparamos com um questionamento: \"Como podemos encontrar o melhor conjunto de hiperparâmetros para nosso modelo?\". Existem diversos métodos que auxiliam nessa busca, e hoje, iremos explorar três dos mais populares."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "O Grid Search, ou Pesquisa em Grade, é um dos métodos mais simples e amplamente utilizados para otimização de hiperparâmetros.\n",
    "\n",
    "O método Grid Search, ou busca em grade, consiste em testar manualmente cada combinação possível de hiperparâmetros. Por exemplo, se tivermos dois hiperparâmetros e quisermos testar 5 valores diferentes para cada um, realizaríamos um total de 25 treinamentos (5x5).\n",
    "\n",
    "``Vantagens``:\n",
    "\n",
    "É determinístico; você sabe exatamente quais combinações serão testadas.\n",
    "Pode ser paralelizado facilmente.\n",
    "\n",
    "``Desvantagens``:\n",
    "\n",
    "Pode ser muito demorado, especialmente quando o espaço de busca é grande.\n",
    "Não é eficiente, pois testa combinações que podem ser consideradas subótimas com base em resultados anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def metricas_classificacao(y_real, y_pred):\n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        \"Acurácia\": accuracy_score(y_real, y_pred),\n",
    "        \"Precisão (macro)\": precision_score(y_real, y_pred, average='macro'),\n",
    "        \"Recall (macro)\": recall_score(y_real, y_pred, average='macro'),\n",
    "        \"F1-Score (macro)\": f1_score(y_real, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    # Printar métricas\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    # Calcular a Matriz de Confusão\n",
    "    confusion_mat = confusion_matrix(y_real, y_pred)\n",
    "\n",
    "    # Printar Matriz de Confusão\n",
    "    print(\"Matriz de Confusão:\")\n",
    "    sns.heatmap(confusion_mat, annot=True, cmap='YlGnBu', fmt='g')\n",
    "    plt.xlabel('Previsto')\n",
    "    plt.ylabel('Real')\n",
    "    plt.show()\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias para construir o modelo da rede neural\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Função que cria e retorna um modelo de rede neural\n",
    "def create_model(neurons1=128, neurons2=64, activation_function='relu'):\n",
    "    \n",
    "    # Inicializando o modelo sequencial\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Adicionando a primeira camada oculta com o número de neurônios especificado (neurons1)\n",
    "    # 'input_dim' especifica o número de características de entrada\n",
    "    model.add(Dense(neurons1, input_dim=X_train.shape[1], activation=activation_function))\n",
    "    \n",
    "    # Adicionando a segunda camada oculta com o número de neurônios especificado (neurons2)\n",
    "    model.add(Dense(neurons2, activation=activation_function))\n",
    "    \n",
    "    # Adicionando a terceira camada oculta com 128 neurônios\n",
    "    model.add(Dense(128, activation=activation_function))\n",
    "    \n",
    "    # Adicionando a camada de saída com 10 neurônios (para um problema de classificação de 10 classes)\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    # Compilando o modelo - especificando o otimizador, a função de perda e a métrica de avaliação\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando o KerasClassifier que permite usar modelos Keras com scikit-learn\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# Importando GridSearchCV para otimização de hiperparâmetros\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Criando uma instância do KerasClassifier com a função de criação do modelo e especificando 20 épocas\n",
    "model_for_grid = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n",
    "\n",
    "# Definindo a grade de hiperparâmetros que queremos testar\n",
    "param_grid = {\n",
    "    'neurons1': [64, 128, 256],                # diferentes quantidades de neurônios para a primeira camada\n",
    "    'neurons2': [64, 128, 256],                # diferentes quantidades de neurônios para a segunda camada\n",
    "    'activation_function': ['relu', 'tanh']  # diferentes funções de ativação\n",
    "}\n",
    "\n",
    "# Configurando o GridSearchCV com o modelo, a grade de parâmetros, e outras opções\n",
    "grid = GridSearchCV(estimator=model_for_grid, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "\n",
    "# Iniciando a busca pelos melhores hiperparâmetros usando o conjunto de treinamento\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados\n",
    "print(f\"Melhores parâmetros: {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * 3 * 2 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = create_model(neurons1=128, neurons2=128, activation_function='relu')\n",
    "best_model.fit(X_train, y_train, epochs=20, verbose = 0)\n",
    "y_pred = np.argmax(best_model.predict(X_test), axis=1)\n",
    "metricas = metricas_classificacao(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(neurons1=64, neurons2=64, activation_function='relu')\n",
    "model.fit(X_train, y_train, epochs=20, verbose = 0)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "metricas = metricas_classificacao(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search\n",
    "\n",
    "Como difere do Grid Search:\n",
    "\n",
    "Em vez de testar todas as combinações possíveis, o Random Search testa uma quantidade determinada de combinações aleatórias de hiperparâmetros. Em nosso exemplo anterior de 3x3x2 combinações, em vez de testar todas as 18 combinações, podemos testar, digamos, 10 combinações aleatórias. Isso pode ser mais eficiente quando se tem um espaço de pesquisa muito grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Definindo o espaço de hiperparâmetros\n",
    "param_dist = {\n",
    "    'neurons1': [64, 128, 256, 512],\n",
    "    'neurons2': [64, 128, 256, 512],\n",
    "    'activation_function': ['relu', 'tanh', 'softmax']\n",
    "}\n",
    "\n",
    "model_for_grid = KerasClassifier(build_fn=create_model, epochs=20, verbose=0)\n",
    "\n",
    "# Configurando o RandomizedSearchCV com 10 iterações\n",
    "random_search = RandomizedSearchCV(estimator=model_for_grid, param_distributions=param_dist, n_iter=10, n_jobs=-1, cv=3)\n",
    "\n",
    "# Iniciando a busca pelos melhores hiperparâmetros usando o conjunto de treinamento\n",
    "random_result = random_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimindo os melhores hiperparâmetros encontrados\n",
    "print(f\"Melhores parâmetros usando RandomizedSearchCV: {random_result.best_params_}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao invés de testarmos 4x4x3 = 48 combinações , testamos 10 combinações aleatórias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(neurons1=128, neurons2=256, activation_function='relu')\n",
    "model.fit(X_train, y_train, epochs=20, verbose = 0)\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "metricas = metricas_classificacao(y_test, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização Bayesiana com BayesianOptimization\n",
    "A otimização bayesiana é uma técnica probabilística para encontrar o mínimo de funções. Diferente do Random Search, que faz uma busca aleatória pelo espaço de hiperparâmetros, a otimização bayesiana tenta racionalizar a melhor área do espaço a ser pesquisada com base em avaliações anteriores. Ela usa um processo gaussiano para fazer isso.\n",
    "\n",
    "### Funcionamento Básico:\n",
    "Modelo probabilístico: Um modelo probabilístico é construído com base nas funções e seus parâmetros avaliados anteriormente. Esse modelo é frequentemente um processo gaussiano.\n",
    "\n",
    "Escolha do próximo ponto: Com base no modelo atual, escolhe-se o próximo ponto para avaliar. Isso não é apenas baseado em áreas onde a performance é boa, mas também onde a incerteza é alta. Assim, equilibra a exploração de novas áreas e a exploração de áreas conhecidas.\n",
    "\n",
    "### Aplicação em Machine Learning:\n",
    "Para otimização de hiperparâmetros, a função que queremos minimizar (ou maximizar) é geralmente a métrica de erro (ou acurácia) do nosso modelo. Por exemplo, se estamos treinando uma rede neural, a função tomará hiperparâmetros como entrada (como taxa de aprendizado, número de neurônios, etc.) e retornará o erro no conjunto de validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bayesian-optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Função Objetivo``: Precisamos definir a função que queremos otimizar. No nosso caso, essa função irá:\n",
    "- Receber hiperparâmetros como entrada.\n",
    "- Construir e treinar um modelo usando esses hiperparâmetros.\n",
    "- Retornar a métrica de erro (ou acurácia).\n",
    "\n",
    "``Limites dos Hiperparâmetros``: A otimização bayesiana requer que os hiperparâmetros tenham limites. Se for um hiperparâmetro contínuo (como a taxa de aprendizado), simplesmente definimos um intervalo. Para hiperparâmetros categóricos (como funções de ativação), usamos um truque: mapeamos cada categoria para um número (por exemplo, 'relu' para 0, 'tanh' para 1) e depois usamos esse índice como hiperparâmetro contínuo. Depois, na função objetivo, mapeamos o índice de volta à sua categoria original.\n",
    "\n",
    "``Otimização``: Com a função objetivo e os limites definidos, podemos executar a otimização. Decidimos quantos pontos iniciais queremos (pontos escolhidos aleatoriamente antes de começar a otimização bayesiana) e quantas iterações de otimização queremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando a biblioteca necessária\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Definindo a função objetivo que queremos otimizar.\n",
    "# Esta função treina um modelo com hiperparâmetros fornecidos e retorna a acurácia de validação.\n",
    "def objective_function(neurons1, neurons2, activation_index):\n",
    "    \n",
    "    # Mapeia os índices para suas respectivas funções de ativação.\n",
    "    activation_functions = ['relu', 'tanh']\n",
    "    activation_function = activation_functions[int(activation_index)]\n",
    "    \n",
    "    # Cria e compila o modelo usando os hiperparâmetros fornecidos.\n",
    "    model = create_model(int(neurons1), int(neurons2), activation_function)\n",
    "    \n",
    "    # Treina o modelo e obtém o histórico de treinamento.\n",
    "    history = model.fit(X_train, y_train, epochs=20, verbose=0, validation_split=0.2)\n",
    "    \n",
    "    # Obtém a acurácia de validação da última época.\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    return val_accuracy\n",
    "\n",
    "# Definindo os limites dos hiperparâmetros para a otimização bayesiana.\n",
    "# Os hiperparâmetros contínuos têm intervalos definidos (por exemplo, neurons1 entre 64 e 256).\n",
    "# Para hiperparâmetros categóricos, usamos índices (por exemplo, activation_index entre 0 e 1).\n",
    "pbounds = {\n",
    "    'neurons1': (64, 256),\n",
    "    'neurons2': (64, 256),\n",
    "    'activation_index': (0, 1)\n",
    "}\n",
    "\n",
    "# Inicializa o otimizador bayesiano com a função objetivo e os limites dos hiperparâmetros.\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective_function,     # Função objetivo definida anteriormente.\n",
    "    pbounds=pbounds,          # Limites dos hiperparâmetros.\n",
    "    random_state=1            # Semente para reprodutibilidade.\n",
    ")\n",
    "\n",
    "# Executa a otimização bayesiana.\n",
    "# 'init_points' define quantos pontos iniciais aleatórios serão testados antes da otimização começar.\n",
    "# 'n_iter' define quantas iterações de otimização serão executadas.\n",
    "optimizer.maximize(init_points=5, n_iter=10)\n",
    "\n",
    "# Exibe os melhores hiperparâmetros encontrados.\n",
    "print(f\"Melhores parâmetros usando otimização bayesiana: {optimizer.max['params']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"'activation_index': 0.48873272287485325:\n",
    "\n",
    "Este valor contínuo representa a função de ativação selecionada para o modelo. Na implementação, duas funções de ativação ('relu' e 'tanh') foram mapeadas para os índices 0 e 1, respectivamente.\n",
    "O valor 0.4887 é mais próximo de 0 do que de 1. Portanto, ao arredondar, o índice 0 é selecionado, correspondendo à função de ativação 'relu'.\n",
    "\n",
    "'neurons1': 225.5063053565966:\n",
    "\n",
    "Representa o número ideal de neurônios na primeira camada oculta identificado pelo otimizador bayesiano.\n",
    "Em uma implementação prática, esse número seria arredondado para 226 neurônios na primeira camada oculta.\n",
    "\n",
    "'neurons2': 251.9811401659988:\n",
    "\n",
    "Indica o número ideal de neurônios para a segunda camada oculta.\n",
    "Ao arredondar, resulta em uma segunda camada oculta com 252 neurônios.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
