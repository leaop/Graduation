{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "---\n",
    "\n",
    "## Recapitulação Rápida\n",
    "\n",
    "- **Overfitting e Underfitting**: Temas recorrentes que já estudamos.\n",
    "- **Tipos de Aprendizagem**: Classificação e Regressão.\n",
    "- **Conceitos Avançados**: Risco empírico vs. risco estrutural, dimensão VC e dilema bias-variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos da Aula de Hoje\n",
    "\n",
    "1. **Aprofundamento em Overfitting**: Entender o que ele realmente é, como identificá-lo e como podemos prevenir ou mitigar seus efeitos.\n",
    "    - Estaremos utilizando exemplos práticos para demonstrar esses conceitos.\n",
    "  \n",
    "2. **Teorema No Free Lunch**: Explorar o que significa este teorema e quais são suas implicações para a aprendizagem de máquinas.\n",
    "    - Compreender por que não existe um algoritmo que seja o melhor para todos os problemas.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que é Importante?\n",
    "\n",
    "- **Complexidade dos Modelos**: A escolha da complexidade do modelo tem um grande impacto na qualidade das previsões. \n",
    "- **Seleção de Modelos**: O Teorema No Free Lunch nos lembra que não há uma única 'bala de prata' em aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve Recapitulação de Overfitting e Underfitting\n",
    "---\n",
    "\n",
    "## O Que é Overfitting?\n",
    "\n",
    "- **Definição**: Quando um modelo aprende o 'ruído' nos dados de treinamento a ponto de afetar negativamente o desempenho em dados não vistos.\n",
    "- **Sintomas**:\n",
    "    - Alto desempenho nos dados de treino.\n",
    "    - Baixo desempenho nos dados de teste.\n",
    "  \n",
    "---\n",
    "  \n",
    "## O Que é Underfitting?\n",
    "\n",
    "- **Definição**: Quando um modelo é demasiado simples para captar as complexidades dos dados e, por isso, apresenta desempenho ruim tanto no treino quanto no teste.\n",
    "- **Sintomas**:\n",
    "    - Baixo desempenho nos dados de treino.\n",
    "    - Baixo desempenho nos dados de teste.\n",
    "\n",
    "---\n",
    "\n",
    "## Como Detectamos Estes Fenômenos?\n",
    "\n",
    "- **Curvas de Aprendizagem**: Gráficos que mostram o desempenho do modelo em relação ao tamanho do conjunto de treinamento.\n",
    "- **Métricas de Desempenho**: Utilizando métricas como acurácia, precisão, revocação, F1-score, etc.\n",
    "  \n",
    "---\n",
    "\n",
    "**Nota**: Já abordamos esses tópicos em detalhes nas aulas anteriores. O objetivo hoje é ir além e entender como lidar com o overfitting e explorar o Teorema 'No Free Lunch'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos da Aula de Hoje\n",
    "---\n",
    "\n",
    "## Visão Geral\n",
    "\n",
    "Hoje, nosso foco será em duas áreas-chave:\n",
    "\n",
    "1. **Aprofundamento em Overfitting**: \n",
    "    - **Por que é crucial?**: O overfitting é um dos problemas mais comuns em machine learning e pode levar a resultados enganosos.\n",
    "    - **O que faremos?**: Vamos nos aprofundar na identificação, prevenção e mitigação do overfitting, utilizando exemplos práticos e técnicas específicas.\n",
    "    - **Métodos Abordados**: Regularização, Early Stopping, entre outros.\n",
    "  \n",
    "2. **Teorema No Free Lunch**:\n",
    "    - **Por que é crucial?**: Este teorema nos mostra que não existe um único algoritmo que seja ideal para todos os tipos de problemas.\n",
    "    - **O que faremos?**: Vamos entender a teoria por trás do teorema e suas implicações práticas em machine learning.\n",
    "    - **Implicações**: Escolha de algoritmos, tuning de hiperparâmetros, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprofundamento em Overfitting\n",
    "---\n",
    "\n",
    "## Como Identificar Overfitting com Métricas (3 minutos)\n",
    "\n",
    "- **Validação Cruzada**: Uma forma robusta de avaliar o desempenho do modelo em diferentes subconjuntos de dados.\n",
    "- **Conjunto de Validação**: Separe um conjunto de dados para validação durante o treinamento.\n",
    "- **Métricas de Desempenho**: Acompanhe métricas como Acurácia, F1-score, etc., em ambos os conjuntos (treino e validação).\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização como uma Abordagem para Mitigar Overfitting (4 minutos)\n",
    "\n",
    "- **Definição**: Técnica que adiciona um termo de penalidade à função de custo.\n",
    "- **Tipos Comuns**:\n",
    "    1. **L1 Regularization**: Adiciona o valor absoluto dos pesos como termo de penalidade.\n",
    "    2. **L2 Regularization**: Adiciona o quadrado dos pesos como termo de penalidade.\n",
    "- **Hiperparâmetros**: O fator de regularização é um hiperparâmetro a ser ajustado.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo: Overfitting em Redes Neurais e Técnicas para Mitigar (8 minutos)\n",
    "\n",
    "- **Dropout**: Técnica de desativar aleatoriamente alguns neurônios durante o treinamento.\n",
    "- **Early Stopping**: Monitorar o desempenho no conjunto de validação e parar o treinamento quando ele começar a degradar.\n",
    "- **Dados Adicionais**: Às vezes, simplesmente adicionando mais dados pode ajudar a mitigar o overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A função np.random.rand do NumPy gera números aleatórios uniformemente distribuídos no intervalo [0.0, 1.0]. \n",
    "- Os números são gerados a partir de uma distribuição uniforme sobre este intervalo, o que significa que cada número tem a mesma probabilidade de ser escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "df = pd.read_csv('notebooks.csv')\n",
    "df_ = df.sample(frac=1).reset_index(drop=True)\n",
    "df_ = df_[1000:3000]\n",
    "X = df_.drop(columns='valor').values\n",
    "y = df_[['valor']]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento (60%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target  = MinMaxScaler()\n",
    "\n",
    "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
    "X_train = scaler_features.fit_transform(X_train)\n",
    "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
    "y_train = scaler_target.fit_transform(y_train)\n",
    "\n",
    "# Ajusta os dados de X_temp\n",
    "X_temp = scaler_features.transform(X_temp)\n",
    "# Ajusta os dados de y_temp\n",
    "y_temp = scaler_target.transform(y_temp)\n",
    "\n",
    "# Separa os dados em X e y de validação e teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo a Regularização para Combater o Overfitting\n",
    "\n",
    "## O que é Regularização?\n",
    "- **O Que é Regularização?**: Regularização é uma técnica que adiciona um termo de penalidade à função de custo.\n",
    "- **Por que Usar Regularização?**: É útil para evitar que o modelo capture ruído nos dados de treinamento, reduzindo, assim, o overfitting.\n",
    "\n",
    "\n",
    "É como um professor que te pede para explicar o porquê da sua resposta em um teste, desencorajando você de apenas decorar as respostas. No aprendizado de máquina, adicionamos um 'termo de penalidade' para desencorajar o modelo de ajustar demais aos dados de treinamento.\n",
    "\n",
    "## Tipos de Regularização\n",
    "\n",
    "### L1 (Lasso)\n",
    "\n",
    "- **Como Funciona**: Imagine que você tem um time de futebol e alguns jogadores nunca tocam na bola. A L1 remove esses jogadores do time.\n",
    "- **Quando Usar**: Use L1 quando você suspeita que muitos recursos (ou variáveis) não ajudam a prever a resposta.\n",
    "\n",
    "### L2 (Ridge)\n",
    "\n",
    "- **Como Funciona**: Em vez de demitir jogadores, o técnico (L2) diz a todos para jogarem mais devagar (menor peso).\n",
    "- **Quando Usar**: Use quando todos os recursos parecem úteis e você quer que eles contribuam igualmente.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "- **Como Funciona**: É como combinar os técnicos de L1 e L2 para gerenciar o time.\n",
    "- **Quando Usar**: Use quando você não tem certeza de qual técnica de regularização escolher.\n",
    "\n",
    "## Onde Aplicar Regularização?\n",
    "\n",
    "- **Camadas Iniciais**: Regularizar as primeiras camadas pode ser útil se você acha que as entradas (recursos) podem conter ruídos ou informações irrelevantes.\n",
    "- **Camadas do Meio**: Podem ser regularizadas para tornar o modelo mais simples e rápido.\n",
    "- **Camadas Finais**: Evite regularizar demais para não perder as características aprendidas.\n",
    "\n",
    "## Como Escolher a Força da Regularização?\n",
    "\n",
    "- Ajuste o 'termo de penalidade'. Se for muito alto, o modelo pode ficar simples demais e perder importantes padrões nos dados (underfitting).\n",
    "- Use técnicas como validação cruzada para encontrar o melhor ajuste.\n",
    "\n",
    "## Resumo\n",
    "\n",
    "- A regularização é uma técnica poderosa para tornar seu modelo mais generalizável e menos propenso a overfitting.\n",
    "- Escolher o tipo e a intensidade da regularização pode depender do seu conhecimento específico do problema e de técnicas de ajuste de hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo o Dropout para Combater o Overfitting\n",
    "\n",
    "## O que é Dropout?\n",
    "- **O Que é Dropout?**: Dropout é uma técnica de regularização em redes neurais que \"desliga\" aleatoriamente um subconjunto de neurônios durante o treinamento.\n",
    "- **Por que Usar Dropout?**: O Dropout evita que qualquer neurônio se torne excessivamente especializado em memorizar ruídos dos dados de treinamento, o que contribui para combater o overfitting.\n",
    "\n",
    "É como um time de futebol onde alguns jogadores são aleatoriamente mandados para o banco durante o jogo para garantir que a equipe não dependa demais de um único jogador estrela. Isso torna o time como um todo mais robusto.\n",
    "\n",
    "## Como Funciona o Dropout?\n",
    "\n",
    "- **Implementação**: Durante cada iteração de treinamento, alguns neurônios são escolhidos aleatoriamente para serem \"desativados\". Isso significa que esses neurônios não participam do processo de treinamento para essa iteração específica.\n",
    "  \n",
    "- **Taxa de Dropout**: É o percentual de neurônios que você quer desativar em cada iteração. Por exemplo, uma taxa de 0.5 significa que 50% dos neurônios em uma camada são desativados.\n",
    "\n",
    "## Onde Aplicar Dropout?\n",
    "\n",
    "- **Camadas Iniciais**: Aplicar Dropout nas primeiras camadas pode ajudar se você acredita que os neurônios estão desenvolvendo dependências indesejadas nos dados de entrada. \n",
    "\n",
    "- **Camadas Ocultas Densas**: É mais comum aplicar Dropout nas camadas ocultas onde há uma alta densidade de neurônios. Isso aumenta as chances de overfitting, e o Dropout pode ajudar a mitigar isso.\n",
    "\n",
    "- **Camadas Finais**: Cuidado ao aplicar Dropout próximo à camada de saída, especialmente em tarefas que requerem alta precisão. Desativar neurônios aqui pode levar a predições imprecisas.\n",
    "\n",
    "\n",
    "## Quando Usar Dropout?\n",
    "\n",
    "- **Camadas Densas e Complexas**: Dropout é comumente usado em camadas que possuem muitos neurônios, como camadas densas.\n",
    "  \n",
    "- **Problemas com Overfitting**: Quando o modelo está muito bem ajustado aos dados de treinamento e não generaliza bem para dados novos.\n",
    "\n",
    "## Cuidados ao Usar Dropout\n",
    "\n",
    "- **Não use uma taxa muito alta**: Desativar muitos neurônios pode levar a underfitting.\n",
    "  \n",
    "- **Ajuste durante a Validação**: Sempre verifique o desempenho em um conjunto de validação para encontrar a taxa ideal.\n",
    "\n",
    "## Resumo\n",
    "\n",
    "- Dropout é uma técnica eficaz para evitar overfitting em redes neurais.\n",
    "- É como adicionar uma forma de \"incerteza\" ou \"ruído\" durante o treinamento, tornando o modelo mais robusto.\n",
    "- A escolha da taxa de dropout e onde aplicá-la são decisões cruciais que podem requerer experimentação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para criar os modelos\n",
    "# Função para criar o modelo base\n",
    "def create_base_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com L1 e L2 (Elastic Net)\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com Dropout\n",
    "def create_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com L1, L2 e Dropout\n",
    "def create_l1_l2_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia com uma forma de entrada específica\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Criar, compilar e treinar os modelos\n",
    "model_base = create_base_model(input_shape)\n",
    "model_l1_l2 = create_l1_l2_model(input_shape)\n",
    "model_dropout = create_dropout_model(input_shape)\n",
    "model_l1_l2_dropout = create_l1_l2_dropout_model(input_shape)\n",
    "\n",
    "# Compilando e treinando os modelos\n",
    "optimizer1 = Adam(learning_rate=0.001)\n",
    "optimizer2 = Adam(learning_rate=0.001)\n",
    "optimizer3 = Adam(learning_rate=0.001)\n",
    "optimizer4 = Adam(learning_rate=0.001)\n",
    "\n",
    "model_base.compile(optimizer=optimizer1, loss='mse')\n",
    "history_base = model_base.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "model_l1_l2.compile(optimizer=optimizer2, loss='mse')\n",
    "history_l1_l2 = model_l1_l2.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "model_dropout.compile(optimizer=optimizer3, loss='mse')\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "model_l1_l2_dropout.compile(optimizer=optimizer4, loss='mse')\n",
    "history_l1_l2_dropout = model_l1_l2_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 525us/step\n",
      "13/13 [==============================] - 0s 514us/step\n",
      "13/13 [==============================] - 0s 500us/step\n",
      "13/13 [==============================] - 0s 491us/step\n"
     ]
    }
   ],
   "source": [
    "# Calculando os MSE para os modelos\n",
    "y_pred_base = model_base.predict(X_test)\n",
    "y_pred_l1_l2 = model_l1_l2.predict(X_test)\n",
    "y_pred_dropout = model_dropout.predict(X_test)\n",
    "y_pred_l1_l2_dropout = model_l1_l2_dropout.predict(X_test)\n",
    "\n",
    "mse_base = mean_squared_error(y_test, y_pred_base)\n",
    "mse_l1_l2 = mean_squared_error(y_test, y_pred_l1_l2)\n",
    "mse_dropout = mean_squared_error(y_test, y_pred_dropout)\n",
    "mse_l1_l2_dropout = mean_squared_error(y_test, y_pred_l1_l2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "labels = ['Base', 'L1_L2', 'Dropout', 'L1_L2_Dropout']\n",
    "mse_values = [mse_base, mse_l1_l2, mse_dropout, mse_l1_l2_dropout]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(labels, mse_values, color='skyblue')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Model Type')\n",
    "plt.title('Comparação de MSE nos Modelos')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ticklabel_format(style='plain', axis='x')  # Remove notação científica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como Identificar Overfitting com Métricas\n",
    "---\n",
    "\n",
    "## Definindo o Problema\n",
    "\n",
    "- **Pergunta Principal**: Como saber se o modelo está se ajustando demais aos dados de treinamento?\n",
    "- **Consequência**: Se o modelo está com overfitting, ele terá um desempenho ruim em dados não vistos.\n",
    "\n",
    "---\n",
    "\n",
    "## Utilizando Métricas de Desempenho\n",
    "\n",
    "- **Treinamento vs Validação**: É crucial comparar as métricas de desempenho nos conjuntos de treinamento e validação.\n",
    "    1. **Acurácia**\n",
    "    2. **F1-score**\n",
    "    3. **Curva ROC-AUC**\n",
    "    \n",
    "- **Indicadores de Overfitting**:\n",
    "    - Acurácia alta no conjunto de treinamento, mas baixa no conjunto de validação.\n",
    "    - F1-score desproporcionalmente menor no conjunto de validação.\n",
    "    - Curva ROC-AUC demonstrando divergência entre treino e validação.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizando com Gráficos\n",
    "\n",
    "- **Plot de Métricas**: Gráficos de linha para acompanhar a evolução das métricas ao longo das épocas ou iterações.\n",
    "    - Eixo X: Épocas ou Iterações\n",
    "    - Eixo Y: Valor da Métrica\n",
    "\n",
    "**Nota**: Na próxima seção, vamos olhar para um exemplo prático que inclui esses gráficos.\n",
    "\n",
    "---\n",
    "\n",
    "Pronto para ver isso na prática? Vamos mergulhar no código a seguir!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "\n",
    "def metricas_regressao(X_test, y_test, scaler_y, model):\n",
    "    \"\"\"\n",
    "    Avalia métricas de regressão para um modelo e conjunto de teste fornecidos.\n",
    "\n",
    "    Parâmetros:\n",
    "    - X_test: características do conjunto de teste.\n",
    "    - y_test: rótulos verdadeiros do conjunto de teste.\n",
    "    - scaler_y: scaler utilizado para normalizar a variável alvo.\n",
    "    - model: modelo treinado para fazer previsões.\n",
    "\n",
    "    Retorna:\n",
    "    Métricas de avaliação de regressão impressas.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Fazer previsões usando o modelo fornecido\n",
    "    predict = model.predict(X_test)\n",
    "    if scaler_y == 0:\n",
    "        real = y_test\n",
    "    else:\n",
    "    # 2. Inverter a transformação para obter os valores originais (não normalizados)\n",
    "        predict = scaler_y.inverse_transform(predict)\n",
    "        real = scaler_y.inverse_transform(y_test)\n",
    "    # 3. Calcular R2 e R2 ajustado\n",
    "    k = X_test.shape[1]  # número de características independentes\n",
    "    n = len(X_test)  # tamanho da amostra\n",
    "    r2 = sm.r2_score(real, predict)\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)  # fórmula para R2 ajustado\n",
    "\n",
    "    # 4. Imprimir métricas\n",
    "    print('Root Mean Square Error:', round(np.sqrt(np.mean(np.array(predict) - np.array(real))**2), 2))\n",
    "    print('Mean Square Error:', round(sm.mean_squared_error(real, predict), 2))\n",
    "    print('Mean Absolut Error:', round(sm.mean_absolute_error(real, predict), 2))\n",
    "    print('Median Absolut Error:', round(sm.median_absolute_error(real, predict), 2))\n",
    "    print('Explain Variance Score:', round(sm.explained_variance_score(real, predict) * 100, 2))\n",
    "    print('R2 score:', round(sm.r2_score(real, predict) * 100, 2))\n",
    "    print('Adjusted R2 =', round(adj_r2, 3) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/13 [=>............................] - ETA: 0s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 583us/step\n",
      "Root Mean Square Error: 150.41\n",
      "Mean Square Error: 8333632.85\n",
      "Mean Absolut Error: 2214.36\n",
      "Median Absolut Error: 1631.62\n",
      "Explain Variance Score: 77.66\n",
      "R2 score: 77.59\n",
      "Adjusted R2 = 76.8\n"
     ]
    }
   ],
   "source": [
    "metricas_regressao(X_test, y_test, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 0s 520us/step\n",
      "Root Mean Square Error: 274.28\n",
      "Mean Square Error: 4545778.59\n",
      "Mean Absolut Error: 1639.81\n",
      "Median Absolut Error: 1264.52\n",
      "Explain Variance Score: 87.8\n",
      "R2 score: 87.59\n",
      "Adjusted R2 = 87.4\n"
     ]
    }
   ],
   "source": [
    "metricas_regressao(X_train, y_train, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Erros do Modelo com 2000 Registros\n",
    "\n",
    "## Métricas de Desempenho\n",
    "\n",
    "- **Root Mean Square Error (RMSE)**\n",
    "    - Teste: 35.15\n",
    "    - Treinamento: 64.24\n",
    "- **Mean Square Error (MSE)**\n",
    "    - Teste: 8084403.99\n",
    "    - Treinamento: 4111220.34\n",
    "- **Mean Absolute Error (MAE)**\n",
    "    - Teste: 2155.01\n",
    "    - Treinamento: 1572.19\n",
    "- **Median Absolute Error**\n",
    "    - Teste: 1608.74\n",
    "    - Treinamento: 1237.49\n",
    "- **Explained Variance Score**\n",
    "    - Teste: 79.15%\n",
    "    - Treinamento: 88.91%\n",
    "- **R2 Score**\n",
    "    - Teste: 79.15%\n",
    "    - Treinamento: 88.9%\n",
    "- **Adjusted R2**\n",
    "    - Teste: 78.4%\n",
    "    - Treinamento: 88.8%\n",
    "\n",
    "## Interpretação das Métricas\n",
    "\n",
    "- **Root Mean Square Error (RMSE)**\n",
    "    - Representa a raiz quadrada da média dos erros quadráticos. Valores menores indicam melhor ajuste do modelo. \n",
    "- **Mean Square Error (MSE)**\n",
    "    - É a média dos erros quadráticos. Valores mais baixos são melhores, mas é mais sensível a outliers.\n",
    "- **Mean Absolute Error (MAE)**\n",
    "    - É a média dos erros absolutos. Fornece uma ideia de quão erradas são as previsões. \n",
    "- **Median Absolute Error**\n",
    "    - É a mediana dos erros absolutos. Menos sensível a outliers que o MAE.\n",
    "- **Explained Variance Score**\n",
    "    - Mede a proporção da variância do target que é explicada pelo modelo. Valores mais próximos de 100% são ideais.\n",
    "- **R2 Score**\n",
    "    - Mede o quanto do target é explicado pelas features. Quanto mais próximo de 100%, melhor.\n",
    "- **Adjusted R2**\n",
    "    - Semelhante ao R2, mas ajustado pelo número de preditores no modelo. É mais útil quando comparando modelos com diferentes números de preditores.\n",
    "\n",
    "## Análise de Overfitting\n",
    "\n",
    "- O modelo tem um desempenho significativamente melhor nos dados de treinamento em comparação com os dados de teste em quase todas as métricas.\n",
    "- A diferença entre o R2 Score de treinamento e teste é aproximadamente 9.75%, o que pode ser um indicador de que o modelo está sofrendo de algum grau de overfitting.\n",
    "- O RMSE para os dados de treinamento é aproximadamente 64, enquanto para os dados de teste é 35. A diferença notável também aponta para o overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "X = df.drop(columns='valor').values\n",
    "y = df[['valor']]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento (60%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target  = MinMaxScaler()\n",
    "\n",
    "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
    "X_train = scaler_features.fit_transform(X_train)\n",
    "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
    "y_train = scaler_target.fit_transform(y_train)\n",
    "\n",
    "# Ajusta os dados de X_temp\n",
    "X_temp = scaler_features.transform(X_temp)\n",
    "# Ajusta os dados de y_temp\n",
    "y_temp = scaler_target.transform(y_temp)\n",
    "\n",
    "# Separa os dados em X e y de validação e teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia com uma forma de entrada específica\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Criar, compilar e treinar o melhor modelo\n",
    "model_dropout = create_dropout_model(input_shape)\n",
    "\n",
    "# Compilando e treinando os modelos\n",
    "optimizer3 = Adam(learning_rate=0.001)\n",
    "model_dropout.compile(optimizer=optimizer3, loss='mse')\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 481us/step\n",
      "Root Mean Square Error: 122.18\n",
      "Mean Square Error: 7056109.64\n",
      "Mean Absolut Error: 2094.63\n",
      "Median Absolut Error: 1668.44\n",
      "Explain Variance Score: 81.13\n",
      "R2 score: 81.09\n",
      "Adjusted R2 = 81.0\n"
     ]
    }
   ],
   "source": [
    "metricas_regressao(X_test, y_test, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188/188 [==============================] - 0s 482us/step\n",
      "Root Mean Square Error: 71.48\n",
      "Mean Square Error: 5673543.7\n",
      "Mean Absolut Error: 1859.41\n",
      "Median Absolut Error: 1457.12\n",
      "Explain Variance Score: 84.68\n",
      "R2 score: 84.66\n",
      "Adjusted R2 = 84.6\n"
     ]
    }
   ],
   "source": [
    "metricas_regressao(X_train, y_train, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Erros do Modelo com Acréscimo de Dados\n",
    "\n",
    "## Contexto\n",
    "\n",
    "- Este modelo foi treinado com um conjunto de 10,000 registros, um aumento significativo em relação aos experimentos anteriores que tinham menos registros.\n",
    "\n",
    "## Métricas de Desempenho e Análise\n",
    "\n",
    "### Root Mean Square Error (RMSE)\n",
    "- **Teste**: 127.68\n",
    "- **Treinamento**: 157.96\n",
    "  - Representa o desvio padrão dos erros do modelo. Valores menores indicam um melhor desempenho.\n",
    "\n",
    "### Mean Square Error (MSE)\n",
    "- **Teste**: 6769172.38\n",
    "- **Treinamento**: 5058889.6\n",
    "  - É a média dos erros ao quadrado, sendo sensível a outliers. Valores menores são melhores.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "- **Teste**: 2017.72\n",
    "- **Treinamento**: 1749.72\n",
    "  - É a média dos erros absolutos, dando uma ideia da magnitude dos erros.\n",
    "\n",
    "### Median Absolute Error\n",
    "- **Teste**: 1544.37\n",
    "- **Treinamento**: 1353.14\n",
    "  - A mediana dos erros absolutos e é menos sensível a outliers.\n",
    "\n",
    "### Explained Variance Score\n",
    "- **Teste**: 81.9%\n",
    "- **Treinamento**: 86.39%\n",
    "  - Representa quanto da variância total é explicada pelo modelo.\n",
    "\n",
    "### R2 Score\n",
    "- **Teste**: 81.86%\n",
    "- **Treinamento**: 86.32%\n",
    "  - Indica o ajuste do modelo aos dados observados.\n",
    "\n",
    "### Adjusted R2\n",
    "- **Teste**: 81.7%\n",
    "- **Treinamento**: 86.3%\n",
    "  - É o R2 ajustado pelo número de preditores no modelo.\n",
    "\n",
    "## Efeito do Acréscimo de Dados\n",
    "\n",
    "- O acréscimo de mais dados no treinamento parece ter ajudado o modelo a generalizar melhor, como evidenciado pelas métricas de teste e treinamento mais próximas.\n",
    "- A diferença no R2 Score entre treinamento e teste diminuiu, sugerindo que o modelo está menos propenso a overfitting.\n",
    "- A inclusão de mais dados pode ter contribuído para uma representação mais abrangente do espaço de características, tornando o modelo mais robusto a variações nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teorema No Free Lunch\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "- **Definição**: O Teorema No Free Lunch (NFL) afirma que não existe um único algoritmo de aprendizado de máquina que funcione melhor para todos os tipos de problemas.\n",
    "- **Importância**: Esse teorema nos ajuda a entender por que a busca pelo \"algoritmo perfeito\" é fútil.\n",
    "\n",
    "---\n",
    "\n",
    "## O Que o Teorema Realmente Significa?\n",
    "\n",
    "1. **Não Existe Algoritmo Universalmente Superior**: Cada algoritmo tem seus próprios pontos fortes e fracos, e o que funciona bem para um problema pode não ser adequado para outro.\n",
    "2. **Dependência do Problema**: O sucesso de um algoritmo é fortemente dependente do tipo de problema que você está tentando resolver.\n",
    "3. **A Importância da Experimentação**: Este teorema reforça a necessidade de experimentar com vários algoritmos e técnicas para encontrar a melhor abordagem para um determinado problema.\n",
    "\n",
    "---\n",
    "\n",
    "## Implicações Práticas\n",
    "\n",
    "- **Seleção de Modelos**: Dada a impossibilidade de um único melhor algoritmo, a seleção de modelos torna-se crucial.\n",
    "- **Otimização de Hiperparâmetros**: O ajuste de hiperparâmetros é mais relevante do que nunca, já que o \"melhor\" algoritmo é problema-específico.\n",
    "\n",
    "---\n",
    "\n",
    "**Exemplos práticos que ilustram o Teorema No Free Lunch em ação.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 703us/step\n",
      "7/7 [==============================] - 0s 500us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gerando um conjunto de dados de exemplo para regressão\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento, validação e teste\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Criando e treinando os modelos de RandomForest e SVM para regressão\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "svm = SVR()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Função para criar o modelo de rede neural com L1 e L2\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo de rede neural com Dropout\n",
    "def create_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Criando e treinando as redes neurais\n",
    "l1_l2_model = create_l1_l2_model(X_train.shape[1])\n",
    "dropout_model = create_dropout_model(X_train.shape[1])\n",
    "\n",
    "l1_l2_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "dropout_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "# Avaliando os modelos\n",
    "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "svm_mse = mean_squared_error(y_test, svm.predict(X_test))\n",
    "l1_l2_mse = mean_squared_error(y_test, l1_l2_model.predict(X_test).reshape(-1))\n",
    "dropout_mse = mean_squared_error(y_test, dropout_model.predict(X_test).reshape(-1))\n",
    "\n",
    "# Gráfico para comparar as métricas\n",
    "labels = ['Random Forest', 'SVM', 'L1_L2', 'Dropout']\n",
    "values = [rf_mse, svm_mse, l1_l2_mse, dropout_mse]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Erro Quadrático Médio (MSE)')\n",
    "plt.title('Comparação de MSE entre Modelos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significado e Importância\n",
    "---\n",
    "\n",
    "## O Que o Teorema No Free Lunch Realmente Significa? \n",
    "\n",
    "- **Sem Almoço Grátis**: O nome sugere que não existe uma \"refeição gratuita\" em termos de eficácia algorítmica; tudo tem um custo.\n",
    "- **Universalidade**: Não existe um único algoritmo que seja superior em todos os cenários e tipos de dados.\n",
    "  \n",
    "---\n",
    "\n",
    "## Por Que Isso é Importante? \n",
    "\n",
    "### Fim da Busca pelo \"Algoritmo de Aprendizado Perfeito\"\n",
    "\n",
    "- Elimina a noção de que poderia existir um \"Santo Graal\" dos algoritmos de aprendizado de máquina.\n",
    "  \n",
    "### Acentua a Necessidade de Personalização\n",
    "\n",
    "- Destaca que a escolha do algoritmo deve ser adaptada ao problema específico em mãos.\n",
    "\n",
    "---\n",
    "\n",
    "**Para resumir, o Teorema No Free Lunch nos ensina a ser mais críticos e adaptáveis como cientistas de dados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como o Teorema No Free Lunch se Relaciona com Overfitting\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "- **Ponto em Comum**: Tanto o Teorema No Free Lunch quanto o conceito de overfitting nos dizem que não há soluções universais no campo do aprendizado de máquina.\n",
    "\n",
    "---\n",
    "\n",
    "## Relação entre NFL e Overfitting\n",
    "\n",
    "1. **Escolha de Algoritmos**: O Teorema No Free Lunch sugere que devemos escolher algoritmos com base no problema específico, algo que também é crucial para evitar o overfitting.\n",
    "\n",
    "2. **Complexidade do Modelo**: \n",
    "    - NFL nos adverte contra a busca por um \"algoritmo perfeito\".\n",
    "    - Overfitting nos adverte contra a busca por um \"modelo perfeitamente ajustado\".\n",
    "    - Ambos são contra a ideia de \"um tamanho serve para todos\".\n",
    "\n",
    "3. **Personalização e Ajuste**:\n",
    "    - NFL destaca a necessidade de ajuste e personalização nos algoritmos.\n",
    "    - Overfitting nos mostra que esse ajuste precisa ser feito com cuidado para evitar memorização em vez de generalização.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "- A consciência do Teorema No Free Lunch pode nos ajudar a ser mais criteriosos na prevenção de overfitting, escolhendo e ajustando algoritmos de forma mais eficaz.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicações Práticas para a Seleção de Modelos\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "- **Contexto**: Agora que entendemos os conceitos de overfitting e o Teorema No Free Lunch, como aplicamos esse conhecimento na prática?\n",
    "\n",
    "---\n",
    "\n",
    "## Passos na Seleção de Modelos\n",
    "\n",
    "1. **Análise do Problema**:\n",
    "    - Entender o tipo de problema (Classificação, Regressão, Agrupamento, etc.) é o primeiro passo na seleção do modelo.\n",
    "\n",
    "2. **Teste de Vários Modelos**:\n",
    "    - Devido ao Teorema No Free Lunch, é recomendável testar diversos algoritmos para encontrar o que se adequa melhor ao problema específico.\n",
    "\n",
    "3. **Validação Cruzada**:\n",
    "    - Usar técnicas como K-Fold para estimar o desempenho do modelo em dados não vistos e ajudar a evitar overfitting.\n",
    "\n",
    "4. **Ajuste de Hiperparâmetros**:\n",
    "    - Com o modelo escolhido, o ajuste de hiperparâmetros torna-se crucial tanto para o desempenho quanto para evitar o overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Métricas e Diagnósticos\n",
    "\n",
    "- **Selecionar Métricas Relevantes**: Acurácia, Precisão, Recall, F1-Score, entre outros, dependendo do problema.\n",
    "- **Curvas ROC e AUC**: Ferramentas úteis para avaliar a qualidade do modelo em problemas de classificação.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "- A integração desses conceitos (NFL e overfitting) nos dá uma abordagem mais robusta e informada para a seleção e otimização de modelos em aprendizado de máquina.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios\n",
    "\n",
    "Exemplos de aplicação de l1, l2 e l1 e l2 juntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importando as bibliotecas necessárias para o modelo de rede neural\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Função para criar o modelo com regularização L1\n",
    "def create_l1_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com regularização L2\n",
    "def create_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com regularização L1 e L2\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura modelo base 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Função para criar um modelo com 7 camadas ocultas\n",
    "def create_7_layer_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_shape,)), # Camada de entrada\n",
    "        Dense(256, activation='relu'),  # Primeira camada oculta\n",
    "        Dense(128, activation='relu'),  # Segunda camada oculta\n",
    "        Dense(64, activation='relu'),   # Terceira camada oculta\n",
    "        Dense(32, activation='relu'),   # Quarta camada oculta\n",
    "        Dense(16, activation='relu'),   # Quinta camada oculta\n",
    "        Dense(8, activation='relu'),    # Sexta camada oculta\n",
    "        Dense(4, activation='relu'),    # Sétima camada oculta\n",
    "        Dense(1, activation='linear')   # Camada de saída\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar um modelo com 3 camadas ocultas\n",
    "def create_3_layer_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),  # Camada de entrada\n",
    "        Dense(32, activation='relu'),  # Primeira camada oculta\n",
    "        Dense(16, activation='relu'),  # Segunda camada oculta\n",
    "        Dense(8, activation='relu'),   # Terceira camada oculta\n",
    "        Dense(1, activation='linear')  # Camada de saída\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura modelo base 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),  # Camada de entrada\n",
    "        Dense(32, activation='relu'),  # Primeira camada oculta\n",
    "        Dense(16, activation='relu'),  # Segunda camada oculta\n",
    "        Dense(8, activation='relu'),   # Terceira camada oculta\n",
    "        Dense(1, activation='linear')  # Camada de saída\n",
    "    ])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize a base de dados notebooks_ruidosos.csv que contém 3 colunas com valores aleatórios que devem atrapalhar o aprendizado do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1 - Separe os dados em X_train, X_temp, y_train, y_temp, com 50% para o treinamento normalize com fit transform os dados de treinamento (um normalizador para X e outro para y), normalize os dados temporários com o transform e em seguida separe X e y temp em X e y de validação e teste, 50% para cada um. (a coluna valor será o y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('notebooks_ruidoso.csv')\n",
    "X = df_.drop(columns='valor').values\n",
    "y = df_[['valor']]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento (50%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target  = MinMaxScaler()\n",
    "\n",
    "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
    "X_train = scaler_features.fit_transform(X_train)\n",
    "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
    "y_train = scaler_target.fit_transform(y_train)\n",
    "\n",
    "# Ajusta os dados de X_temp\n",
    "X_temp = scaler_features.transform(X_temp)\n",
    "# Ajusta os dados de y_temp\n",
    "y_temp = scaler_target.transform(y_temp)\n",
    "\n",
    "# Separa os dados em X e y de validação e teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos), em seguida crie uma função que gere um modelo baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 8ms/step - loss: 134.7059 - accuracy: 0.0010 - val_loss: 86.5533 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 56.1937 - accuracy: 0.0010 - val_loss: 29.3395 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 16.7381 - accuracy: 0.0010 - val_loss: 8.3280 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 4.9554 - accuracy: 0.0010 - val_loss: 2.6227 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.8039 - accuracy: 0.0010 - val_loss: 1.1291 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.8568 - accuracy: 0.0010 - val_loss: 0.6969 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6342 - accuracy: 0.0010 - val_loss: 0.5853 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5561 - accuracy: 0.0010 - val_loss: 0.5327 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5154 - accuracy: 0.0010 - val_loss: 0.5064 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.5005 - accuracy: 0.0010 - val_loss: 0.4983 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4979 - accuracy: 0.0010 - val_loss: 0.4993 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4971 - accuracy: 0.0010 - val_loss: 0.4980 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4963 - accuracy: 0.0010 - val_loss: 0.4953 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4954 - accuracy: 0.0010 - val_loss: 0.4957 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4951 - accuracy: 0.0010 - val_loss: 0.4955 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4940 - accuracy: 0.0010 - val_loss: 0.4943 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4931 - accuracy: 0.0010 - val_loss: 0.4941 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4924 - accuracy: 0.0010 - val_loss: 0.4948 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4916 - accuracy: 0.0010 - val_loss: 0.4910 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4909 - accuracy: 0.0010 - val_loss: 0.4909 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.0010 - val_loss: 0.4931 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4904 - accuracy: 0.0010 - val_loss: 0.4903 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4896 - accuracy: 0.0010 - val_loss: 0.4907 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4892 - accuracy: 0.0010 - val_loss: 0.4916 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4882 - accuracy: 0.0010 - val_loss: 0.4878 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.0010 - val_loss: 0.4876 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4863 - accuracy: 0.0010 - val_loss: 0.4885 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4854 - accuracy: 0.0010 - val_loss: 0.4849 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.0010 - val_loss: 0.4870 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4841 - accuracy: 0.0010 - val_loss: 0.4869 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4844 - accuracy: 0.0010 - val_loss: 0.4831 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4842 - accuracy: 0.0010 - val_loss: 0.4860 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.0010 - val_loss: 0.4858 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4842 - accuracy: 0.0010 - val_loss: 0.4854 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.0010 - val_loss: 0.4847 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.0010 - val_loss: 0.4883 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4861 - accuracy: 0.0010 - val_loss: 0.4877 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.0010 - val_loss: 0.4881 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.0010 - val_loss: 0.4908 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4878 - accuracy: 0.0010 - val_loss: 0.4857 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4878 - accuracy: 0.0010 - val_loss: 0.4903 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.0010 - val_loss: 0.4906 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4885 - accuracy: 0.0010 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.0010 - val_loss: 0.4903 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.0010 - val_loss: 0.4899 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.0010 - val_loss: 0.4894 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.0010 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4875 - accuracy: 0.0010 - val_loss: 0.4887 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4870 - accuracy: 0.0010 - val_loss: 0.4874 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.4867 - accuracy: 0.0010 - val_loss: 0.4884 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense  # Importe Input e Dense corretamente\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_l1_l2(input_shape):\n",
    "    modelo = Sequential([\n",
    "        Input(shape=input_shape),  # Use Input corretamente com input_shape\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear') \n",
    "    ])\n",
    "   \n",
    "    modelo.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "# Supondo que você tenha um conjunto de dados de treinamento e um conjunto de dados de validação\n",
    "# X_train e y_train são seus dados de treinamento e rótulos\n",
    "# X_val e y_val são seus dados de validação e rótulos\n",
    "\n",
    "modelo = create_l1_l2(input_shape=X_train.shape[1:])  # Passe o formato dos seus dados de entrada\n",
    "\n",
    "# Treine o modelo por 50 épocas\n",
    "historico_treinamento = modelo.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 - De acordo com as peculiaridades de aplicação da aplicação do dropout, crie uma função que gere um modelo baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 15ms/step - loss: 0.1668 - accuracy: 0.0010 - val_loss: 0.1610 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.0010 - val_loss: 0.1321 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0822 - accuracy: 0.0010 - val_loss: 0.1158 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.0020 - val_loss: 0.1023 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0665 - accuracy: 0.0010 - val_loss: 0.0924 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0615 - accuracy: 0.0010 - val_loss: 0.0924 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0620 - accuracy: 0.0010 - val_loss: 0.0838 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.0020 - val_loss: 0.0808 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0559 - accuracy: 0.0020 - val_loss: 0.0878 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0578 - accuracy: 0.0020 - val_loss: 0.0843 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0575 - accuracy: 0.0020 - val_loss: 0.0853 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0528 - accuracy: 0.0010 - val_loss: 0.0826 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0514 - accuracy: 0.0020 - val_loss: 0.0861 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0508 - accuracy: 0.0020 - val_loss: 0.0844 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0490 - accuracy: 0.0020 - val_loss: 0.0808 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0508 - accuracy: 0.0020 - val_loss: 0.0882 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0476 - accuracy: 0.0020 - val_loss: 0.0836 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.0020 - val_loss: 0.0880 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0457 - accuracy: 0.0010 - val_loss: 0.0819 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0453 - accuracy: 0.0020 - val_loss: 0.0775 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0466 - accuracy: 0.0010 - val_loss: 0.0768 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0471 - accuracy: 0.0020 - val_loss: 0.0763 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0457 - accuracy: 0.0020 - val_loss: 0.0807 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0445 - accuracy: 0.0020 - val_loss: 0.0807 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0415 - accuracy: 0.0020 - val_loss: 0.0826 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0423 - accuracy: 0.0020 - val_loss: 0.0791 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0398 - accuracy: 0.0020 - val_loss: 0.0857 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0425 - accuracy: 0.0020 - val_loss: 0.0872 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.0020 - val_loss: 0.0845 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0399 - accuracy: 0.0020 - val_loss: 0.0841 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.0020 - val_loss: 0.0790 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0387 - accuracy: 0.0010 - val_loss: 0.0871 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0374 - accuracy: 0.0020 - val_loss: 0.0745 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0379 - accuracy: 0.0020 - val_loss: 0.0805 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0395 - accuracy: 0.0020 - val_loss: 0.0777 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0365 - accuracy: 0.0010 - val_loss: 0.0830 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0381 - accuracy: 0.0010 - val_loss: 0.0701 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0342 - accuracy: 0.0020 - val_loss: 0.0784 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0345 - accuracy: 0.0020 - val_loss: 0.0815 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0356 - accuracy: 0.0020 - val_loss: 0.0756 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0366 - accuracy: 0.0010 - val_loss: 0.0750 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0336 - accuracy: 0.0020 - val_loss: 0.0794 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0355 - accuracy: 0.0020 - val_loss: 0.0854 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0365 - accuracy: 0.0020 - val_loss: 0.0790 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.0020 - val_loss: 0.0736 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0339 - accuracy: 0.0020 - val_loss: 0.0820 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.0020 - val_loss: 0.0841 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.0345 - accuracy: 0.0010 - val_loss: 0.0866 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0334 - accuracy: 0.0020 - val_loss: 0.0656 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.0338 - accuracy: 0.0020 - val_loss: 0.0822 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def create_dropout_model(input_shape, dropout_rate=0.5):\n",
    "    modelo = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a primeira camada oculta\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a segunda camada oculta\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a terceira camada oculta\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a quarta camada oculta\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a quinta camada oculta\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a sexta camada oculta\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),  # Adiciona uma camada de dropout após a sétima camada oculta\n",
    "        Dense(1, activation='linear') \n",
    "    ])\n",
    "   \n",
    "    modelo.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "# Suponha que você tenha um conjunto de dados de treinamento e um conjunto de dados de validação\n",
    "# X_train e y_train são seus dados de treinamento e rótulos\n",
    "# X_val e y_val são seus dados de validação e rótulos\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "modelo = create_dropout_model(input_shape=input_shape)\n",
    "\n",
    "# Treine o modelo por 50 épocas\n",
    "historico_treinamento = modelo.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos) e da aplicação do dropout, crie uma função que gere um modelo  baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (basicamente mesclar os dois modelos anteriores), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 2s 9ms/step - loss: 86.7321 - accuracy: 0.0010 - val_loss: 58.2609 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 39.3011 - accuracy: 0.0010 - val_loss: 22.0642 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 13.3213 - accuracy: 0.0010 - val_loss: 6.8874 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 4.0941 - accuracy: 0.0010 - val_loss: 2.1259 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 1.5144 - accuracy: 0.0010 - val_loss: 1.0170 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.8145 - accuracy: 0.0010 - val_loss: 0.6584 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.5832 - accuracy: 0.0010 - val_loss: 0.5126 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4709 - accuracy: 0.0010 - val_loss: 0.4305 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.4068 - accuracy: 0.0010 - val_loss: 0.3867 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3788 - accuracy: 0.0010 - val_loss: 0.3677 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3627 - accuracy: 0.0010 - val_loss: 0.3582 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3562 - accuracy: 0.0010 - val_loss: 0.3511 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3502 - accuracy: 0.0010 - val_loss: 0.3463 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3453 - accuracy: 0.0010 - val_loss: 0.3426 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3414 - accuracy: 0.0010 - val_loss: 0.3399 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3395 - accuracy: 0.0000e+00 - val_loss: 0.3382 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3371 - accuracy: 0.0010 - val_loss: 0.3355 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3346 - accuracy: 0.0010 - val_loss: 0.3347 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3328 - accuracy: 0.0010 - val_loss: 0.3333 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.0010 - val_loss: 0.3314 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3295 - accuracy: 0.0010 - val_loss: 0.3300 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3300 - accuracy: 0.0000e+00 - val_loss: 0.3297 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3278 - accuracy: 0.0010 - val_loss: 0.3275 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3269 - accuracy: 0.0010 - val_loss: 0.3275 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.3250 - accuracy: 0.0020 - val_loss: 0.3268 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3248 - accuracy: 0.0010 - val_loss: 0.3256 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3238 - accuracy: 0.0010 - val_loss: 0.3237 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3219 - accuracy: 0.0010 - val_loss: 0.3240 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3218 - accuracy: 0.0010 - val_loss: 0.3231 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3209 - accuracy: 0.0010 - val_loss: 0.3213 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.0010 - val_loss: 0.3223 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3195 - accuracy: 0.0000e+00 - val_loss: 0.3214 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3193 - accuracy: 0.0010 - val_loss: 0.3219 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.0010 - val_loss: 0.3207 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.0010 - val_loss: 0.3200 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3184 - accuracy: 0.0010 - val_loss: 0.3207 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3188 - accuracy: 0.0010 - val_loss: 0.3207 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.0010 - val_loss: 0.3200 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3180 - accuracy: 0.0010 - val_loss: 0.3198 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.0010 - val_loss: 0.3196 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.0010 - val_loss: 0.3194 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.0010 - val_loss: 0.3193 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.3178 - accuracy: 0.0010 - val_loss: 0.3197 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3177 - accuracy: 0.0010 - val_loss: 0.3193 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3181 - accuracy: 0.0010 - val_loss: 0.3195 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.0010 - val_loss: 0.3208 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3178 - accuracy: 0.0010 - val_loss: 0.3195 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.0010 - val_loss: 0.3188 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.0010 - val_loss: 0.3193 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3172 - accuracy: 0.0010 - val_loss: 0.3193 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def create_combined_model(input_shape, dropout_rate=0.5, l1_strength=0.01, l2_strength=0.01, elastic_strength=0.01):\n",
    "    modelo = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(l1_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(512, activation='relu', kernel_regularizer=l1(l1_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l2(l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1(l1_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)) \n",
    "    ])\n",
    "   \n",
    "    modelo.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "# Suponha que você tenha um conjunto de dados de treinamento e um conjunto de dados de validação\n",
    "# X_train e y_train são seus dados de treinamento e rótulos\n",
    "# X_val e y_val são seus dados de validação e rótulos\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "modelo = create_combined_model(input_shape=input_shape)\n",
    "\n",
    "# Treine o modelo por 50 épocas\n",
    "historico_treinamento = modelo.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teoricamente, a escolha de aplicar a regularização L1, L2 e Elastic Net (L1 e L2 juntos) em diferentes partes do modelo pode ter um impacto nas características de regularização e no comportamento do modelo durante o treinamento. Vou explicar brevemente as diferenças teóricas entre essas abordagens:\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "\n",
    "A regularização L1 adiciona um termo à função de perda que penaliza os pesos dos parâmetros com valores absolutos maiores, levando a modelos com pesos mais esparsos.\n",
    "É útil para seleção de recursos, pois tende a forçar alguns pesos a se tornarem exatamente zero.\n",
    "Pode resultar em modelos mais simples e interpretáveis.\n",
    "L2 Regularization (Ridge):\n",
    "\n",
    "A regularização L2 adiciona um termo à função de perda que penaliza os pesos dos parâmetros com valores quadráticos maiores, levando a modelos com pesos mais pequenos.\n",
    "Ajuda a evitar que os pesos dos parâmetros fiquem muito grandes, reduzindo o risco de overfitting.\n",
    "Não leva à esparsidade dos pesos como a regularização L1.\n",
    "Elastic Net (Combinação de L1 e L2):\n",
    "\n",
    "Elastic Net é uma combinação de L1 e L2, o que significa que ela inclui ambos os termos de penalização na função de perda.\n",
    "Isso combina as características de seleção de recursos da L1 com a estabilidade da L2.\n",
    "É uma escolha popular quando você deseja equilibrar a esparsidade dos pesos (L1) com a estabilidade (L2).\n",
    "A aplicação da regularização em diferentes partes do modelo pode influenciar como os pesos dos parâmetros são regularizados. No exemplo que você deu, a regularização L1 é aplicada nas primeiras camadas, a L2 nas camadas intermediárias e a Elastic Net nas camadas finais.\n",
    "\n",
    "A escolha depende do seu problema específico e da natureza dos seus dados. Experimentar diferentes combinações e intensidades de regularização é uma prática comum para encontrar a melhor configuração para um problema específico. Portanto, a diferença teórica entre essas abordagens está relacionada aos efeitos específicos que cada tipo de regularização pode ter nos pesos do modelo, mas a escolha real depende da experimentação e do ajuste fino em seus dados e problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos), em seguida crie uma função que gere um modelo baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 6ms/step - loss: 25.6088 - accuracy: 0.0010 - val_loss: 19.0812 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 14.3945 - accuracy: 0.0010 - val_loss: 9.8419 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 6.8430 - accuracy: 0.0010 - val_loss: 4.1009 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.6076 - accuracy: 0.0010 - val_loss: 1.4689 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0098 - accuracy: 0.0010 - val_loss: 0.6291 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.0010 - val_loss: 0.3371 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2792 - accuracy: 0.0010 - val_loss: 0.2341 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2012 - accuracy: 0.0010 - val_loss: 0.1766 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1580 - accuracy: 0.0010 - val_loss: 0.1473 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1400 - accuracy: 0.0010 - val_loss: 0.1393 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.0010 - val_loss: 0.1362 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.0010 - val_loss: 0.1347 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.0010 - val_loss: 0.1326 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.0010 - val_loss: 0.1305 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.0010 - val_loss: 0.1295 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1262 - accuracy: 0.0010 - val_loss: 0.1279 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.0010 - val_loss: 0.1267 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.0010 - val_loss: 0.1251 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.0010 - val_loss: 0.1241 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.0010 - val_loss: 0.1234 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.0010 - val_loss: 0.1229 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.0010 - val_loss: 0.1227 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1194 - accuracy: 0.0010 - val_loss: 0.1210 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.0010 - val_loss: 0.1205 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1184 - accuracy: 0.0010 - val_loss: 0.1199 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.0010 - val_loss: 0.1190 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.0010 - val_loss: 0.1188 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1172 - accuracy: 0.0010 - val_loss: 0.1193 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.0010 - val_loss: 0.1185 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1163 - accuracy: 0.0010 - val_loss: 0.1182 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 0.0010 - val_loss: 0.1180 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.0010 - val_loss: 0.1184 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1161 - accuracy: 0.0010 - val_loss: 0.1183 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.0010 - val_loss: 0.1177 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.0010 - val_loss: 0.1174 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.0010 - val_loss: 0.1180 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.0010 - val_loss: 0.1176 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.0010 - val_loss: 0.1174 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.0010 - val_loss: 0.1174 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.0010 - val_loss: 0.1170 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1154 - accuracy: 0.0010 - val_loss: 0.1174 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.0010 - val_loss: 0.1171 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.0010 - val_loss: 0.1171 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1152 - accuracy: 0.0010 - val_loss: 0.1174 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1156 - accuracy: 0.0010 - val_loss: 0.1169 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1150 - accuracy: 0.0010 - val_loss: 0.1171 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.0010 - val_loss: 0.1168 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1147 - accuracy: 0.0010 - val_loss: 0.1164 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.0010 - val_loss: 0.1165 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1146 - accuracy: 0.0010 - val_loss: 0.1164 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def create_regularized_model(input_shape, l1_strength=0.01, l2_strength=0.01, elastic_strength=0.01):\n",
    "    modelo = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=elastic_strength, l2=elastic_strength))\n",
    "    ])\n",
    "   \n",
    "    modelo.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "# Suponha que você tenha um conjunto de dados de treinamento e um conjunto de dados de validação\n",
    "# X_train e y_train são seus dados de treinamento e rótulos\n",
    "# X_val e y_val são seus dados de validação e rótulos\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "modelo = create_regularized_model(input_shape=input_shape)\n",
    "\n",
    "# Treine o modelo por 50 épocas\n",
    "historico_treinamento = modelo.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 6 - De acordo com as peculiaridades de aplicação da aplicação do dropout, crie uma função que gere um modelo baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 5ms/step - loss: 0.0851 - accuracy: 0.0020 - val_loss: 0.0875 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.0020 - val_loss: 0.0674 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0510 - accuracy: 0.0020 - val_loss: 0.0771 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 0.0020 - val_loss: 0.0638 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.0020 - val_loss: 0.0725 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.0020 - val_loss: 0.0688 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0400 - accuracy: 0.0020 - val_loss: 0.0603 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.0020 - val_loss: 0.0551 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.0020 - val_loss: 0.0612 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0344 - accuracy: 0.0020 - val_loss: 0.0533 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.0020 - val_loss: 0.0447 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0329 - accuracy: 0.0020 - val_loss: 0.0574 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.0020 - val_loss: 0.0507 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0318 - accuracy: 0.0020 - val_loss: 0.0520 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0313 - accuracy: 0.0020 - val_loss: 0.0483 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.0020 - val_loss: 0.0526 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0301 - accuracy: 0.0020 - val_loss: 0.0502 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.0020 - val_loss: 0.0535 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0292 - accuracy: 0.0020 - val_loss: 0.0365 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0266 - accuracy: 0.0020 - val_loss: 0.0399 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.0020 - val_loss: 0.0394 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.0020 - val_loss: 0.0439 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.0020 - val_loss: 0.0394 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.0020 - val_loss: 0.0336 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0020 - val_loss: 0.0357 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.0020 - val_loss: 0.0388 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.0020 - val_loss: 0.0270 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0020 - val_loss: 0.0278 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.0020 - val_loss: 0.0324 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.0020 - val_loss: 0.0317 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0252 - accuracy: 0.0020 - val_loss: 0.0381 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0020 - val_loss: 0.0297 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.0020 - val_loss: 0.0315 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0220 - accuracy: 0.0020 - val_loss: 0.0334 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0020 - val_loss: 0.0312 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0226 - accuracy: 0.0020 - val_loss: 0.0284 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.0020 - val_loss: 0.0306 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.0020 - val_loss: 0.0278 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0020 - val_loss: 0.0292 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.0020 - val_loss: 0.0289 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.0020 - val_loss: 0.0292 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0207 - accuracy: 0.0020 - val_loss: 0.0277 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.0020 - val_loss: 0.0262 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0199 - accuracy: 0.0020 - val_loss: 0.0303 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0020 - val_loss: 0.0255 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.0020 - val_loss: 0.0253 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0208 - accuracy: 0.0020 - val_loss: 0.0249 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.0020 - val_loss: 0.0251 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.0020 - val_loss: 0.0256 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.0020 - val_loss: 0.0261 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def create_dropout_model(input_shape, dropout_rate=0.5):\n",
    "    modelo = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "   \n",
    "    modelo.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "# Suponha que você tenha um conjunto de dados de treinamento e um conjunto de dados de validação\n",
    "# X_train e y_train são seus dados de treinamento e rótulos\n",
    "# X_val e y_val são seus dados de validação e rótulos\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "modelo = create_dropout_model(input_shape=input_shape)\n",
    "\n",
    "# Treine o modelo por 50 épocas\n",
    "historico_treinamento = modelo.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos) e da aplicação do dropout, crie uma função que gere um modelo  baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (basicamente mesclar os dois modelos anteriores), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 1s 6ms/step - loss: 25.8056 - accuracy: 0.0020 - val_loss: 19.3379 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 14.6717 - accuracy: 0.0020 - val_loss: 10.1091 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 7.0912 - accuracy: 0.0010 - val_loss: 4.3101 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.7812 - accuracy: 0.0010 - val_loss: 1.5972 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1153 - accuracy: 0.0010 - val_loss: 0.7054 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.0000e+00 - val_loss: 0.3796 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3164 - accuracy: 0.0010 - val_loss: 0.2620 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2241 - accuracy: 0.0010 - val_loss: 0.1914 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1689 - accuracy: 0.0010 - val_loss: 0.1514 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.0010 - val_loss: 0.1380 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1345 - accuracy: 0.0010 - val_loss: 0.1345 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.0010 - val_loss: 0.1318 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1298 - accuracy: 0.0010 - val_loss: 0.1298 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.0010 - val_loss: 0.1280 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1254 - accuracy: 0.0010 - val_loss: 0.1264 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1236 - accuracy: 0.0010 - val_loss: 0.1247 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1240 - accuracy: 0.0010 - val_loss: 0.1241 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1227 - accuracy: 0.0010 - val_loss: 0.1226 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.0010 - val_loss: 0.1214 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1190 - accuracy: 0.0010 - val_loss: 0.1213 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1189 - accuracy: 0.0010 - val_loss: 0.1198 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1182 - accuracy: 0.0010 - val_loss: 0.1194 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.0010 - val_loss: 0.1185 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 0.0010 - val_loss: 0.1179 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1151 - accuracy: 0.0010 - val_loss: 0.1171 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1145 - accuracy: 0.0010 - val_loss: 0.1166 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.0010 - val_loss: 0.1170 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1138 - accuracy: 0.0010 - val_loss: 0.1160 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1134 - accuracy: 0.0010 - val_loss: 0.1156 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1135 - accuracy: 0.0010 - val_loss: 0.1153 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.0010 - val_loss: 0.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.0010 - val_loss: 0.1154 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.0010 - val_loss: 0.1153 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.0010 - val_loss: 0.1149 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.0010 - val_loss: 0.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1124 - accuracy: 0.0010 - val_loss: 0.1145 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.0010 - val_loss: 0.1145 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.0010 - val_loss: 0.1145 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.0010 - val_loss: 0.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.0010 - val_loss: 0.1142 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1128 - accuracy: 0.0010 - val_loss: 0.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.0010 - val_loss: 0.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.0010 - val_loss: 0.1145 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.0010 - val_loss: 0.1149 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1130 - accuracy: 0.0010 - val_loss: 0.1152 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.0010 - val_loss: 0.1147 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.0010 - val_loss: 0.1150 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1131 - accuracy: 0.0010 - val_loss: 0.1145 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.0010 - val_loss: 0.1148 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1129 - accuracy: 0.0010 - val_loss: 0.1144 - val_accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "def create_combined_model(input_shape, dropout_rate=0.5, l1_strength=0.01, l2_strength=0.01, elastic_strength=0.01):\n",
    "    modelo = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Dense(64, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(256, activation='relu', kernel_regularizer=l1_l2(l1=l1_strength, l2=l2_strength)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(1, activation='linear', kernel_regularizer=l1_l2(l1=elastic_strength, l2=elastic_strength))\n",
    "    ])\n",
    "   \n",
    "    modelo.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    \n",
    "    return modelo\n",
    "\n",
    "# Suponha que você tenha um conjunto de dados de treinamento e um conjunto de dados de validação\n",
    "# X_train e y_train são seus dados de treinamento e rótulos\n",
    "# X_val e y_val são seus dados de validação e rótulos\n",
    "\n",
    "input_shape = X_train.shape[1:]\n",
    "modelo = create_combined_model(input_shape=input_shape)\n",
    "\n",
    "# Treine o modelo por 50 épocas\n",
    "historico_treinamento = modelo.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8 - Crie e treine por 50 épocas um modelo gerado com a função create_7_layer_model e outro com a função create_3_layer_model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 1s 4ms/step - loss: 108733224.0000 - val_loss: 32829054.0000\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 27471582.0000 - val_loss: 26296386.0000\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 22330718.0000 - val_loss: 22159852.0000\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 18924294.0000 - val_loss: 19275262.0000\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 17655316.0000 - val_loss: 18087554.0000\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 17170482.0000 - val_loss: 18866774.0000\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 16345121.0000 - val_loss: 18163534.0000\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 16598711.0000 - val_loss: 16284288.0000\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 15557096.0000 - val_loss: 15324829.0000\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 15032970.0000 - val_loss: 14950350.0000\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 14580207.0000 - val_loss: 14933684.0000\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 14604234.0000 - val_loss: 15956026.0000\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 15415519.0000 - val_loss: 13759247.0000\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 14642213.0000 - val_loss: 13650339.0000\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13783537.0000 - val_loss: 13358714.0000\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13707630.0000 - val_loss: 13164858.0000\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13569482.0000 - val_loss: 13306189.0000\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13439220.0000 - val_loss: 13813001.0000\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13590705.0000 - val_loss: 12871552.0000\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13504780.0000 - val_loss: 13696407.0000\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 14387736.0000 - val_loss: 13149295.0000\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13267460.0000 - val_loss: 14667898.0000\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 13641820.0000 - val_loss: 12639739.0000\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12571180.0000 - val_loss: 12451889.0000\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12504159.0000 - val_loss: 12673573.0000\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12712884.0000 - val_loss: 12751716.0000\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12235506.0000 - val_loss: 14156837.0000\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12513672.0000 - val_loss: 12579802.0000\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12912785.0000 - val_loss: 11699441.0000\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12181843.0000 - val_loss: 11657156.0000\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12056008.0000 - val_loss: 13365206.0000\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11886280.0000 - val_loss: 11639843.0000\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11791748.0000 - val_loss: 12317380.0000\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12427159.0000 - val_loss: 14538948.0000\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12147173.0000 - val_loss: 11761582.0000\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 12386317.0000 - val_loss: 12401033.0000\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11806249.0000 - val_loss: 11167245.0000\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11395124.0000 - val_loss: 11311525.0000\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11108559.0000 - val_loss: 11448876.0000\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11383794.0000 - val_loss: 11528947.0000\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10961432.0000 - val_loss: 12595817.0000\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10779844.0000 - val_loss: 10908516.0000\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 11203780.0000 - val_loss: 11423414.0000\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10953385.0000 - val_loss: 10876829.0000\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10549703.0000 - val_loss: 10365930.0000\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10820399.0000 - val_loss: 10326595.0000\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10461850.0000 - val_loss: 11339644.0000\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10401498.0000 - val_loss: 11287731.0000\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10715730.0000 - val_loss: 12141705.0000\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 2ms/step - loss: 10340868.0000 - val_loss: 10302669.0000\n",
      "Epoch 1/50\n",
      "50/50 [==============================] - 1s 3ms/step - loss: 233665152.0000 - val_loss: 174010976.0000\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 77000776.0000 - val_loss: 30887672.0000\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 31036534.0000 - val_loss: 28678114.0000\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 27551890.0000 - val_loss: 27310320.0000\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 24977592.0000 - val_loss: 25973424.0000\n",
      "Epoch 6/50\n",
      "50/50 [==============================] - 0s 995us/step - loss: 22828060.0000 - val_loss: 23830360.0000\n",
      "Epoch 7/50\n",
      "50/50 [==============================] - 0s 983us/step - loss: 20725246.0000 - val_loss: 22141544.0000\n",
      "Epoch 8/50\n",
      "50/50 [==============================] - 0s 996us/step - loss: 19041326.0000 - val_loss: 20501158.0000\n",
      "Epoch 9/50\n",
      "50/50 [==============================] - 0s 978us/step - loss: 18262004.0000 - val_loss: 19322248.0000\n",
      "Epoch 10/50\n",
      "50/50 [==============================] - 0s 989us/step - loss: 17926212.0000 - val_loss: 18908870.0000\n",
      "Epoch 11/50\n",
      "50/50 [==============================] - 0s 958us/step - loss: 17562174.0000 - val_loss: 18825490.0000\n",
      "Epoch 12/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 17510478.0000 - val_loss: 18614958.0000\n",
      "Epoch 13/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 17393926.0000 - val_loss: 18568190.0000\n",
      "Epoch 14/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 17102416.0000 - val_loss: 18192906.0000\n",
      "Epoch 15/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 16921496.0000 - val_loss: 18243404.0000\n",
      "Epoch 16/50\n",
      "50/50 [==============================] - 0s 982us/step - loss: 16851056.0000 - val_loss: 17870980.0000\n",
      "Epoch 17/50\n",
      "50/50 [==============================] - 0s 991us/step - loss: 16639711.0000 - val_loss: 17589904.0000\n",
      "Epoch 18/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 16423785.0000 - val_loss: 17721232.0000\n",
      "Epoch 19/50\n",
      "50/50 [==============================] - 0s 972us/step - loss: 16377960.0000 - val_loss: 17431560.0000\n",
      "Epoch 20/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 16114113.0000 - val_loss: 17401816.0000\n",
      "Epoch 21/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 16198888.0000 - val_loss: 17525650.0000\n",
      "Epoch 22/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 15943337.0000 - val_loss: 17056208.0000\n",
      "Epoch 23/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 15673592.0000 - val_loss: 16671759.0000\n",
      "Epoch 24/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 15538303.0000 - val_loss: 16327643.0000\n",
      "Epoch 25/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 15508247.0000 - val_loss: 16261129.0000\n",
      "Epoch 26/50\n",
      "50/50 [==============================] - 0s 959us/step - loss: 15444680.0000 - val_loss: 16125334.0000\n",
      "Epoch 27/50\n",
      "50/50 [==============================] - 0s 969us/step - loss: 15255594.0000 - val_loss: 16285864.0000\n",
      "Epoch 28/50\n",
      "50/50 [==============================] - 0s 988us/step - loss: 15044992.0000 - val_loss: 15936851.0000\n",
      "Epoch 29/50\n",
      "50/50 [==============================] - 0s 970us/step - loss: 14967702.0000 - val_loss: 15706605.0000\n",
      "Epoch 30/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 15155566.0000 - val_loss: 15466829.0000\n",
      "Epoch 31/50\n",
      "50/50 [==============================] - 0s 988us/step - loss: 14742066.0000 - val_loss: 15230324.0000\n",
      "Epoch 32/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 14604262.0000 - val_loss: 15131448.0000\n",
      "Epoch 33/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 14378019.0000 - val_loss: 14902340.0000\n",
      "Epoch 34/50\n",
      "50/50 [==============================] - 0s 979us/step - loss: 14241088.0000 - val_loss: 14992844.0000\n",
      "Epoch 35/50\n",
      "50/50 [==============================] - 0s 999us/step - loss: 14422936.0000 - val_loss: 14480805.0000\n",
      "Epoch 36/50\n",
      "50/50 [==============================] - 0s 987us/step - loss: 14118746.0000 - val_loss: 14604499.0000\n",
      "Epoch 37/50\n",
      "50/50 [==============================] - 0s 951us/step - loss: 14334168.0000 - val_loss: 14377924.0000\n",
      "Epoch 38/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 13897004.0000 - val_loss: 14367535.0000\n",
      "Epoch 39/50\n",
      "50/50 [==============================] - 0s 994us/step - loss: 13920548.0000 - val_loss: 13889939.0000\n",
      "Epoch 40/50\n",
      "50/50 [==============================] - 0s 999us/step - loss: 13630199.0000 - val_loss: 14207187.0000\n",
      "Epoch 41/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 13597724.0000 - val_loss: 13858705.0000\n",
      "Epoch 42/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 13653775.0000 - val_loss: 14043548.0000\n",
      "Epoch 43/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 13491647.0000 - val_loss: 13809487.0000\n",
      "Epoch 44/50\n",
      "50/50 [==============================] - 0s 991us/step - loss: 13183685.0000 - val_loss: 13607927.0000\n",
      "Epoch 45/50\n",
      "50/50 [==============================] - 0s 990us/step - loss: 13283709.0000 - val_loss: 13285271.0000\n",
      "Epoch 46/50\n",
      "50/50 [==============================] - 0s 982us/step - loss: 12984719.0000 - val_loss: 13356058.0000\n",
      "Epoch 47/50\n",
      "50/50 [==============================] - 0s 958us/step - loss: 13268323.0000 - val_loss: 13205078.0000\n",
      "Epoch 48/50\n",
      "50/50 [==============================] - 0s 983us/step - loss: 12927406.0000 - val_loss: 13509321.0000\n",
      "Epoch 49/50\n",
      "50/50 [==============================] - 0s 1ms/step - loss: 12865573.0000 - val_loss: 13263035.0000\n",
      "Epoch 50/50\n",
      "50/50 [==============================] - 0s 992us/step - loss: 12749966.0000 - val_loss: 13032682.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suponha que você tenha seus dados de entrada X e rótulos y\n",
    "# Divida seus dados em conjuntos de treinamento e validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crie o modelo com 7 camadas ocultas\n",
    "modelo_7_layers = create_7_layer_model(input_shape=X_train.shape[1])\n",
    "\n",
    "# Treine o modelo com 7 camadas ocultas por 50 épocas\n",
    "historico_treinamento_7_layers = modelo_7_layers.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n",
    "\n",
    "# Crie o modelo com 3 camadas ocultas\n",
    "modelo_3_layers = create_3_layer_model(input_shape=X_train.shape[1])\n",
    "\n",
    "# Treine o modelo com 3 camadas ocultas por 50 épocas\n",
    "historico_treinamento_3_layers = modelo_3_layers.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9 - Faça previsão com os 8 modelos nos dados de teste exibindo um gráfico com o MSE de cada um deles;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 688us/step\n",
      "7/7 [==============================] - 0s 638us/step\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2QAAAIjCAYAAABswtioAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABoJUlEQVR4nO3deXxN1/7/8fdJZJJIxJBEKuYaUnMoMU8VhFJaY4mx5RqKGjuY2kur3xYtpTqIq7RoaXspqogWMZTGPJfSklAkMUeS/fujN/vnSJDTJrYmr+fjcR7NWXudtT/7OI3ztvZe22YYhiEAAAAAwAPnZHUBAAAAAJBbEcgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAA2apz587Kly+fRowYoUuXLil//vyKj4/P9v1GRkbKZrPp5MmT2b4v/POdPHlSNptNkZGRDr82KipKNptNUVFRWV4XgJyPQAYAdzh+/Lief/55lSpVSu7u7vL29lbdunU1Y8YMXb9+3ery/lEOHDigqKgoTZw4Ud98840KFiyoZs2aKX/+/FaX5rC0L902m02ffvpphn3q1q0rm82mihUr2rUnJSVpxowZqlatmry9vZU/f3499thjeu6553To0CGzX1qIvNtj69at2XqMd7p27ZomTJjwQIPGhAkTZLPZ5OTkpNOnT6fbnpiYKA8PD9lsNg0aNOiB1QUA2SWP1QUAwMNk5cqVeuaZZ+Tm5qYePXqoYsWKSkpK0qZNmzRy5Ejt379fc+fOtbrMf4xSpUpp586deuSRRzR06FDFxsaqSJEiVpf1t7i7u2vRokV69tln7dpPnjypLVu2yN3dPd1rOnTooFWrVqlLly7q16+fbt26pUOHDmnFihWqU6eOypcvb9d/0qRJKlmyZLpxypQpk7UHcx/Xrl3TxIkTJUmNGjV6oPt2c3PTZ599plGjRtm1L1u27IHWAQDZjUAGAP9z4sQJde7cWcWLF9f69evtgsPAgQN17NgxrVy50sIKs09qaqqSkpIyDBN/h7u7ux555BFJkpOTkwIDA7N0fCu0atVK33zzjf744w8VKlTIbF+0aJH8/f316KOP6tKlS2b7jh07tGLFCv373//WSy+9ZDfWzJkzMzx9s2XLlqpRo0a2HUN2uXr1qjw9PbNkrFatWmUYyBYtWqTw8HB9+eWXWbIfALAapywCwP9MnTpVV65c0ccff5zhLE6ZMmX0wgsvmM+Tk5P12muvqXTp0nJzc1OJEiX00ksv6ebNm3avK1GihFq3bq2oqCjVqFFDHh4eqlSpknka2LJly1SpUiW5u7srJCREP//8s93re/bsKS8vL/3yyy8KCwuTp6enAgMDNWnSJBmGYdf3//7v/1SnTh0VLFhQHh4eCgkJ0RdffJHuWNJO91q4cKEee+wxubm5afXq1Q6NIUmffvqpHn/8ceXNm1e+vr5q0KCBvvvuO3P78uXL1apVKwUGBsrNzU2lS5fWa6+9ppSUlHRjLV26VCEhIfLw8FChQoX07LPP6vfff89wv3fav3+/mjRpIg8PDxUtWlSvv/66UlNTM+y7atUq1a9fX56ensqXL5/Cw8O1f//+TO1Hktq2bSs3NzctXbrUrn3RokXq2LGjnJ2d7dqPHz8u6c/TGe/k7OysggULZnrfmZGZ40v7TP3+++9q166dvLy8VLhwYY0YMcL8szl58qQKFy4sSZo4caJ52uSECRPsxjh+/LhatWqlfPnyqVu3bpL+DPjTp0/XY489Jnd3d/n7++v555+3C6r307VrV8XExNid0hkbG6v169era9euGb7m3Llz6tOnj/z9/eXu7q4qVapo/vz56frFx8erZ8+e8vHxUf78+RUREXHX6xoPHTqkp59+WgUKFJC7u7tq1Kihb775JlPHkJnPdGxsrHr16qWiRYvKzc1NRYoUUdu2bbn2EchFCGQA8D///e9/VapUKdWpUydT/fv27atx48apevXqmjZtmho2bKgpU6aoc+fO6foeO3ZMXbt2VZs2bTRlyhRdunRJbdq00cKFCzVs2DA9++yzmjhxoo4fP66OHTumCxMpKSlq0aKF/P39NXXqVIWEhGj8+PEaP368Xb+065QmTZqkyZMnK0+ePHrmmWcynNlbv369hg0bpk6dOmnGjBkqUaKEQ2NMnDhR3bt3l4uLiyZNmqSJEycqKChI69evN/t88sknypcvn4YPH67p06crJCRE48aN05gxY+zGioyMNMPMlClT1K9fPy1btkz16tW77wIgsbGxaty4sWJiYjRmzBgNHTpU//nPfzRjxox0fRcsWKDw8HB5eXnpzTff1KuvvqoDBw6oXr16mf4CnDdvXrVt21afffaZ2bZ7927t378/w6BQvHhxSdLChQuVnJycqX0kJCTojz/+sHtcuHDhvq9z5PhSUlIUFhamggUL6v/+7//UsGFDvf322+YpuYULF9bs2bMlSU899ZQWLFigBQsWqH379uYYycnJCgsLk5+fn/7v//5PHTp0kCQ9//zzGjlypHntZa9evbRw4UKFhYXp1q1bmXoPGjRooKJFi2rRokVm2+LFi+Xl5aXw8PB0/a9fv65GjRppwYIF6tatm9566y35+PioZ8+edp8FwzDUtm1bLViwQM8++6xef/11/fbbb4qIiEg35v79+1W7dm0dPHhQY8aM0dtvvy1PT0+1a9dOy5cvv2f9mf1Md+jQQcuXL1evXr30/vvva8iQIbp8+bJOnTqVqfcJQA5gAACMhIQEQ5LRtm3bTPWPiYkxJBl9+/a1ax8xYoQhyVi/fr3ZVrx4cUOSsWXLFrNtzZo1hiTDw8PD+PXXX832Dz74wJBkbNiwwWyLiIgwJBmDBw8221JTU43w8HDD1dXVOH/+vNl+7do1u3qSkpKMihUrGk2aNLFrl2Q4OTkZ+/fvT3dsmRnj6NGjhpOTk/HUU08ZKSkpdv1TU1PNn69evZpu/Oeff97ImzevcePGDXN8Pz8/o2LFisb169fNfitWrDAkGePGjUs3xu2GDh1qSDK2bdtmtp07d87w8fExJBknTpwwDMMwLl++bOTPn9/o16+f3etjY2MNHx+fdO132rBhgyHJWLp0qbFixQrDZrMZp06dMgzDMEaOHGmUKlXKMAzDaNiwofHYY4/ZvR8NGzY0JBn+/v5Gly5djFmzZtn9uaeZN2+eISnDh5ub2z3rc+T40j5TkyZNsutbrVo1IyQkxHx+/vx5Q5Ixfvz4dPtLG2PMmDF27T/++KMhyVi4cKFd++rVqzNsv9P48eMNScb58+eNESNGGGXKlDG31axZ0+jVq5dhGH9+hgcOHGhumz59uiHJ+PTTT822pKQkIzQ01PDy8jISExMNwzCMr776ypBkTJ061eyXnJxs1K9f35BkzJs3z2xv2rSpUalSJfOzahh//nnWqVPHePTRR822tM9G2v+3mf1MX7p0yZBkvPXWW/d8TwDkbMyQAYD+XLlNkvLly5ep/t9++60kafjw4XbtL774oiSlm00KDg5WaGio+bxWrVqSpCZNmqhYsWLp2n/55Zd0+7x9Rbm0Uw6TkpL0/fffm+0eHh7mz5cuXVJCQoLq16+vXbt2pRuvYcOGCg4OTteemTG++uorpaamaty4cXJysv+rxGazmT/nzZvX/Pny5cv6448/VL9+fV27ds08Fe2nn37SuXPn9K9//cvuGrbw8HCVL1/+vtftffvtt6pdu7Yef/xxs61w4cLm6XNp1q5dq/j4eHXp0sVu5snZ2Vm1atXShg0b7rmf2zVv3lwFChTQ559/LsMw9Pnnn6tLly4Z9rXZbFqzZo1ef/11+fr66rPPPtPAgQNVvHhxderUKcMZwFmzZmnt2rV2j1WrVt2zpr9yfP3797d7Xr9+/Qw/e/cyYMAAu+dLly6Vj4+PnnjiCbs6QkJC5OXl5dD73LVrVx07dkw7duww/3u30xW//fZbBQQE2P05uLi4aMiQIbpy5Yo2btxo9suTJ49d3c7Ozho8eLDdeBcvXtT69evVsWNH87ObNlMZFhamo0eP3vWU2sx+pj08POTq6qqoqCiHTucEkLOwqAcASPL29pb0Z2jIjF9//VVOTk7pVr0LCAhQ/vz59euvv9q13x66JMnHx0eSFBQUlGH7nV/OnJycVKpUKbu2smXLSpLdqWgrVqzQ66+/rpiYGLtr2W4PSWkyWsUvs2McP35cTk5OGQa62+3fv1+vvPKK1q9fb4beNAkJCZJkvlflypVL9/ry5ctr06ZN99zHr7/+agbZ29053tGjRyX9GYIzkvYZyAwXFxc988wzWrRokR5//HGdPn36rkFB+nPFwJdfflkvv/yyzp49q40bN2rGjBlasmSJXFxc0i2j//jjjzu8qIejx+fu7m5eI5bG19fXoWCQJ08eFS1aNF0dCQkJ8vPzy/A1586dy/T41apVU/ny5bVo0SLlz59fAQEBdz2+X3/9VY8++mi6fyCoUKGCuT3tv0WKFJGXl5ddvzs/L8eOHZNhGHr11Vf16quv3vVY0hatubOWjMaU7D/Tbm5uevPNN/Xiiy/K399ftWvXVuvWrdWjRw8FBARkuE8AOQ+BDAD055fVwMBA7du3z6HXZRR0MnLnQg/3azfuWKwjM3788Uc9+eSTatCggd5//30VKVJELi4umjdvnt11OGlunwn7q2PcS3x8vBo2bChvb29NmjRJpUuXlru7u3bt2qXRo0ffddGN7JK2vwULFmT4ZTdPHsf+SuzatavmzJmjCRMmqEqVKvcNp2mKFCmizp07q0OHDnrssce0ZMkSRUZGOrz/Ozl6fHf77DnCzc0tXQBKTU2Vn5+fFi5cmOFr7gyB99O1a1fNnj1b+fLlU6dOndLtL7ukvZ8jRoxQWFhYhn2y4jYEQ4cOVZs2bfTVV19pzZo1evXVVzVlyhStX79e1apV+9vjA3j4EcgA4H9at26tuXPnKjo62u70wowUL15cqampOnr0qPkv8JIUFxen+Ph4cyGHrJKamqpffvnFnBWTpCNHjkiSuRjHl19+KXd3d61Zs0Zubm5mv3nz5mV6P5kdo3Tp0kpNTdWBAwdUtWrVDMeKiorShQsXtGzZMjVo0MBsP3HihF2/tPfq8OHD6WY/Dh8+fN/3snjx4ubs0J2vvbNmSfLz81OzZs3uOWZm1KtXT8WKFVNUVJTefPNNh1/v4uKiypUr6+jRo/rjjz/+9oxIVh+flPl/cLizju+//15169bNMPQ7qmvXrho3bpzOnj2rBQsW3LVf8eLFtWfPHqWmptqFtrRTY9M+R8WLF9e6det05coVu1myOz8vaTPSLi4uDr+fjn6mS5curRdffFEvvviijh49qqpVq+rtt9++6w3IAeQsXEMGAP8zatQoeXp6qm/fvoqLi0u3/fjx4+Zqba1atZIkTZ8+3a7PO++8I0kZrgL3d82cOdP82TAMzZw5Uy4uLmratKmkP2c8bDab3ZLyJ0+e1FdffZXpfWR2jHbt2snJyUmTJk1KN9OVNruXNgNz+2xfUlKS3n//fbv+NWrUkJ+fn+bMmWN3iuSqVat08ODB+76XrVq10tatW7V9+3az7fz58+lmaMLCwuTt7a3JkydnuNLf+fPn77mfO9lsNr377rsaP368unfvftd+R48ezXDFvPj4eEVHR8vX19fhWaOMZPXxSf//GsD7rXR5u44dOyolJUWvvfZaum3JyckOjSX9GVamT5+uKVOm2F0neKdWrVopNjZWixcvttvfe++9Jy8vLzVs2NDsl5ycbK4gKf254uR7771nN56fn58aNWqkDz74QGfPnk23v3u9n5n9TF+7dk03btxId7z58uVLd/sMADkXM2QA8D+lS5fWokWL1KlTJ1WoUEE9evRQxYoVlZSUpC1btmjp0qXq2bOnJKlKlSqKiIjQ3LlzzVPztm/frvnz56tdu3Zq3Lhxltbm7u6u1atXKyIiQrVq1dKqVau0cuVKvfTSS+aX+fDwcL3zzjtq0aKFunbtqnPnzmnWrFkqU6aM9uzZk6n9ZHaMMmXK6OWXX9Zrr72m+vXrq3379nJzc9OOHTsUGBioKVOmqE6dOvL19VVERISGDBkim82mBQsWpDsd08XFRW+++aZ69eqlhg0bqkuXLoqLizOX4h82bNg9ax41apQWLFigFi1a6IUXXpCnp6fmzp1rzpik8fb21uzZs9W9e3dVr15dnTt3VuHChXXq1CmtXLlSdevWtQu9mdG2bVu1bdv2nn12796trl27qmXLlqpfv74KFCig33//XfPnz9eZM2c0ffr0dKcPrlq1yu7+W2nq1KmT7lrC7Dw+Dw8PBQcHa/HixSpbtqwKFCigihUrqmLFind9TcOGDfX8889rypQpiomJUfPmzeXi4qKjR49q6dKlmjFjhp5++mmH6rj9/n9389xzz+mDDz5Qz549tXPnTpUoUUJffPGFNm/erOnTp5sL9rRp00Z169bVmDFjdPLkSQUHB2vZsmXmNY23mzVrlurVq6dKlSqpX79+KlWqlOLi4hQdHa3ffvtNu3fvzrCWzH6mjxw5oqZNm6pjx44KDg5Wnjx5tHz5csXFxWV4+wwAOZSFKzwCwEPpyJEjRr9+/YwSJUoYrq6uRr58+Yy6desa7733nt3y17du3TImTpxolCxZ0nBxcTGCgoKMsWPH2vUxjD+XvQ8PD0+3H92xbLdhGMaJEyfSLYMdERFheHp6GsePHzeaN29u5M2b1/D39zfGjx+fbsn5jz/+2Hj00UcNNzc3o3z58sa8efPMZcTvt29HxzAMw/jkk0+MatWqmUuzN2zY0Fi7dq25ffPmzUbt2rUNDw8PIzAw0Bg1apS55P/tS/sbhmEsXrzYqFatmuHm5mYUKFDA6Natm/Hbb79lWOOd9uzZYzRs2NBwd3c3HnnkEeO1114zPv74Y7tl79Ns2LDBCAsLM3x8fAx3d3ejdOnSRs+ePY2ffvrpnvu4fdn7e7lz2fu4uDjjjTfeMBo2bGgUKVLEyJMnj+Hr62s0adLE+OKLL+xee69l73XHkuz3qvN+x5f2mbpTRn/OW7ZsMUJCQgxXV1e7JfDvNkaauXPnGiEhIYaHh4eRL18+o1KlSsaoUaOMM2fO3LP+25e9v5eMPsNxcXFGr169jEKFChmurq5GpUqVMnzPLly4YHTv3t3w9vY2fHx8jO7duxs///xzhu/x8ePHjR49ehgBAQGGi4uL8cgjjxitW7e2+7O7c9n7NPf7TP/xxx/GwIEDjfLlyxuenp6Gj4+PUatWLWPJkiX3PHYAOYvNMP7CleMAgAemZ8+e+uKLL3TlyhWrS7mrkydP6oknntD+/fvl6upqdTkAAPxjcA0ZAOBvK1GihLy8vO67RD0AALDHNWQAgL9lwoQJKlSokI4ePfpQz+IBAPAwIpABAP6W//znPzpz5owaN2581/s1AQCAjHENGQAAAABYhGvIAAAAAMAiBDIAAAAAsAjXkGWR1NRUnTlzRvny5ZPNZrO6HAAAAAAWMQxDly9fVmBgoJyc7j0HRiDLImfOnFFQUJDVZQAAAAB4SJw+fVpFixa9Zx8CWRbJly+fpD/fdG9vb4urAQAAAGCVxMREBQUFmRnhXghkWSTtNEVvb28CGQAAAIBMXcrEoh4AAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFrE0kM2ePVuVK1eWt7e3vL29FRoaqlWrVpnbGzVqJJvNZvfo37+/3RinTp1SeHi48ubNKz8/P40cOVLJycl2faKiolS9enW5ubmpTJkyioyMTFfLrFmzVKJECbm7u6tWrVravn17thwzAAAAAKSxNJAVLVpUb7zxhnbu3KmffvpJTZo0Udu2bbV//36zT79+/XT27FnzMXXqVHNbSkqKwsPDlZSUpC1btmj+/PmKjIzUuHHjzD4nTpxQeHi4GjdurJiYGA0dOlR9+/bVmjVrzD6LFy/W8OHDNX78eO3atUtVqlRRWFiYzp0792DeCAAAAAC5ks0wDMPqIm5XoEABvfXWW+rTp48aNWqkqlWravr06Rn2XbVqlVq3bq0zZ87I399fkjRnzhyNHj1a58+fl6urq0aPHq2VK1dq37595us6d+6s+Ph4rV69WpJUq1Yt1axZUzNnzpQkpaamKigoSIMHD9aYMWMyVXdiYqJ8fHyUkJAgb2/vv/EOAAAAAPgncyQbPDTXkKWkpOjzzz/X1atXFRoaarYvXLhQhQoVUsWKFTV27Fhdu3bN3BYdHa1KlSqZYUySwsLClJiYaM6yRUdHq1mzZnb7CgsLU3R0tCQpKSlJO3futOvj5OSkZs2amX0ycvPmTSUmJto9AAAAAMAReawuYO/evQoNDdWNGzfk5eWl5cuXKzg4WJLUtWtXFS9eXIGBgdqzZ49Gjx6tw4cPa9myZZKk2NhYuzAmyXweGxt7zz6JiYm6fv26Ll26pJSUlAz7HDp06K51T5kyRRMnTvx7Bw8AAAAgV7M8kJUrV04xMTFKSEjQF198oYiICG3cuFHBwcF67rnnzH6VKlVSkSJF1LRpUx0/flylS5e2sGpp7NixGj58uPk8MTFRQUFBFlYEAAAA4J/G8kDm6uqqMmXKSJJCQkK0Y8cOzZgxQx988EG6vrVq1ZIkHTt2TKVLl1ZAQEC61RDj4uIkSQEBAeZ/09pu7+Pt7S0PDw85OzvL2dk5wz5pY2TEzc1Nbm5uDh4tAAAAAPx/D801ZGlSU1N18+bNDLfFxMRIkooUKSJJCg0N1d69e+1WQ1y7dq28vb3N0x5DQ0O1bt06u3HWrl1rXqfm6uqqkJAQuz6pqalat26d3bVsAAAAAJDVLJ0hGzt2rFq2bKlixYrp8uXLWrRokaKiorRmzRodP35cixYtUqtWrVSwYEHt2bNHw4YNU4MGDVS5cmVJUvPmzRUcHKzu3btr6tSpio2N1SuvvKKBAweas1f9+/fXzJkzNWrUKPXu3Vvr16/XkiVLtHLlSrOO4cOHKyIiQjVq1NDjjz+u6dOn6+rVq+rVq5cl7wuAzLFNtFldAnIQY/xDtegwACCXsDSQnTt3Tj169NDZs2fl4+OjypUra82aNXriiSd0+vRpff/992Y4CgoKUocOHfTKK6+Yr3d2dtaKFSs0YMAAhYaGytPTUxEREZo0aZLZp2TJklq5cqWGDRumGTNmqGjRovroo48UFhZm9unUqZPOnz+vcePGKTY2VlWrVtXq1avTLfQBAAAAAFnpobsP2T8V9yEDHjxmyJCVmCEDAGSVf+R9yAAAAAAgtyGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABaxNJDNnj1blStXlre3t7y9vRUaGqpVq1aZ22/cuKGBAweqYMGC8vLyUocOHRQXF2c3xqlTpxQeHq68efPKz89PI0eOVHJysl2fqKgoVa9eXW5ubipTpowiIyPT1TJr1iyVKFFC7u7uqlWrlrZv354txwwAAAAAaSwNZEWLFtUbb7yhnTt36qefflKTJk3Utm1b7d+/X5I0bNgw/fe//9XSpUu1ceNGnTlzRu3btzdfn5KSovDwcCUlJWnLli2aP3++IiMjNW7cOLPPiRMnFB4ersaNGysmJkZDhw5V3759tWbNGrPP4sWLNXz4cI0fP167du1SlSpVFBYWpnPnzj24NwMAAABArmMzDMOwuojbFShQQG+99ZaefvppFS5cWIsWLdLTTz8tSTp06JAqVKig6Oho1a5dW6tWrVLr1q115swZ+fv7S5LmzJmj0aNH6/z583J1ddXo0aO1cuVK7du3z9xH586dFR8fr9WrV0uSatWqpZo1a2rmzJmSpNTUVAUFBWnw4MEaM2ZMhnXevHlTN2/eNJ8nJiYqKChICQkJ8vb2zpb3BoA920Sb1SUgBzHGP1R/HQIA/sESExPl4+OTqWzw0FxDlpKSos8//1xXr15VaGiodu7cqVu3bqlZs2Zmn/Lly6tYsWKKjo6WJEVHR6tSpUpmGJOksLAwJSYmmrNs0dHRdmOk9UkbIykpSTt37rTr4+TkpGbNmpl9MjJlyhT5+PiYj6CgoL//JgAAAADIVSwPZHv37pWXl5fc3NzUv39/LV++XMHBwYqNjZWrq6vy589v19/f31+xsbGSpNjYWLswlrY9bdu9+iQmJur69ev6448/lJKSkmGftDEyMnbsWCUkJJiP06dP/6XjBwAAAJB75bG6gHLlyikmJkYJCQn64osvFBERoY0bN1pd1n25ubnJzc3N6jIAAAAA/INZHshcXV1VpkwZSVJISIh27NihGTNmqFOnTkpKSlJ8fLzdLFlcXJwCAgIkSQEBAelWQ0xbhfH2PneuzBgXFydvb295eHjI2dlZzs7OGfZJGwMAAAAAsoPlpyzeKTU1VTdv3lRISIhcXFy0bt06c9vhw4d16tQphYaGSpJCQ0O1d+9eu9UQ165dK29vbwUHB5t9bh8jrU/aGK6urgoJCbHrk5qaqnXr1pl9AAAAACA7WDpDNnbsWLVs2VLFihXT5cuXtWjRIkVFRWnNmjXy8fFRnz59NHz4cBUoUEDe3t4aPHiwQkNDVbt2bUlS8+bNFRwcrO7du2vq1KmKjY3VK6+8ooEDB5qnE/bv318zZ87UqFGj1Lt3b61fv15LlizRypUrzTqGDx+uiIgI1ahRQ48//rimT5+uq1evqlevXpa8LwAAAAByB0sD2blz59SjRw+dPXtWPj4+qly5stasWaMnnnhCkjRt2jQ5OTmpQ4cOunnzpsLCwvT++++br3d2dtaKFSs0YMAAhYaGytPTUxEREZo0aZLZp2TJklq5cqWGDRumGTNmqGjRovroo48UFhZm9unUqZPOnz+vcePGKTY2VlWrVtXq1avTLfQBAAAAAFnpobsP2T+VI/caAJA1uA8ZshL3IQMAZJV/5H3IAAAAACC3IZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYJI+jLzh16pR+/fVXXbt2TYULF9Zjjz0mNze37KgNAAAAAHK0TAWykydPavbs2fr888/122+/yTAMc5urq6vq16+v5557Th06dJCTE5NuAAAAAJAZ901PQ4YMUZUqVXTixAm9/vrrOnDggBISEpSUlKTY2Fh9++23qlevnsaNG6fKlStrx44dD6JuAAAAAPjHu+8Mmaenp3755RcVLFgw3TY/Pz81adJETZo00fjx47V69WqdPn1aNWvWzJZiAQAAACAnuW8gmzJlSqYHa9Gixd8qBgAAAAByk0xd8HXu3Ll7bk9OTtb27duzpCAAAAAAyC0yFciKFCliF8oqVaqk06dPm88vXLig0NDQrK8OAAAAAHKwTAWy21dVlP5cdfHWrVv37AMAAAAAuLcsW6PeZrM5/JopU6aoZs2aypcvn/z8/NSuXTsdPnzYrk+jRo1ks9nsHv3797frc+rUKYWHhytv3rzy8/PTyJEjlZycbNcnKipK1atXl5ubm8qUKaPIyMh09cyaNUslSpSQu7u7atWqxWmYAAAAALKVpTcN27hxowYOHKitW7dq7dq1unXrlpo3b66rV6/a9evXr5/Onj1rPqZOnWpuS0lJUXh4uJKSkrRlyxbNnz9fkZGRGjdunNnnxIkTCg8PV+PGjRUTE6OhQ4eqb9++WrNmjdln8eLFGj58uMaPH69du3apSpUqCgsLu+/1cwAAAADwV9mMTJxr6OzsrCNHjqhw4cIyDENBQUHatGmTSpQoIUmKi4tT+fLllZKS8reKOX/+vPz8/LRx40Y1aNBA0p8zZFWrVtX06dMzfM2qVavUunVrnTlzRv7+/pKkOXPmaPTo0Tp//rxcXV01evRorVy5Uvv27TNf17lzZ8XHx2v16tWSpFq1aqlmzZqaOXOmJCk1NVVBQUEaPHiwxowZc9/aExMT5ePjo4SEBHl7e/+dtwFAJtkmOj4zD9yNMZ5T7wEAWcORbJDpa8jKli0rX19fFShQQFeuXFG1atXk6+srX19flStXLksKT0hIkCQVKFDArn3hwoUqVKiQKlasqLFjx+ratWvmtujoaFWqVMkMY5IUFhamxMRE7d+/3+zTrFkzuzHDwsIUHR0tSUpKStLOnTvt+jg5OalZs2ZmnzvdvHlTiYmJdg8AAAAAcMR970MmSRs2bMjuOpSamqqhQ4eqbt26qlixotnetWtXFS9eXIGBgdqzZ49Gjx6tw4cPa9myZZKk2NhYuzAmyXweGxt7zz6JiYm6fv26Ll26pJSUlAz7HDp0KMN6p0yZookTJ/69gwYAAACQq2UqkDVs2DC769DAgQO1b98+bdq0ya79ueeeM3+uVKmSihQpoqZNm+r48eMqXbp0ttd1N2PHjtXw4cPN54mJiQoKCrKsHgAAAAD/PJkKZMnJyUpJSZGbm5vZFhcXpzlz5ujq1at68sknVa9evb9cxKBBg7RixQr98MMPKlq06D371qpVS5J07NgxlS5dWgEBAelWQ4yLi5MkBQQEmP9Na7u9j7e3tzw8POTs7CxnZ+cM+6SNcSc3Nze79wMAAAAAHJWpa8j69eunIUOGmM8vX76smjVratasWVqzZo0aN26sb7/91uGdG4ahQYMGafny5Vq/fr1Klix539fExMRI+vNm1ZIUGhqqvXv32q2GuHbtWnl7eys4ONjss27dOrtx1q5da97M2tXVVSEhIXZ9UlNTtW7dOm54DQAAACDbZCqQbd68WR06dDCf/+c//1FKSoqOHj2q3bt3a/jw4Xrrrbcc3vnAgQP16aefatGiRcqXL59iY2MVGxur69evS5KOHz+u1157TTt37tTJkyf1zTffqEePHmrQoIEqV64sSWrevLmCg4PVvXt37d69W2vWrNErr7yigQMHmjNY/fv31y+//KJRo0bp0KFDev/997VkyRINGzbMrGX48OH68MMPNX/+fB08eFADBgzQ1atX1atXL4ePCwAAAAAyI1PL3nt6emrfvn3mDFb79u1VtGhRvfvuu5KkAwcOqFGjRg7fs+tuN5OeN2+eevbsqdOnT+vZZ5/Vvn37dPXqVQUFBempp57SK6+8Yrd85K+//qoBAwYoKipKnp6eioiI0BtvvKE8ef7/GZlRUVEaNmyYDhw4oKJFi+rVV19Vz5497fY7c+ZMvfXWW4qNjVXVqlX17rvvmqdI3g/L3gMPHsveIyux7D0AIKs4kg0yFcgKFiyoH3/80TwFMDAwUG+99Za6desmSfrll19UsWJFu+XocxsCGfDgEciQlQhkAICskuX3IatataoWLFggSfrxxx8VFxenJk2amNuPHz+uwMDAv1EyAAAAAOQ+mVplcdy4cWrZsqWWLFmis2fPqmfPnuaiGpK0fPly1a1bN9uKBAAAAICcKNP3Idu5c6e+++47BQQE6JlnnrHbXrVqVT3++OPZUiAAAAAA5FSZCmSSVKFCBVWoUCHDbbffvBkAAAAAkDmZCmQ//PBDpgZr0KDB3yoGAAAAAHKTTAWyRo0amUvU321RRpvNppSUlKyrDAAAAAByuEwFMl9fX+XLl089e/ZU9+7dVahQoeyuCwAAAAByvEwte3/27Fm9+eabio6OVqVKldSnTx9t2bJF3t7e8vHxMR8AAAAAgMzLVCBzdXVVp06dtGbNGh06dEiVK1fWoEGDFBQUpJdfflnJycnZXScAAAAA5DiZCmS3K1asmMaNG6fvv/9eZcuW1RtvvKHExMTsqA0AAAAAcjSHAtnNmze1aNEiNWvWTBUrVlShQoW0cuVKFShQILvqAwAAAIAcK1OLemzfvl3z5s3T559/rhIlSqhXr15asmQJQQwAAAAA/oZMBbLatWurWLFiGjJkiEJCQiRJmzZtStfvySefzNrqAAAAACAHy1Qgk6RTp07ptddeu+t27kMGAAAAAI7JVCBLTU3N7joAAAAAINdxeJVFAAAAAEDWuG8g27p1a6YHu3btmvbv3/+3CgIAAACA3OK+gax79+4KCwvT0qVLdfXq1Qz7HDhwQC+99JJKly6tnTt3ZnmRAAAAAJAT3fcasgMHDmj27Nl65ZVX1LVrV5UtW1aBgYFyd3fXpUuXdOjQIV25ckVPPfWUvvvuO1WqVOlB1A0AAAAA/3g2wzCMzHb+6aeftGnTJv3666+6fv26ChUqpGrVqqlx48a5/p5kiYmJ8vHxUUJCgry9va0uB8gVbBNtVpeAHMQYn+m/DgEAuCdHskGml72XpBo1aqhGjRp/qzgAAAAAwJ9YZREAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACzicCDbuHGj2rRpozJlyqhMmTJ68skn9eOPP2ZHbQAAAACQozkUyD799FM1a9ZMefPm1ZAhQzRkyBB5eHioadOmWrRoUXbVCAAAAAA5kkM3hq5QoYKee+45DRs2zK79nXfe0YcffqiDBw9meYH/FNwYGnjwuDE0shI3hgYAZBVHsoFDM2S//PKL2rRpk679ySef1IkTJxyrEgAAAAByOYcCWVBQkNatW5eu/fvvv1dQUFCWFQUAAAAAuUEeRzq/+OKLGjJkiGJiYlSnTh1J0ubNmxUZGakZM2ZkS4EAAAAAkFM5FMgGDBiggIAAvf3221qyZImkP68rW7x4sdq2bZstBQIAAABATuVQIJOkp556Sk899VR21AIAAAAAuQo3hgYAAAAAi9x3hqxAgQI6cuSIChUqJF9fX9lsd19m+uLFi1laHAAAAADkZPcNZNOmTVO+fPkkSdOnT8/uegAAAAAg17hvIIuIiMjwZwAAAADA33PfQJaYmJjpwe53F2oAAAAAwP9330CWP3/+e143druUlJS/XRAAAAAA5Bb3DWQbNmwwfz558qTGjBmjnj17KjQ0VJIUHR2t+fPna8qUKdlXJQAAAADkQPcNZA0bNjR/njRpkt555x116dLFbHvyySdVqVIlzZ07l2vMAAAAAMABDt2HLDo6WjVq1EjXXqNGDW3fvj3LigIAAACA3MChQBYUFKQPP/wwXftHH32koKCgLCsKAAAAAHKD+56yeLtp06apQ4cOWrVqlWrVqiVJ2r59u44ePaovv/wyWwoEAAAAgJzKoRmyVq1a6ciRI2rTpo0uXryoixcvqk2bNjpy5IhatWqVXTUCAAAAQI7k0AyZ9Odpi5MnT86OWgAAAAAgV3FohkySfvzxRz377LOqU6eOfv/9d0nSggULtGnTpiwvDgAAAABysnsGsm3btunWrVvm8y+//FJhYWHy8PDQrl27dPPmTUlSQkICs2YAAAAA4KD7BrLmzZvr8uXLkqTXX39dc+bM0YcffigXFxezX926dbVr167srRQAAAAAcph7XkM2ZMgQ3bp1Sw0bNtSuXbt0+PBhNWjQIF0/Hx8fxcfHZ1eNAAAAAJAj3XdRjxdffFGhoaGSpICAAB07dkwlSpSw67Np0yaVKlUqWwoEAAAAgJwqU4t61KlTR5LUr18/vfDCC9q2bZtsNpvOnDmjhQsXasSIERowYIDDO58yZYpq1qypfPnyyc/PT+3atdPhw4ft+ty4cUMDBw5UwYIF5eXlpQ4dOiguLs6uz6lTpxQeHq68efPKz89PI0eOVHJysl2fqKgoVa9eXW5ubipTpowiIyPT1TNr1iyVKFFC7u7uqlWrlrZv3+7wMQEAAABAZjm0yuKYMWPUtWtXNW3aVFeuXFGDBg3Ut29fPf/88xo8eLDDO9+4caMGDhyorVu3au3atbp165aaN2+uq1evmn2GDRum//73v1q6dKk2btyoM2fOqH379ub2lJQUhYeHKykpSVu2bNH8+fMVGRmpcePGmX1OnDih8PBwNW7cWDExMRo6dKj69u2rNWvWmH0WL16s4cOHa/z48dq1a5eqVKmisLAwnTt3zuHjAgAAAIDMsBmGYTj6oqSkJB07dkxXrlxRcHCwvLy8sqSY8+fPy8/PTxs3blSDBg2UkJCgwoULa9GiRXr66aclSYcOHVKFChUUHR2t2rVra9WqVWrdurXOnDkjf39/SdKcOXM0evRonT9/Xq6urho9erRWrlypffv2mfvq3Lmz4uPjtXr1aklSrVq1VLNmTc2cOVOSlJqaqqCgIA0ePFhjxoy5b+2JiYny8fFRQkKCvL29s+T9AHBvtok2q0tADmKMd/ivQwAAMuRINnD4PmSS5OrqquDgYD3++ONZFsakP5fPl6QCBQpIknbu3Klbt26pWbNmZp/y5curWLFiio6OliRFR0erUqVKZhiTpLCwMCUmJmr//v1mn9vHSOuTNkZSUpJ27txp18fJyUnNmjUz+9zp5s2bSkxMtHsAAAAAgCPuu6iHJPXu3TtTg33yySd/uZDU1FQNHTpUdevWVcWKFSVJsbGxcnV1Vf78+e36+vv7KzY21uxzexhL25627V59EhMTdf36dV26dEkpKSkZ9jl06FCG9U6ZMkUTJ078awcLAAAAAMpkIIuMjFTx4sVVrVo1/YUzHDNl4MCB2rdvnzZt2pQt42e1sWPHavjw4ebzxMREBQUFWVgRAAAAgH+aTAWyAQMG6LPPPtOJEyfUq1cvPfvss+ZphVlh0KBBWrFihX744QcVLVrUbA8ICFBSUpLi4+PtZsni4uIUEBBg9rlzNcS0VRhv73PnyoxxcXHy9vaWh4eHnJ2d5ezsnGGftDHu5ObmJjc3t792wAAAAACgTF5DNmvWLJ09e1ajRo3Sf//7XwUFBaljx45as2bN35oxMwxDgwYN0vLly7V+/XqVLFnSbntISIhcXFy0bt06s+3w4cM6deqUeW+00NBQ7d271241xLVr18rb21vBwcFmn9vHSOuTNoarq6tCQkLs+qSmpmrdunVmHwAAAADIan9plcVff/1VkZGR+s9//qPk5GTt37//Ly3u8a9//UuLFi3S119/rXLlypntPj4+8vDwkPTn7Ny3336ryMhIeXt7m8vrb9myRdKfy95XrVpVgYGBmjp1qmJjY9W9e3f17dtXkydPlvTnsvcVK1bUwIED1bt3b61fv15DhgzRypUrFRYWJunPZe8jIiL0wQcf6PHHH9f06dO1ZMkSHTp0KN21ZRlhlUXgwWOVRWQlVlkEAGQVR7JBpk5ZvJOTk5NsNpsMw1BKSspfKlKSZs+eLUlq1KiRXfu8efPUs2dPSdK0adPk5OSkDh066ObNmwoLC9P7779v9nV2dtaKFSs0YMAAhYaGytPTUxEREZo0aZLZp2TJklq5cqWGDRumGTNmqGjRovroo4/MMCZJnTp10vnz5zVu3DjFxsaqatWqWr16dabCGAAAAAD8FZmeIbt586aWLVumTz75RJs2bVLr1q3Vq1cvtWjRQk5Of2n1/ByFGTLgwWOGDFmJGTIAQFbJ8hmyf/3rX/r8888VFBSk3r1767PPPlOhQoWypFgAAAAAyK0yNUPm5OSkYsWKqVq1arLZ7v4v0suWLcvS4v5JmCEDHjxmyJCVmCEDAGSVLJ8h69Gjxz2DGAAAAADAcZm+MTQAAAAAIGuxGgcAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFgkU6ss3u748eOaPn26Dh48KEkKDg7WCy+8oNKlS2d5cQAAAACQkzk0Q7ZmzRoFBwdr+/btqly5sipXrqxt27bpscce09q1a7OrRgAAAADIkRyaIRszZoyGDRumN954I1376NGj9cQTT2RpcQAAAACQkzk0Q3bw4EH16dMnXXvv3r114MCBLCsKAAAAAHIDhwJZ4cKFFRMTk649JiZGfn5+WVUTAAAAAOQKDp2y2K9fPz333HP65ZdfVKdOHUnS5s2b9eabb2r48OHZUiAAAAAA5FQOBbJXX31V+fLl09tvv62xY8dKkgIDAzVhwgQNGTIkWwoEAAAAgJzKZhiG8VdeePnyZUlSvnz5srSgf6rExET5+PgoISFB3t7eVpcD5Aq2iTarS0AOYoz/S38dAgCQjiPZwKEZshMnTig5OVmPPvqoXRA7evSoXFxcVKJEib9UMAAAAADkRg4t6tGzZ09t2bIlXfu2bdvUs2fPrKoJAAAAAHIFhwLZzz//rLp166Zrr127doarLwIAAAAA7s6hQGaz2cxrx26XkJCglJSULCsKAAAAAHIDhwJZgwYNNGXKFLvwlZKSoilTpqhevXpZXhwAAAAA5GQOLerx5ptvqkGDBipXrpzq168vSfrxxx+VmJio9evXZ0uBAAAAAJBTOTRDFhwcrD179qhjx446d+6cLl++rB49eujQoUOqWLFidtUIAAAAADmSQzNk0p83gp48eXJ21AIAAAAAucp9A9mePXtUsWJFOTk5ac+ePffsW7ly5SwrDAAAAAByuvsGsqpVqyo2NlZ+fn6qWrWqbDabDMNI189ms7HSIgAAAAA44L6B7MSJEypcuLD5MwAAAAAga9w3kBUvXtz8+ddff1WdOnWUJ4/9y5KTk7Vlyxa7vgAAAACAe3NolcXGjRvr4sWL6doTEhLUuHHjLCsKAAAAAHIDhwKZYRiy2Wzp2i9cuCBPT88sKwoAAAAAcoNMLXvfvn17SX8u3NGzZ0+5ubmZ21JSUrRnzx7VqVMneyoEAAAAgBwqU4HMx8dH0p8zZPny5ZOHh4e5zdXVVbVr11a/fv2yp0IAAAAAyKEyFcjmzZsnSSpRooRGjBjB6YkAAAAAkAUyFcjSjB8/PrvqAAAAAIBcx6FAJklffPGFlixZolOnTikpKclu265du7KsMAAAAADI6e65yuI333yjc+fOmc/fffdd9e7dWwEBAdqxY4eaN28uLy8vnThxQq1atcr2YgEAAAAgJ7lnILtx44bq1aunI0eOSJLef/99ffTRR3r33XdlGIbeeOMN/fDDD+rfv7/i4+MfRL0AAAAAkGPcM5B17NhRc+fO1dNPPy1JOnXqlGrXri1Jcnd315UrVyRJvXv31meffZbNpQIAAABAznLfG0M3atRIGzZskCQFBATowoULkqTixYtry5YtkqTjx49nY4kAAAAAkDPdN5BJUsGCBSVJTZo00TfffCNJ6tOnjzp16qSwsDB16tTJvHk0AAAAACBzHFplce7cuUpNTZUkjRgxQkWKFNHWrVvVpk0bPf/889lSIAAAAADkVJkOZMnJyZo8ebJ69+6tokWLSpK6deumbt26ZVtxAAAAAJCTZeqURUnKkyePpk6dquTk5OysBwAAAAByjUwHMklq2rSpNm7cmF21AAAAAECu4tA1ZC1bttSYMWO0d+9ehYSEyNPT0277k08+maXFAQAAAEBOZjMMw8hsZyenu0+o2Ww2paSkZElR/0SJiYny8fFRQkKCvL29rS4HyBVsE21Wl4AcxBif6b8OAQC4J0eygUMzZGkrLAIAAAAA/j6HriEDAAAAAGSd+86Qvfvuu5kebMiQIX+rGAAAAADITe4byKZNm2b3/Pz587p27Zry588vSYqPj1fevHnl5+dHIAMAAAAAB9z3lMUTJ06Yj3//+9+qWrWqDh48qIsXL+rixYs6ePCgqlevrtdee+1B1AsAAAAAOYZDqyyWLl1aX3zxhapVq2bXvnPnTj399NM6ceJElhf4T8Eqi8CDxyqLyEqssggAyCqOZAOHFvU4e/askpOT07WnpKQoLi7OsSol/fDDD2rTpo0CAwNls9n01Vdf2W3v2bOnbDab3aNFixZ2fS5evKhu3brJ29tb+fPnV58+fXTlyhW7Pnv27FH9+vXl7u6uoKAgTZ06NV0tS5cuVfny5eXu7q5KlSrp22+/dfh4AAAAAMARDgWypk2b6vnnn9euXbvMtp07d2rAgAFq1qyZwzu/evWqqlSpolmzZt21T4sWLXT27Fnz8dlnn9lt79atm/bv36+1a9dqxYoV+uGHH/Tcc8+Z2xMTE9W8eXMVL15cO3fu1FtvvaUJEyZo7ty5Zp8tW7aoS5cu6tOnj37++We1a9dO7dq10759+xw+JgAAAADILIdOWTx//rwiIiK0evVqubi4SJKSk5MVFhamyMhI+fn5/fVCbDYtX75c7dq1M9t69uyp+Pj4dDNnaQ4ePKjg4GDt2LFDNWrUkCStXr1arVq10m+//abAwEDNnj1bL7/8smJjY+Xq6ipJGjNmjL766isdOnRIktSpUyddvXpVK1asMMeuXbu2qlatqjlz5mSqfk5ZBB48TllEVuKURQBAVsm2UxYLFy6sb7/9VocOHdLSpUu1dOlSHTx4UN9+++3fCmP3EhUVJT8/P5UrV04DBgzQhQsXzG3R0dHKnz+/GcYkqVmzZnJyctK2bdvMPg0aNDDDmCSFhYXp8OHDunTpktnnzhm+sLAwRUdH37WumzdvKjEx0e4BAAAAAI6477L3GSlbtqzKli2b1bWk06JFC7Vv314lS5bU8ePH9dJLL6lly5aKjo6Ws7OzYmNj0wXBPHnyqECBAoqNjZUkxcbGqmTJknZ9/P39zW2+vr6KjY01227vkzZGRqZMmaKJEydmxWECAAAAyKUcDmS//fabvvnmG506dUpJSUl22955550sK0ySOnfubP5cqVIlVa5cWaVLl1ZUVJSaNm2apfty1NixYzV8+HDzeWJiooKCgiysCAAAAMA/jUOBbN26dXryySdVqlQpHTp0SBUrVtTJkydlGIaqV6+eXTWaSpUqpUKFCunYsWNq2rSpAgICdO7cObs+ycnJunjxogICAiRJAQEB6VaATHt+vz5p2zPi5uYmNze3v31MAAAAAHIvh64hGzt2rEaMGKG9e/fK3d1dX375pU6fPq2GDRvqmWeeya4aTb/99psuXLigIkWKSJJCQ0MVHx+vnTt3mn3Wr1+v1NRU1apVy+zzww8/6NatW2aftWvXqly5cvL19TX7rFu3zm5fa9euVWhoaHYfEgAAAIBczKFAdvDgQfXo0UPSn9dqXb9+XV5eXpo0aZLefPNNh3d+5coVxcTEKCYmRpJ04sQJxcTE6NSpU7py5YpGjhyprVu36uTJk1q3bp3atm2rMmXKKCwsTJJUoUIFtWjRQv369dP27du1efNmDRo0SJ07d1ZgYKAkqWvXrnJ1dVWfPn20f/9+LV68WDNmzLA73fCFF17Q6tWr9fbbb+vQoUOaMGGCfvrpJw0aNMjhYwIAAACAzHIokHl6eprXjRUpUkTHjx83t/3xxx8O7/ynn35StWrVVK1aNUnS8OHDVa1aNY0bN07Ozs7as2ePnnzySZUtW1Z9+vRRSEiIfvzxR7tTBRcuXKjy5curadOmatWqlerVq2d3jzEfHx999913OnHihEJCQvTiiy9q3Lhxdvcqq1OnjhYtWqS5c+eqSpUq+uKLL/TVV1+pYsWKDh8TAAAAAGSWQ/cha9euncLDw9WvXz+NGDFCX3/9tXr27Klly5bJ19dX33//fXbW+lDjPmTAg8d9yJCVuA8ZACCrOJINHFrU45133tGVK1ckSRMnTtSVK1e0ePFiPfroo1m+wiIAAAAA5HQOBbJSpUqZP3t6emrOnDlZXhAAAAAA5BYOXUMGAAAAAMg6Ds2QOTk5yWa7+zUbKSkpf7sgAAAAAMgtHApky5cvt3t+69Yt/fzzz5o/f74mTpyYpYUBAAAAQE7nUCBr27Zturann35ajz32mBYvXqw+ffpkWWEAAAAAkNNlyTVktWvX1rp167JiKAAAAADINf52ILt+/breffddPfLII1lRDwAAAADkGg6dsujr62u3qIdhGLp8+bLy5s2rTz/9NMuLAwAAAICczKFANm3aNLtA5uTkpMKFC6tWrVry9fXN8uIAAAAAICdzKJD17Nkzm8oAAAAAgNzHoUC2Y8cOffbZZzpy5IhcXV1Vrlw59ejRQxUqVMiu+gAAAAAgx8r0oh6jRo1SrVq19NFHH+m3337TL7/8opkzZ6pSpUp68803JUk3btzQhg0bsq1YAAAAAMhJMhXI5s+fr/fee0/vvvuuLly4oJiYGMXExOjixYt65513NHHiRC1ZskQtW7bU5s2bs7tmAAAAAMgRMnXK4qxZszR58mQNGjTIrt3FxUVDhgxRcnKyunTpoqpVq2rgwIHZUigAAAAA5DSZmiHbv3+/2rZte9ft7dq1k2EYWrduHastAgAAAEAmZSqQOTs7Kykp6a7bb926JS8vL+XPnz+r6gIAAACAHC9Tgax69epauHDhXbcvWLBA1atXz7KiAAAAACA3yNQ1ZCNGjFC7du108+ZNvfjii/L395ckxcbG6u2339b06dO1bNmybC0UAAAAAHKaTAWy1q1ba9q0aRoxYoTefvtt+fj4SJISEhLk7Oyst956S23atMnWQgEAAAAgp8n0jaEHDx6sp556SkuXLtXRo0clSY8++qiefvppBQUFZVuBAAAAAJBTZTqQSVLRokU1bNiw7KoFAAAAAHKVTC3qAQAAAADIegQyAAAAALAIgQwAAAAALEIgAwAAAACLOLSoR5qdO3fq4MGDkqTg4GBuCg0AAAAAf4FDgezcuXPq3LmzoqKilD9/fklSfHy8GjdurM8//1yFCxfOjhoBAAAAIEdy6JTFwYMH6/Lly9q/f78uXryoixcvat++fUpMTNSQIUOyq0YAAAAAyJEcmiFbvXq1vv/+e1WoUMFsCw4O1qxZs9S8efMsLw4AAAAAcjKHZshSU1Pl4uKSrt3FxUWpqalZVhQAAAAA5AYOBbImTZrohRde0JkzZ8y233//XcOGDVPTpk2zvDgAAAAAyMkcCmQzZ85UYmKiSpQoodKlS6t06dIqWbKkEhMT9d5772VXjQAAAACQIzl0DVlQUJB27dql77//XocOHZIkVahQQc2aNcuW4gAAAAAgJ8t0ILt165Y8PDwUExOjJ554Qk888UR21gUAAAAAOV6mT1l0cXFRsWLFlJKSkp31AAAAAECu4dA1ZC+//LJeeuklXbx4MbvqAQAAAIBcw6FryGbOnKljx44pMDBQxYsXl6enp932Xbt2ZWlxAAAAAJCTORTI2rVrl01lAAAAAEDuk+lAlpycLJvNpt69e6to0aLZWRMAAAAA5AqZvoYsT548euutt5ScnJyd9QAAAABAruHQoh5NmjTRxo0bs6sWAAAAAMhVHLqGrGXLlhozZoz27t2rkJCQdIt6PPnkk1laHAAAAADkZDbDMIzMdnZyuvuEms1my9X3KEtMTJSPj48SEhLk7e1tdTlArmCbaLO6BOQgxvhM/3UIAMA9OZINHJohS01N/VuFAQAAAAD+P4euIQMAAAAAZJ1MBbJWrVopISHBfP7GG28oPj7efH7hwgUFBwdneXEAAAAAkJNlKpCtWbNGN2/eNJ9PnjxZFy9eNJ8nJyfr8OHDWV8dAAAAAORgmQpkd6774cA6IAAAAACAu+AaMgAAAACwSKYCmc1mk81mS9cGAAAAAPjrMrXsvWEY6tmzp9zc3CRJN27cUP/+/c0bQ99+fRkAAAAAIHMyNUMWEREhPz8/+fj4yMfHR88++6wCAwPN535+furRo4fDO//hhx/Upk0bBQYGymaz6auvvrLbbhiGxo0bpyJFisjDw0PNmjXT0aNH7fpcvHhR3bp1k7e3t/Lnz68+ffroypUrdn327Nmj+vXry93dXUFBQZo6dWq6WpYuXary5cvL3d1dlSpV0rfffuvw8QAAAACAIzI1QzZv3rxs2fnVq1dVpUoV9e7dW+3bt0+3ferUqXr33Xc1f/58lSxZUq+++qrCwsJ04MABubu7S5K6deums2fPau3atbp165Z69eql5557TosWLZL0512ymzdvrmbNmmnOnDnau3evevfurfz58+u5556TJG3ZskVdunTRlClT1Lp1ay1atEjt2rXTrl27VLFixWw5dgAAAACwGQ/Jkok2m03Lly9Xu3btJP05OxYYGKgXX3xRI0aMkCQlJCTI399fkZGR6ty5sw4ePKjg4GDt2LFDNWrUkCStXr1arVq10m+//abAwEDNnj1bL7/8smJjY+Xq6ipJGjNmjL766isdOnRIktSpUyddvXpVK1asMOupXbu2qlatqjlz5mSq/sTERPn4+CghIUHe3t5Z9bYAuAfbRK5lRdYxxj8Ufx0CAHIAR7LBQ7vK4okTJxQbG6tmzZqZbT4+PqpVq5aio6MlSdHR0cqfP78ZxiSpWbNmcnJy0rZt28w+DRo0MMOYJIWFhenw4cO6dOmS2ef2/aT1SdtPRm7evKnExES7BwAAAAA44qENZLGxsZIkf39/u3Z/f39zW2xsrPz8/Oy258mTRwUKFLDrk9EYt+/jbn3StmdkypQp5jV0Pj4+CgoKcvQQAQAAAORyD20ge9iNHTtWCQkJ5uP06dNWlwQAAADgH+ahDWQBAQGSpLi4OLv2uLg4c1tAQIDOnTtntz05OVkXL16065PRGLfv42590rZnxM3NTd7e3nYPAAAAAHDEQxvISpYsqYCAAK1bt85sS0xM1LZt2xQaGipJCg0NVXx8vHbu3Gn2Wb9+vVJTU1WrVi2zzw8//KBbt26ZfdauXaty5crJ19fX7HP7ftL6pO0HAAAAALKDpYHsypUriomJUUxMjKQ/F/KIiYnRqVOnZLPZNHToUL3++uv65ptvtHfvXvXo0UOBgYHmSowVKlRQixYt1K9fP23fvl2bN2/WoEGD1LlzZwUGBkqSunbtKldXV/Xp00f79+/X4sWLNWPGDA0fPtys44UXXtDq1av19ttv69ChQ5owYYJ++uknDRo06EG/JQAAAAByEUuXvY+KilLjxo3TtUdERCgyMlKGYWj8+PGaO3eu4uPjVa9ePb3//vsqW7as2ffixYsaNGiQ/vvf/8rJyUkdOnTQu+++Ky8vL7PPnj17NHDgQO3YsUOFChXS4MGDNXr0aLt9Ll26VK+88opOnjypRx99VFOnTlWrVq0yfSwsew88eCx7j6zEsvcAgKziSDZ4aO5D9k9HIAMePAIZshKBDACQVXLEfcgAAAAAIKcjkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBF8lhdALKHzWZ1BchpDMPqCgAAAHIeZsgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIsQyAAAAADAIgQyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiD3UgmzBhgmw2m92jfPny5vYbN25o4MCBKliwoLy8vNShQwfFxcXZjXHq1CmFh4crb9688vPz08iRI5WcnGzXJyoqStWrV5ebm5vKlCmjyMjIB3F4AAAAAHK5hzqQSdJjjz2ms2fPmo9NmzaZ24YNG6b//ve/Wrp0qTZu3KgzZ86offv25vaUlBSFh4crKSlJW7Zs0fz58xUZGalx48aZfU6cOKHw8HA1btxYMTExGjp0qPr27as1a9Y80OMEAAAAkPvksbqA+8mTJ48CAgLStSckJOjjjz/WokWL1KRJE0nSvHnzVKFCBW3dulW1a9fWd999pwMHDuj777+Xv7+/qlatqtdee02jR4/WhAkT5Orqqjlz5qhkyZJ6++23JUkVKlTQpk2bNG3aNIWFhT3QYwUAAACQuzz0M2RHjx5VYGCgSpUqpW7duunUqVOSpJ07d+rWrVtq1qyZ2bd8+fIqVqyYoqOjJUnR0dGqVKmS/P39zT5hYWFKTEzU/v37zT63j5HWJ22Mu7l586YSExPtHgAAAADgiIc6kNWqVUuRkZFavXq1Zs+erRMnTqh+/fq6fPmyYmNj5erqqvz589u9xt/fX7GxsZKk2NhYuzCWtj1t2736JCYm6vr163etbcqUKfLx8TEfQUFBf/dwAQAAAOQyD/Upiy1btjR/rly5smrVqqXixYtryZIl8vDwsLAyaezYsRo+fLj5PDExkVAGAAAAwCEP9QzZnfLnz6+yZcvq2LFjCggIUFJSkuLj4+36xMXFmdecBQQEpFt1Me35/fp4e3vfM/S5ubnJ29vb7gEAAAAAjvhHBbIrV67o+PHjKlKkiEJCQuTi4qJ169aZ2w8fPqxTp04pNDRUkhQaGqq9e/fq3LlzZp+1a9fK29tbwcHBZp/bx0jrkzYGAAAAAGSXhzqQjRgxQhs3btTJkye1ZcsWPfXUU3J2dlaXLl3k4+OjPn36aPjw4dqwYYN27typXr16KTQ0VLVr15YkNW/eXMHBwerevbt2796tNWvW6JVXXtHAgQPl5uYmSerfv79++eUXjRo1SocOHdL777+vJUuWaNiwYVYeOgAAAIBc4KG+huy3335Tly5ddOHCBRUuXFj16tXT1q1bVbhwYUnStGnT5OTkpA4dOujmzZsKCwvT+++/b77e2dlZK1as0IABAxQaGipPT09FRERo0qRJZp+SJUtq5cqVGjZsmGbMmKGiRYvqo48+Ysl7AAAAANnOZhiGYXUROUFiYqJ8fHyUkJDwUFxPZrNZXQFymofxN4VtIh90ZB1j/EP4IQcA/CM5kg0e6lMWAQAAACAnI5ABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgkTxWFwAAAIDca6JtotUlIAcZb4y3ugSHMUMGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkAEAAACARQhkAAAAAGARAhkAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAAAAFiEQAYAAAAAFiGQAQAAAIBFCGQAAAAAYBECGQAAAABYhEAGAAAAABYhkN1h1qxZKlGihNzd3VWrVi1t377d6pIAAAAA5FAEstssXrxYw4cP1/jx47Vr1y5VqVJFYWFhOnfunNWlAQAAAMiBCGS3eeedd9SvXz/16tVLwcHBmjNnjvLmzatPPvnE6tIAAAAA5EB5rC7gYZGUlKSdO3dq7NixZpuTk5OaNWum6OjodP1v3rypmzdvms8TEhIkSYmJidlfLGCBh/KjfcPqApCTPJS/v318rK4AOc3/vq88TG7wyxxZ6GH5XZ5Wh2EY9+1LIPufP/74QykpKfL397dr9/f316FDh9L1nzJliiZOnJiuPSgoKNtqBKzE90LkdD5v8CFHLsAvc+Rwb/i8YXUJdi5fviyf+/x/RyD7i8aOHavhw4ebz1NTU3Xx4kUVLFhQNpvNwsrgiMTERAUFBen06dPy9va2uhwgy/EZR07HZxy5AZ/zfx7DMHT58mUFBgbety+B7H8KFSokZ2dnxcXF2bXHxcUpICAgXX83Nze5ubnZteXPnz87S0Q28vb25hcccjQ+48jp+IwjN+Bz/s9yv5mxNCzq8T+urq4KCQnRunXrzLbU1FStW7dOoaGhFlYGAAAAIKdihuw2w4cPV0REhGrUqKHHH39c06dP19WrV9WrVy+rSwMAAACQAxHIbtOpUyedP39e48aNU2xsrKpWrarVq1enW+gDOYebm5vGjx+f7vRTIKfgM46cjs84cgM+5zmbzcjMWowAAAAAgCzHNWQAAAAAYBECGQAAAABYhEAGAAAAABYhkOEfx2az6auvvrK6DAAAAOBvI5DBYT179pTNZpPNZpOLi4tKliypUaNG6caNG1aXlq1uP+7bH8eOHbO0pnbt2lm2fzx458+f14ABA1SsWDG5ubkpICBAYWFh2rhxowoVKqQ33ngjw9e99tpr8vf3161btxQZGSmbzaYKFSqk67d06VLZbDaVKFEim48EuPfvsLlz56pRo0by9vaWzWZTfHy8Q2Pf6x/voqKi1LZtWxUpUkSenp6qWrWqFi5c6FjxgNJ/J/L399cTTzyhTz75RKmpqVaXl2klSpTQ9OnTrS4j1yKQ4S9p0aKFzp49q19++UXTpk3TBx98oPHjx1tdVrZLO+7bHyVLlvxLYyUlJWVxdcgNOnTooJ9//lnz58/XkSNH9M0336hRo0ZKSEjQs88+q3nz5qV7jWEYioyMVI8ePeTi4iJJ8vT01Llz5xQdHW3X9+OPP1axYsUeyLEA93Lt2jW1aNFCL730UpaPvWXLFlWuXFlffvml9uzZo169eqlHjx5asWJFlu8LOV/ad4OTJ09q1apVaty4sV544QW1bt1aycnJGb7m1q1bD7hKPNQMwEERERFG27Zt7drat29vVKtWzXz+xx9/GJ07dzYCAwMNDw8Po2LFisaiRYvsXtOwYUNj8ODBxsiRIw1fX1/D39/fGD9+vF2fI0eOGPXr1zfc3NyMChUqGN99950hyVi+fLnZZ8+ePUbjxo0Nd3d3o0CBAka/fv2My5cvp6v33//+t+Hn52f4+PgYEydONG7dumWMGDHC8PX1NR555BHjk08+cfi4bxcVFWXUrFnTcHV1NQICAozRo0cbt27dsjvegQMHGi+88IJRsGBBo1GjRoZhGMbevXuNFi1aGJ6enoafn5/x7LPPGufPnzdft3TpUqNixYrm8TVt2tS4cuWKMX78eEOS3WPDhg33PAb8s126dMmQZERFRWW4fc+ePYYk48cff7Rr37BhgyHJOHjwoGEYhjFv3jzDx8fHGDRokNG3b1+z3+nTpw03NzdjzJgxRvHixbPtOIA09/u9ahj///N76dIlh8a+8++K+2nVqpXRq1cvh/YB3O0zvG7dOkOS8eGHHxqG8efn8f333zfatGlj5M2b1/y+8/777xulSpUyXFxcjLJlyxr/+c9/7MZJe12LFi0Md3d3o2TJksbSpUvt+tzve1DDhg2NF154we41bdu2NSIiIsztd36fwIPFDBn+tn379mnLli1ydXU1227cuKGQkBCtXLlS+/bt03PPPafu3btr+/btdq+dP3++PD09tW3bNk2dOlWTJk3S2rVrJUmpqalq3769XF1dtW3bNs2ZM0ejR4+2e/3Vq1cVFhYmX19f7dixQ0uXLtX333+vQYMG2fVbv369zpw5ox9++EHvvPOOxo8fr9atW8vX11fbtm1T//799fzzz+u33377S+/B77//rlatWqlmzZravXu3Zs+erY8//livv/56uuN1dXXV5s2bNWfOHMXHx6tJkyaqVq2afvrpJ61evVpxcXHq2LGjJOns2bPq0qWLevfurYMHDyoqKkrt27eXYRgaMWKEOnbsaDdrV6dOnb9UP/4ZvLy85OXlpa+++ko3b95Mt71SpUqqWbOmPvnkE7v2efPmqU6dOipfvrxde+/evbVkyRJdu3ZNkhQZGakWLVrI398/+w4CeEglJCSoQIECVpeBHKJJkyaqUqWKli1bZrZNmDBBTz31lPbu3avevXtr+fLleuGFF/Tiiy9q3759ev7559WrVy9t2LDBbqxXX31VHTp00O7du9WtWzd17txZBw8elJT570H3smzZMhUtWlSTJk0yv0/gAbM6EeKfJyIiwnB2djY8PT0NNzc3Q5Lh5ORkfPHFF/d8XXh4uPHiiy+azxs2bGjUq1fPrk/NmjWN0aNHG4ZhGGvWrDHy5Mlj/P777+b2VatW2f2r59y5cw1fX1/jypUrZp+VK1caTk5ORmxsrFlv8eLFjZSUFLNPuXLljPr165vPk5OTDU9PT+Ozzz7L1HGnPZ5++mnDMAzjpZdeMsqVK2ekpqaa/WfNmmV4eXmZ+23YsKHdLKJhGMZrr71mNG/e3K7t9OnThiTj8OHDxs6dOw1JxsmTJ+9a0/3+dRk5yxdffGH4+voa7u7uRp06dYyxY8cau3fvNrfPmTPH8PLyMv91NDEx0cibN6/x0UcfmX3SZsgMwzCqVq1qzJ8/30hNTTVKly5tfP3118a0adOYIcMD8bDMkC1evNhwdXU19u3b59A+gHt9hjt16mRUqFDBMIw/P49Dhw61216nTh2jX79+dm3PPPOM0apVK/O5JKN///52fWrVqmUMGDDAMIzMfQ+63wyZYRhG8eLFjWnTpt33eJE9mCHDX9K4cWPFxMRo27ZtioiIUK9evdShQwdze0pKil577TVVqlRJBQoUkJeXl9asWaNTp07ZjVO5cmW750WKFNG5c+ckSQcPHlRQUJACAwPN7aGhoXb9Dx48qCpVqsjT09Nsq1u3rlJTU3X48GGz7bHHHpOT0///uPv7+6tSpUrmc2dnZxUsWNDc9/2OO+3x7rvvmnWEhobKZrPZ1XHlyhW7WbeQkBC78Xbv3q0NGzaYMx9eXl7mLMbx48dVpUoVNW3aVJUqVdIzzzyjDz/8UJcuXbpnjcjZOnTooDNnzuibb75RixYtFBUVperVqysyMlKS1KVLF6WkpGjJkiWSpMWLF8vJyUmdOnXKcLzevXtr3rx52rhxo65evapWrVo9qEMBHgobNmxQr1699OGHH+qxxx6zuhzkIIZh2H0vqFGjht32gwcPqm7dunZtdevWNWe/0tz53Sc0NNTsk9nvQXi4Ecjwl3h6eqpMmTKqUqWKPvnkE23btk0ff/yxuf2tt97SjBkzNHr0aG3YsEExMTEKCwtLt5BF2gIDaWw2W7asSpTRfv7KvtOOO+1RpEgRh+q4/RemJF25ckVt2rSxC3kxMTE6evSoGjRoIGdnZ61du1arVq1ScHCw3nvvPZUrV04nTpxwaL/IWdzd3fXEE0/o1Vdf1ZYtW9SzZ09zUR1vb289/fTT5uIe8+bNU8eOHeXl5ZXhWN26ddPWrVs1YcIEde/eXXny5HlgxwFYbePGjWrTpo2mTZumHj16WF0OcpiDBw/aLfx153eAB8XJyUmGYdi1sajIw4VAhr/NyclJL730kl555RVdv35dkrR582a1bdtWzz77rKpUqaJSpUrpyJEjDo1boUIFnT592u5c5q1bt6brs3v3bl29etVs27x5s5ycnFSuXLm/cVSOqVChgqKjo+1+4W3evFn58uVT0aJF7/q66tWra//+/SpRooRd0CtTpoz5i9tms6lu3bqaOHGifv75Z7m6umr58uWSJFdXV6WkpGTvweGhFxwcbPf/QJ8+fbRp0yatWLFCW7ZsUZ8+fe762gIFCujJJ5/Uxo0b1bt37wdRLvBQiIqKUnh4uN58800999xzVpeDHGb9+vXau3ev3dlDd6pQoYI2b95s17Z582YFBwfbtd353Wfr1q3mbUsy8z2ocOHCdt+lUlJStG/fPrsx+T5hLQIZssQzzzwjZ2dnzZo1S5L06KOPau3atdqyZYsOHjyo559/XnFxcQ6N2axZM5UtW1YRERHavXu3fvzxR7388st2fbp16yZ3d3dFRERo37592rBhgwYPHqzu3bs/0IUJ/vWvf+n06dMaPHiwDh06pK+//lrjx4/X8OHD7U6VvNPAgQN18eJFdenSRTt27NDx48e1Zs0a9erVSykpKdq2bZsmT56sn376SadOndKyZct0/vx58xdxiRIltGfPHh0+fFh//PEH/+KVw124cEFNmjTRp59+qj179ujEiRNaunSppk6dqrZt25r9GjRooDJlyqhHjx4qX778fRd7iYyM1B9//JFu0Q/gQUhISEh3lsDp06cVGxurmJgY816Pe/fuVUxMjC5evJjpsU+cOJFu7KtXr2rDhg0KDw/XkCFD1KFDB8XGxio2NtahsYE0N2/eVGxsrH7//Xft2rVLkydPVtu2bdW6det7zryOHDlSkZGRmj17to4ePap33nlHy5Yt04gRI+z6LV26VJ988omOHDmi8ePHa/v27eaiHZn5HtSkSROtXLlSK1eu1KFDhzRgwIB09/UrUaKEfvjhB/3+++/6448/svYNwv1ZfA0b/oHudgHrlClTjMKFCxtXrlwxLly4YLRt29bw8vIy/Pz8jFdeecXo0aOH3esyc5Hp4cOHjXr16hmurq5G2bJljdWrV//lZe9vl9G+73dBa1Yse3/nPg3jz6X9n3rqKSN//vyGh4eHUb58eWPo0KFGamqqceDAASMsLMwoXLiw4ebmZpQtW9Z47733zNeeO3fOeOKJJwwvLy+Wvc8Fbty4YYwZM8aoXr264ePjY+TNm9coV66c8corrxjXrl2z6zt58mRDkjF16tR049y+qEdGWNQDD0pERES65bYlGX369Mnw1h6SjHnz5mVq7Ixeq//dFuJu+23YsGG2Hi9ynts/S3ny5DEKFy5sNGvWzPjkk0/sFhO787tLmswsez9r1izjiSeeMNzc3IwSJUoYixcvtutzv+9BSUlJxoABA4wCBQoYfn5+xpQpU9J934qOjjYqV65sLtaGB8tmGHecVAoAAADAcjabTcuXL1e7du2sLgXZiFMWAQAAAMAiBDIAAPCPMnnyZLvbhdz+aNmypdXlAYBDOGURAAD8o1y8ePGuC3B4eHjokUceecAVAcBfRyADAAAAAItwyiIAAAAAWIRABgAAAAAWIZABAAAAgEUIZAAAAABgEQIZAAB/U1RUlGw2m+Lj4zP9mhIlSmj69OnZVhMA4J+BQAYAyPF69uwpm82m/v37p9s2cOBA2Ww29ezZ88EXBgDI9QhkAIBcISgoSJ9//rmuX79utt24cUOLFi1SsWLFLKwMAJCbEcgAALlC9erVFRQUpGXLlplty5YtU7FixVStWjWz7ebNmxoyZIj8/Pzk7u6uevXqaceOHXZjffvttypbtqw8PDzUuHFjnTx5Mt3+Nm3apPr168vDw0NBQUEaMmSIrl69etf6Tp06pbZt28rLy0ve3t7q2LGj4uLizO27d+9W48aNlS9fPnl7eyskJEQ//fTT33hHAAAPAwIZACDX6N27t+bNm2c+/+STT9SrVy+7PqNGjdKXX36p+fPna9euXSpTpozCwsJ08eJFSdLp06fVvn17tWnTRjExMerbt6/GjBljN8bx48fVokULdejQQXv27NHixYu1adMmDRo0KMO6UlNT1bZtW128eFEbN27U2rVr9csvv6hTp05mn27duqlo0aLasWOHdu7cqTFjxsjFxSWr3hoAgEVshmEYVhcBAEB26tmzp+Lj4/Xhhx8qKChIhw8fliSVL19ep0+fVt++fZU/f37NmjVLvr6+ioyMVNeuXSVJt27dUokSJTR06FCNHDlSL730kr7++mvt37/fHH/MmDF68803denSJeXPn199+/aVs7OzPvjgA7PPpk2b1LBhQ129elXu7u7mmEOHDtXatWvVsmVLnThxQkFBQZKkAwcO6LHHHtP27dtVs2ZNeXt767333lNERMQDfOcAANktj9UFAADwoBQuXFjh4eGKjIyUYRgKDw9XoUKFzO3Hjx/XrVu3VLduXbPNxcVFjz/+uA4ePChJOnjwoGrVqmU3bmhoqN3z3bt3a8+ePVq4cKHZZhiGUlNTdeLECVWoUMGu/8GDBxUUFGSGMUkKDg5W/vz5dfDgQdWsWVPDhw9X3759tWDBAjVr1kzPPPOMSpcu/fffFACApThlEQCQq/Tu3VuRkZGaP3++evfunS37uHLlip5//nnFxMSYj927d+vo0aN/OURNmDBB+/fvV3h4uNavX6/g4GAtX748iysHADxoBDIAQK7SokULJSUl6datWwoLC7PbVrp0abm6umrz5s1m261bt7Rjxw4FBwdLkipUqKDt27fbvW7r1q12z6tXr64DBw6oTJky6R6urq7paqpQoYJOnz6t06dPm20HDhxQfHy8uV9JKlu2rIYNG6bvvvtO7du3t7seDgDwz0QgAwDkKs7Ozjp48KAOHDggZ2dnu22enp4aMGCARo4cqdWrV+vAgQPq16+frl27pj59+kiS+vfvr6NHj2rkyJE6fPiwFi1apMjISLtxRo8erS1btmjQoEGKiYnR0aNH9fXXX991UY9mzZqpUqVK6tatm3bt2qXt27erR48eatiwoWrUqKHr169r0KBBioqK0q+//qrNmzdrx44d6U59BAD88xDIAAC5jre3t7y9vTPc9sYbb6hDhw7q3r27qlevrmPHjmnNmjXy9fWVJBUrVkxffvmlvvrqK1WpUkVz5szR5MmT7caoXLmyNm7cqCNHjqh+/fqqVq2axo0bp8DAwAz3abPZ9PXXX8vX11cNGjRQs2bNVKpUKS1evFjSnyHywoUL6tGjh8qWLauOHTuqZcuWmjhxYha+KwAAK7DKIgAAAABYhBkyAAAAALAIgQwAAAAALEIgAwAAAACLEMgAAAAAwCIEMgAAAACwCIEMAAAAACxCIAMAAAAAixDIAAAAAMAiBDIAAAAAsAiBDAAAAAAsQiADAAAAAIv8P8BaFkTABhnqAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gerando um conjunto de dados de exemplo para regressão\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento, validação e teste\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Criando e treinando os modelos de RandomForest e SVM para regressão\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "svm = SVR()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Função para criar o modelo de rede neural com L1 e L2\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo de rede neural com Dropout\n",
    "def create_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Criando e treinando as redes neurais\n",
    "l1_l2_model = create_l1_l2_model(X_train.shape[1])\n",
    "dropout_model = create_dropout_model(X_train.shape[1])\n",
    "\n",
    "l1_l2_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "dropout_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "# Avaliando os modelos\n",
    "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "svm_mse = mean_squared_error(y_test, svm.predict(X_test))\n",
    "l1_l2_mse = mean_squared_error(y_test, l1_l2_model.predict(X_test).reshape(-1))\n",
    "dropout_mse = mean_squared_error(y_test, dropout_model.predict(X_test).reshape(-1))\n",
    "\n",
    "# Gráfico para comparar as métricas\n",
    "labels = ['Random Forest', 'SVM', 'L1_L2', 'Dropout']\n",
    "values = [rf_mse, svm_mse, l1_l2_mse, dropout_mse]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Erro Quadrático Médio (MSE)')\n",
    "plt.title('Comparação de MSE entre Modelos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10 - Selecione o melhor modelo e exiba as métricas (da função metricas_regressao) com os valores de teste e os valores de treinamento. Comparando as métricas, o modelo apresenta características de overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
