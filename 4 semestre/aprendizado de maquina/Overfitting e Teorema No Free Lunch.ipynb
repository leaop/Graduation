{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "---\n",
    "\n",
    "## Recapitulação Rápida\n",
    "\n",
    "- **Overfitting e Underfitting**: Temas recorrentes que já estudamos.\n",
    "- **Tipos de Aprendizagem**: Classificação e Regressão.\n",
    "- **Conceitos Avançados**: Risco empírico vs. risco estrutural, dimensão VC e dilema bias-variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivos da Aula de Hoje\n",
    "\n",
    "1. **Aprofundamento em Overfitting**: Entender o que ele realmente é, como identificá-lo e como podemos prevenir ou mitigar seus efeitos.\n",
    "    - Estaremos utilizando exemplos práticos para demonstrar esses conceitos.\n",
    "  \n",
    "2. **Teorema No Free Lunch**: Explorar o que significa este teorema e quais são suas implicações para a aprendizagem de máquinas.\n",
    "    - Compreender por que não existe um algoritmo que seja o melhor para todos os problemas.\n",
    "\n",
    "---\n",
    "\n",
    "### Por que é Importante?\n",
    "\n",
    "- **Complexidade dos Modelos**: A escolha da complexidade do modelo tem um grande impacto na qualidade das previsões. \n",
    "- **Seleção de Modelos**: O Teorema No Free Lunch nos lembra que não há uma única 'bala de prata' em aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breve Recapitulação de Overfitting e Underfitting\n",
    "---\n",
    "\n",
    "## O Que é Overfitting?\n",
    "\n",
    "- **Definição**: Quando um modelo aprende o 'ruído' nos dados de treinamento a ponto de afetar negativamente o desempenho em dados não vistos.\n",
    "- **Sintomas**:\n",
    "    - Alto desempenho nos dados de treino.\n",
    "    - Baixo desempenho nos dados de teste.\n",
    "  \n",
    "---\n",
    "  \n",
    "## O Que é Underfitting?\n",
    "\n",
    "- **Definição**: Quando um modelo é demasiado simples para captar as complexidades dos dados e, por isso, apresenta desempenho ruim tanto no treino quanto no teste.\n",
    "- **Sintomas**:\n",
    "    - Baixo desempenho nos dados de treino.\n",
    "    - Baixo desempenho nos dados de teste.\n",
    "\n",
    "---\n",
    "\n",
    "## Como Detectamos Estes Fenômenos?\n",
    "\n",
    "- **Curvas de Aprendizagem**: Gráficos que mostram o desempenho do modelo em relação ao tamanho do conjunto de treinamento.\n",
    "- **Métricas de Desempenho**: Utilizando métricas como acurácia, precisão, revocação, F1-score, etc.\n",
    "  \n",
    "---\n",
    "\n",
    "**Nota**: Já abordamos esses tópicos em detalhes nas aulas anteriores. O objetivo hoje é ir além e entender como lidar com o overfitting e explorar o Teorema 'No Free Lunch'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos da Aula de Hoje\n",
    "---\n",
    "\n",
    "## Visão Geral\n",
    "\n",
    "Hoje, nosso foco será em duas áreas-chave:\n",
    "\n",
    "1. **Aprofundamento em Overfitting**: \n",
    "    - **Por que é crucial?**: O overfitting é um dos problemas mais comuns em machine learning e pode levar a resultados enganosos.\n",
    "    - **O que faremos?**: Vamos nos aprofundar na identificação, prevenção e mitigação do overfitting, utilizando exemplos práticos e técnicas específicas.\n",
    "    - **Métodos Abordados**: Regularização, Early Stopping, entre outros.\n",
    "  \n",
    "2. **Teorema No Free Lunch**:\n",
    "    - **Por que é crucial?**: Este teorema nos mostra que não existe um único algoritmo que seja ideal para todos os tipos de problemas.\n",
    "    - **O que faremos?**: Vamos entender a teoria por trás do teorema e suas implicações práticas em machine learning.\n",
    "    - **Implicações**: Escolha de algoritmos, tuning de hiperparâmetros, entre outros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprofundamento em Overfitting\n",
    "---\n",
    "\n",
    "## Como Identificar Overfitting com Métricas (3 minutos)\n",
    "\n",
    "- **Validação Cruzada**: Uma forma robusta de avaliar o desempenho do modelo em diferentes subconjuntos de dados.\n",
    "- **Conjunto de Validação**: Separe um conjunto de dados para validação durante o treinamento.\n",
    "- **Métricas de Desempenho**: Acompanhe métricas como Acurácia, F1-score, etc., em ambos os conjuntos (treino e validação).\n",
    "\n",
    "---\n",
    "\n",
    "## Regularização como uma Abordagem para Mitigar Overfitting (4 minutos)\n",
    "\n",
    "- **Definição**: Técnica que adiciona um termo de penalidade à função de custo.\n",
    "- **Tipos Comuns**:\n",
    "    1. **L1 Regularization**: Adiciona o valor absoluto dos pesos como termo de penalidade.\n",
    "    2. **L2 Regularization**: Adiciona o quadrado dos pesos como termo de penalidade.\n",
    "- **Hiperparâmetros**: O fator de regularização é um hiperparâmetro a ser ajustado.\n",
    "\n",
    "---\n",
    "\n",
    "## Exemplo: Overfitting em Redes Neurais e Técnicas para Mitigar (8 minutos)\n",
    "\n",
    "- **Dropout**: Técnica de desativar aleatoriamente alguns neurônios durante o treinamento.\n",
    "- **Early Stopping**: Monitorar o desempenho no conjunto de validação e parar o treinamento quando ele começar a degradar.\n",
    "- **Dados Adicionais**: Às vezes, simplesmente adicionando mais dados pode ajudar a mitigar o overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A função np.random.rand do NumPy gera números aleatórios uniformemente distribuídos no intervalo [0.0, 1.0]. \n",
    "- Os números são gerados a partir de uma distribuição uniforme sobre este intervalo, o que significa que cada número tem a mesma probabilidade de ser escolhido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "df = pd.read_csv('notebooks.csv')\n",
    "df_ = df.sample(frac=1).reset_index(drop=True)\n",
    "df_ = df_[1000:3000]\n",
    "X = df_.drop(columns='valor').values\n",
    "y = df_[['valor']]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento (60%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target  = MinMaxScaler()\n",
    "\n",
    "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
    "X_train = scaler_features.fit_transform(X_train)\n",
    "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
    "y_train = scaler_target.fit_transform(y_train)\n",
    "\n",
    "# Ajusta os dados de X_temp\n",
    "X_temp = scaler_features.transform(X_temp)\n",
    "# Ajusta os dados de y_temp\n",
    "y_temp = scaler_target.transform(y_temp)\n",
    "\n",
    "# Separa os dados em X e y de validação e teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo a Regularização para Combater o Overfitting\n",
    "\n",
    "## O que é Regularização?\n",
    "- **O Que é Regularização?**: Regularização é uma técnica que adiciona um termo de penalidade à função de custo.\n",
    "- **Por que Usar Regularização?**: É útil para evitar que o modelo capture ruído nos dados de treinamento, reduzindo, assim, o overfitting.\n",
    "\n",
    "\n",
    "É como um professor que te pede para explicar o porquê da sua resposta em um teste, desencorajando você de apenas decorar as respostas. No aprendizado de máquina, adicionamos um 'termo de penalidade' para desencorajar o modelo de ajustar demais aos dados de treinamento.\n",
    "\n",
    "## Tipos de Regularização\n",
    "\n",
    "### L1 (Lasso)\n",
    "\n",
    "- **Como Funciona**: Imagine que você tem um time de futebol e alguns jogadores nunca tocam na bola. A L1 remove esses jogadores do time.\n",
    "- **Quando Usar**: Use L1 quando você suspeita que muitos recursos (ou variáveis) não ajudam a prever a resposta.\n",
    "\n",
    "### L2 (Ridge)\n",
    "\n",
    "- **Como Funciona**: Em vez de demitir jogadores, o técnico (L2) diz a todos para jogarem mais devagar (menor peso).\n",
    "- **Quando Usar**: Use quando todos os recursos parecem úteis e você quer que eles contribuam igualmente.\n",
    "\n",
    "### Elastic Net\n",
    "\n",
    "- **Como Funciona**: É como combinar os técnicos de L1 e L2 para gerenciar o time.\n",
    "- **Quando Usar**: Use quando você não tem certeza de qual técnica de regularização escolher.\n",
    "\n",
    "## Onde Aplicar Regularização?\n",
    "\n",
    "- **Camadas Iniciais**: Regularizar as primeiras camadas pode ser útil se você acha que as entradas (recursos) podem conter ruídos ou informações irrelevantes.\n",
    "- **Camadas do Meio**: Podem ser regularizadas para tornar o modelo mais simples e rápido.\n",
    "- **Camadas Finais**: Evite regularizar demais para não perder as características aprendidas.\n",
    "\n",
    "## Como Escolher a Força da Regularização?\n",
    "\n",
    "- Ajuste o 'termo de penalidade'. Se for muito alto, o modelo pode ficar simples demais e perder importantes padrões nos dados (underfitting).\n",
    "- Use técnicas como validação cruzada para encontrar o melhor ajuste.\n",
    "\n",
    "## Resumo\n",
    "\n",
    "- A regularização é uma técnica poderosa para tornar seu modelo mais generalizável e menos propenso a overfitting.\n",
    "- Escolher o tipo e a intensidade da regularização pode depender do seu conhecimento específico do problema e de técnicas de ajuste de hiperparâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo o Dropout para Combater o Overfitting\n",
    "\n",
    "## O que é Dropout?\n",
    "- **O Que é Dropout?**: Dropout é uma técnica de regularização em redes neurais que \"desliga\" aleatoriamente um subconjunto de neurônios durante o treinamento.\n",
    "- **Por que Usar Dropout?**: O Dropout evita que qualquer neurônio se torne excessivamente especializado em memorizar ruídos dos dados de treinamento, o que contribui para combater o overfitting.\n",
    "\n",
    "É como um time de futebol onde alguns jogadores são aleatoriamente mandados para o banco durante o jogo para garantir que a equipe não dependa demais de um único jogador estrela. Isso torna o time como um todo mais robusto.\n",
    "\n",
    "## Como Funciona o Dropout?\n",
    "\n",
    "- **Implementação**: Durante cada iteração de treinamento, alguns neurônios são escolhidos aleatoriamente para serem \"desativados\". Isso significa que esses neurônios não participam do processo de treinamento para essa iteração específica.\n",
    "  \n",
    "- **Taxa de Dropout**: É o percentual de neurônios que você quer desativar em cada iteração. Por exemplo, uma taxa de 0.5 significa que 50% dos neurônios em uma camada são desativados.\n",
    "\n",
    "## Onde Aplicar Dropout?\n",
    "\n",
    "- **Camadas Iniciais**: Aplicar Dropout nas primeiras camadas pode ajudar se você acredita que os neurônios estão desenvolvendo dependências indesejadas nos dados de entrada. \n",
    "\n",
    "- **Camadas Ocultas Densas**: É mais comum aplicar Dropout nas camadas ocultas onde há uma alta densidade de neurônios. Isso aumenta as chances de overfitting, e o Dropout pode ajudar a mitigar isso.\n",
    "\n",
    "- **Camadas Finais**: Cuidado ao aplicar Dropout próximo à camada de saída, especialmente em tarefas que requerem alta precisão. Desativar neurônios aqui pode levar a predições imprecisas.\n",
    "\n",
    "\n",
    "## Quando Usar Dropout?\n",
    "\n",
    "- **Camadas Densas e Complexas**: Dropout é comumente usado em camadas que possuem muitos neurônios, como camadas densas.\n",
    "  \n",
    "- **Problemas com Overfitting**: Quando o modelo está muito bem ajustado aos dados de treinamento e não generaliza bem para dados novos.\n",
    "\n",
    "## Cuidados ao Usar Dropout\n",
    "\n",
    "- **Não use uma taxa muito alta**: Desativar muitos neurônios pode levar a underfitting.\n",
    "  \n",
    "- **Ajuste durante a Validação**: Sempre verifique o desempenho em um conjunto de validação para encontrar a taxa ideal.\n",
    "\n",
    "## Resumo\n",
    "\n",
    "- Dropout é uma técnica eficaz para evitar overfitting em redes neurais.\n",
    "- É como adicionar uma forma de \"incerteza\" ou \"ruído\" durante o treinamento, tornando o modelo mais robusto.\n",
    "- A escolha da taxa de dropout e onde aplicá-la são decisões cruciais que podem requerer experimentação.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções para criar os modelos\n",
    "# Função para criar o modelo base\n",
    "def create_base_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com L1 e L2 (Elastic Net)\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com Dropout\n",
    "def create_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com L1, L2 e Dropout\n",
    "def create_l1_l2_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dropout(0.2),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia com uma forma de entrada específica\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Criar, compilar e treinar os modelos\n",
    "model_base = create_base_model(input_shape)\n",
    "model_l1_l2 = create_l1_l2_model(input_shape)\n",
    "model_dropout = create_dropout_model(input_shape)\n",
    "model_l1_l2_dropout = create_l1_l2_dropout_model(input_shape)\n",
    "\n",
    "# Compilando e treinando os modelos\n",
    "optimizer1 = Adam(learning_rate=0.001)\n",
    "optimizer2 = Adam(learning_rate=0.001)\n",
    "optimizer3 = Adam(learning_rate=0.001)\n",
    "optimizer4 = Adam(learning_rate=0.001)\n",
    "\n",
    "model_base.compile(optimizer=optimizer1, loss='mse')\n",
    "history_base = model_base.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "model_l1_l2.compile(optimizer=optimizer2, loss='mse')\n",
    "history_l1_l2 = model_l1_l2.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "model_dropout.compile(optimizer=optimizer3, loss='mse')\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "model_l1_l2_dropout.compile(optimizer=optimizer4, loss='mse')\n",
    "history_l1_l2_dropout = model_l1_l2_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando os MSE para os modelos\n",
    "y_pred_base = model_base.predict(X_test)\n",
    "y_pred_l1_l2 = model_l1_l2.predict(X_test)\n",
    "y_pred_dropout = model_dropout.predict(X_test)\n",
    "y_pred_l1_l2_dropout = model_l1_l2_dropout.predict(X_test)\n",
    "\n",
    "mse_base = mean_squared_error(y_test, y_pred_base)\n",
    "mse_l1_l2 = mean_squared_error(y_test, y_pred_l1_l2)\n",
    "mse_dropout = mean_squared_error(y_test, y_pred_dropout)\n",
    "mse_l1_l2_dropout = mean_squared_error(y_test, y_pred_l1_l2_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotando os resultados\n",
    "labels = ['Base', 'L1_L2', 'Dropout', 'L1_L2_Dropout']\n",
    "mse_values = [mse_base, mse_l1_l2, mse_dropout, mse_l1_l2_dropout]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(labels, mse_values, color='skyblue')\n",
    "plt.xlabel('MSE')\n",
    "plt.ylabel('Model Type')\n",
    "plt.title('Comparação de MSE nos Modelos')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ticklabel_format(style='plain', axis='x')  # Remove notação científica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como Identificar Overfitting com Métricas\n",
    "---\n",
    "\n",
    "## Definindo o Problema\n",
    "\n",
    "- **Pergunta Principal**: Como saber se o modelo está se ajustando demais aos dados de treinamento?\n",
    "- **Consequência**: Se o modelo está com overfitting, ele terá um desempenho ruim em dados não vistos.\n",
    "\n",
    "---\n",
    "\n",
    "## Utilizando Métricas de Desempenho\n",
    "\n",
    "- **Treinamento vs Validação**: É crucial comparar as métricas de desempenho nos conjuntos de treinamento e validação.\n",
    "    1. **Acurácia**\n",
    "    2. **F1-score**\n",
    "    3. **Curva ROC-AUC**\n",
    "    \n",
    "- **Indicadores de Overfitting**:\n",
    "    - Acurácia alta no conjunto de treinamento, mas baixa no conjunto de validação.\n",
    "    - F1-score desproporcionalmente menor no conjunto de validação.\n",
    "    - Curva ROC-AUC demonstrando divergência entre treino e validação.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualizando com Gráficos\n",
    "\n",
    "- **Plot de Métricas**: Gráficos de linha para acompanhar a evolução das métricas ao longo das épocas ou iterações.\n",
    "    - Eixo X: Épocas ou Iterações\n",
    "    - Eixo Y: Valor da Métrica\n",
    "\n",
    "**Nota**: Na próxima seção, vamos olhar para um exemplo prático que inclui esses gráficos.\n",
    "\n",
    "---\n",
    "\n",
    "Pronto para ver isso na prática? Vamos mergulhar no código a seguir!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as sm\n",
    "\n",
    "def metricas_regressao(X_test, y_test, scaler_y, model):\n",
    "    \"\"\"\n",
    "    Avalia métricas de regressão para um modelo e conjunto de teste fornecidos.\n",
    "\n",
    "    Parâmetros:\n",
    "    - X_test: características do conjunto de teste.\n",
    "    - y_test: rótulos verdadeiros do conjunto de teste.\n",
    "    - scaler_y: scaler utilizado para normalizar a variável alvo.\n",
    "    - model: modelo treinado para fazer previsões.\n",
    "\n",
    "    Retorna:\n",
    "    Métricas de avaliação de regressão impressas.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Fazer previsões usando o modelo fornecido\n",
    "    predict = model.predict(X_test)\n",
    "    if scaler_y == 0:\n",
    "        real = y_test\n",
    "    else:\n",
    "    # 2. Inverter a transformação para obter os valores originais (não normalizados)\n",
    "        predict = scaler_y.inverse_transform(predict)\n",
    "        real = scaler_y.inverse_transform(y_test)\n",
    "    # 3. Calcular R2 e R2 ajustado\n",
    "    k = X_test.shape[1]  # número de características independentes\n",
    "    n = len(X_test)  # tamanho da amostra\n",
    "    r2 = sm.r2_score(real, predict)\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - k - 1)  # fórmula para R2 ajustado\n",
    "\n",
    "    # 4. Imprimir métricas\n",
    "    print('Root Mean Square Error:', round(np.sqrt(np.mean(np.array(predict) - np.array(real))**2), 2))\n",
    "    print('Mean Square Error:', round(sm.mean_squared_error(real, predict), 2))\n",
    "    print('Mean Absolut Error:', round(sm.mean_absolute_error(real, predict), 2))\n",
    "    print('Median Absolut Error:', round(sm.median_absolute_error(real, predict), 2))\n",
    "    print('Explain Variance Score:', round(sm.explained_variance_score(real, predict) * 100, 2))\n",
    "    print('R2 score:', round(sm.r2_score(real, predict) * 100, 2))\n",
    "    print('Adjusted R2 =', round(adj_r2, 3) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_regressao(X_test, y_test, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_regressao(X_train, y_train, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Erros do Modelo com 2000 Registros\n",
    "\n",
    "## Métricas de Desempenho\n",
    "\n",
    "- **Root Mean Square Error (RMSE)**\n",
    "    - Teste: 35.15\n",
    "    - Treinamento: 64.24\n",
    "- **Mean Square Error (MSE)**\n",
    "    - Teste: 8084403.99\n",
    "    - Treinamento: 4111220.34\n",
    "- **Mean Absolute Error (MAE)**\n",
    "    - Teste: 2155.01\n",
    "    - Treinamento: 1572.19\n",
    "- **Median Absolute Error**\n",
    "    - Teste: 1608.74\n",
    "    - Treinamento: 1237.49\n",
    "- **Explained Variance Score**\n",
    "    - Teste: 79.15%\n",
    "    - Treinamento: 88.91%\n",
    "- **R2 Score**\n",
    "    - Teste: 79.15%\n",
    "    - Treinamento: 88.9%\n",
    "- **Adjusted R2**\n",
    "    - Teste: 78.4%\n",
    "    - Treinamento: 88.8%\n",
    "\n",
    "## Interpretação das Métricas\n",
    "\n",
    "- **Root Mean Square Error (RMSE)**\n",
    "    - Representa a raiz quadrada da média dos erros quadráticos. Valores menores indicam melhor ajuste do modelo. \n",
    "- **Mean Square Error (MSE)**\n",
    "    - É a média dos erros quadráticos. Valores mais baixos são melhores, mas é mais sensível a outliers.\n",
    "- **Mean Absolute Error (MAE)**\n",
    "    - É a média dos erros absolutos. Fornece uma ideia de quão erradas são as previsões. \n",
    "- **Median Absolute Error**\n",
    "    - É a mediana dos erros absolutos. Menos sensível a outliers que o MAE.\n",
    "- **Explained Variance Score**\n",
    "    - Mede a proporção da variância do target que é explicada pelo modelo. Valores mais próximos de 100% são ideais.\n",
    "- **R2 Score**\n",
    "    - Mede o quanto do target é explicado pelas features. Quanto mais próximo de 100%, melhor.\n",
    "- **Adjusted R2**\n",
    "    - Semelhante ao R2, mas ajustado pelo número de preditores no modelo. É mais útil quando comparando modelos com diferentes números de preditores.\n",
    "\n",
    "## Análise de Overfitting\n",
    "\n",
    "- O modelo tem um desempenho significativamente melhor nos dados de treinamento em comparação com os dados de teste em quase todas as métricas.\n",
    "- A diferença entre o R2 Score de treinamento e teste é aproximadamente 9.75%, o que pode ser um indicador de que o modelo está sofrendo de algum grau de overfitting.\n",
    "- O RMSE para os dados de treinamento é aproximadamente 64, enquanto para os dados de teste é 35. A diferença notável também aponta para o overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "X = df.drop(columns='valor').values\n",
    "y = df[['valor']]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento (60%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target  = MinMaxScaler()\n",
    "\n",
    "# Normaliza e ajusta o escalonizador com os dados de X de treinamento\n",
    "X_train = scaler_features.fit_transform(X_train)\n",
    "# Normaliza e ajusta o escalonizador com os dados de y de treinamento\n",
    "y_train = scaler_target.fit_transform(y_train)\n",
    "\n",
    "# Ajusta os dados de X_temp\n",
    "X_temp = scaler_features.transform(X_temp)\n",
    "# Ajusta os dados de y_temp\n",
    "y_temp = scaler_target.transform(y_temp)\n",
    "\n",
    "# Separa os dados em X e y de validação e teste\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia com uma forma de entrada específica\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "# Criar, compilar e treinar o melhor modelo\n",
    "model_dropout = create_dropout_model(input_shape)\n",
    "\n",
    "# Compilando e treinando os modelos\n",
    "optimizer3 = Adam(learning_rate=0.001)\n",
    "model_dropout.compile(optimizer=optimizer3, loss='mse')\n",
    "history_dropout = model_dropout.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_val, y_val), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_regressao(X_test, y_test, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metricas_regressao(X_train, y_train, scaler_target, model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Erros do Modelo com Acréscimo de Dados\n",
    "\n",
    "## Contexto\n",
    "\n",
    "- Este modelo foi treinado com um conjunto de 10,000 registros, um aumento significativo em relação aos experimentos anteriores que tinham menos registros.\n",
    "\n",
    "## Métricas de Desempenho e Análise\n",
    "\n",
    "### Root Mean Square Error (RMSE)\n",
    "- **Teste**: 127.68\n",
    "- **Treinamento**: 157.96\n",
    "  - Representa o desvio padrão dos erros do modelo. Valores menores indicam um melhor desempenho.\n",
    "\n",
    "### Mean Square Error (MSE)\n",
    "- **Teste**: 6769172.38\n",
    "- **Treinamento**: 5058889.6\n",
    "  - É a média dos erros ao quadrado, sendo sensível a outliers. Valores menores são melhores.\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "- **Teste**: 2017.72\n",
    "- **Treinamento**: 1749.72\n",
    "  - É a média dos erros absolutos, dando uma ideia da magnitude dos erros.\n",
    "\n",
    "### Median Absolute Error\n",
    "- **Teste**: 1544.37\n",
    "- **Treinamento**: 1353.14\n",
    "  - A mediana dos erros absolutos e é menos sensível a outliers.\n",
    "\n",
    "### Explained Variance Score\n",
    "- **Teste**: 81.9%\n",
    "- **Treinamento**: 86.39%\n",
    "  - Representa quanto da variância total é explicada pelo modelo.\n",
    "\n",
    "### R2 Score\n",
    "- **Teste**: 81.86%\n",
    "- **Treinamento**: 86.32%\n",
    "  - Indica o ajuste do modelo aos dados observados.\n",
    "\n",
    "### Adjusted R2\n",
    "- **Teste**: 81.7%\n",
    "- **Treinamento**: 86.3%\n",
    "  - É o R2 ajustado pelo número de preditores no modelo.\n",
    "\n",
    "## Efeito do Acréscimo de Dados\n",
    "\n",
    "- O acréscimo de mais dados no treinamento parece ter ajudado o modelo a generalizar melhor, como evidenciado pelas métricas de teste e treinamento mais próximas.\n",
    "- A diferença no R2 Score entre treinamento e teste diminuiu, sugerindo que o modelo está menos propenso a overfitting.\n",
    "- A inclusão de mais dados pode ter contribuído para uma representação mais abrangente do espaço de características, tornando o modelo mais robusto a variações nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Teorema No Free Lunch\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "- **Definição**: O Teorema No Free Lunch (NFL) afirma que não existe um único algoritmo de aprendizado de máquina que funcione melhor para todos os tipos de problemas.\n",
    "- **Importância**: Esse teorema nos ajuda a entender por que a busca pelo \"algoritmo perfeito\" é fútil.\n",
    "\n",
    "---\n",
    "\n",
    "## O Que o Teorema Realmente Significa?\n",
    "\n",
    "1. **Não Existe Algoritmo Universalmente Superior**: Cada algoritmo tem seus próprios pontos fortes e fracos, e o que funciona bem para um problema pode não ser adequado para outro.\n",
    "2. **Dependência do Problema**: O sucesso de um algoritmo é fortemente dependente do tipo de problema que você está tentando resolver.\n",
    "3. **A Importância da Experimentação**: Este teorema reforça a necessidade de experimentar com vários algoritmos e técnicas para encontrar a melhor abordagem para um determinado problema.\n",
    "\n",
    "---\n",
    "\n",
    "## Implicações Práticas\n",
    "\n",
    "- **Seleção de Modelos**: Dada a impossibilidade de um único melhor algoritmo, a seleção de modelos torna-se crucial.\n",
    "- **Otimização de Hiperparâmetros**: O ajuste de hiperparâmetros é mais relevante do que nunca, já que o \"melhor\" algoritmo é problema-específico.\n",
    "\n",
    "---\n",
    "\n",
    "**Exemplos práticos que ilustram o Teorema No Free Lunch em ação.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Gerando um conjunto de dados de exemplo para regressão\n",
    "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento, validação e teste\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Criando e treinando os modelos de RandomForest e SVM para regressão\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "svm = SVR()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Função para criar o modelo de rede neural com L1 e L2\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo de rede neural com Dropout\n",
    "def create_dropout_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(8, activation='relu'),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Criando e treinando as redes neurais\n",
    "l1_l2_model = create_l1_l2_model(X_train.shape[1])\n",
    "dropout_model = create_dropout_model(X_train.shape[1])\n",
    "\n",
    "l1_l2_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "dropout_model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "\n",
    "# Avaliando os modelos\n",
    "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
    "svm_mse = mean_squared_error(y_test, svm.predict(X_test))\n",
    "l1_l2_mse = mean_squared_error(y_test, l1_l2_model.predict(X_test).reshape(-1))\n",
    "dropout_mse = mean_squared_error(y_test, dropout_model.predict(X_test).reshape(-1))\n",
    "\n",
    "# Gráfico para comparar as métricas\n",
    "labels = ['Random Forest', 'SVM', 'L1_L2', 'Dropout']\n",
    "values = [rf_mse, svm_mse, l1_l2_mse, dropout_mse]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(labels, values, color=['blue', 'green', 'red', 'purple'])\n",
    "plt.xlabel('Modelos')\n",
    "plt.ylabel('Erro Quadrático Médio (MSE)')\n",
    "plt.title('Comparação de MSE entre Modelos')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Significado e Importância\n",
    "---\n",
    "\n",
    "## O Que o Teorema No Free Lunch Realmente Significa? \n",
    "\n",
    "- **Sem Almoço Grátis**: O nome sugere que não existe uma \"refeição gratuita\" em termos de eficácia algorítmica; tudo tem um custo.\n",
    "- **Universalidade**: Não existe um único algoritmo que seja superior em todos os cenários e tipos de dados.\n",
    "  \n",
    "---\n",
    "\n",
    "## Por Que Isso é Importante? \n",
    "\n",
    "### Fim da Busca pelo \"Algoritmo de Aprendizado Perfeito\"\n",
    "\n",
    "- Elimina a noção de que poderia existir um \"Santo Graal\" dos algoritmos de aprendizado de máquina.\n",
    "  \n",
    "### Acentua a Necessidade de Personalização\n",
    "\n",
    "- Destaca que a escolha do algoritmo deve ser adaptada ao problema específico em mãos.\n",
    "\n",
    "---\n",
    "\n",
    "**Para resumir, o Teorema No Free Lunch nos ensina a ser mais críticos e adaptáveis como cientistas de dados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Como o Teorema No Free Lunch se Relaciona com Overfitting\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "- **Ponto em Comum**: Tanto o Teorema No Free Lunch quanto o conceito de overfitting nos dizem que não há soluções universais no campo do aprendizado de máquina.\n",
    "\n",
    "---\n",
    "\n",
    "## Relação entre NFL e Overfitting\n",
    "\n",
    "1. **Escolha de Algoritmos**: O Teorema No Free Lunch sugere que devemos escolher algoritmos com base no problema específico, algo que também é crucial para evitar o overfitting.\n",
    "\n",
    "2. **Complexidade do Modelo**: \n",
    "    - NFL nos adverte contra a busca por um \"algoritmo perfeito\".\n",
    "    - Overfitting nos adverte contra a busca por um \"modelo perfeitamente ajustado\".\n",
    "    - Ambos são contra a ideia de \"um tamanho serve para todos\".\n",
    "\n",
    "3. **Personalização e Ajuste**:\n",
    "    - NFL destaca a necessidade de ajuste e personalização nos algoritmos.\n",
    "    - Overfitting nos mostra que esse ajuste precisa ser feito com cuidado para evitar memorização em vez de generalização.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "- A consciência do Teorema No Free Lunch pode nos ajudar a ser mais criteriosos na prevenção de overfitting, escolhendo e ajustando algoritmos de forma mais eficaz.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicações Práticas para a Seleção de Modelos\n",
    "---\n",
    "\n",
    "## Introdução\n",
    "\n",
    "- **Contexto**: Agora que entendemos os conceitos de overfitting e o Teorema No Free Lunch, como aplicamos esse conhecimento na prática?\n",
    "\n",
    "---\n",
    "\n",
    "## Passos na Seleção de Modelos\n",
    "\n",
    "1. **Análise do Problema**:\n",
    "    - Entender o tipo de problema (Classificação, Regressão, Agrupamento, etc.) é o primeiro passo na seleção do modelo.\n",
    "\n",
    "2. **Teste de Vários Modelos**:\n",
    "    - Devido ao Teorema No Free Lunch, é recomendável testar diversos algoritmos para encontrar o que se adequa melhor ao problema específico.\n",
    "\n",
    "3. **Validação Cruzada**:\n",
    "    - Usar técnicas como K-Fold para estimar o desempenho do modelo em dados não vistos e ajudar a evitar overfitting.\n",
    "\n",
    "4. **Ajuste de Hiperparâmetros**:\n",
    "    - Com o modelo escolhido, o ajuste de hiperparâmetros torna-se crucial tanto para o desempenho quanto para evitar o overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Métricas e Diagnósticos\n",
    "\n",
    "- **Selecionar Métricas Relevantes**: Acurácia, Precisão, Recall, F1-Score, entre outros, dependendo do problema.\n",
    "- **Curvas ROC e AUC**: Ferramentas úteis para avaliar a qualidade do modelo em problemas de classificação.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusão\n",
    "\n",
    "- A integração desses conceitos (NFL e overfitting) nos dá uma abordagem mais robusta e informada para a seleção e otimização de modelos em aprendizado de máquina.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercícios\n",
    "\n",
    "Exemplos de aplicação de l1, l2 e l1 e l2 juntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importando as bibliotecas necessárias para o modelo de rede neural\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "\n",
    "# Função para criar o modelo com regularização L1\n",
    "def create_l1_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l1(0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com regularização L2\n",
    "def create_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar o modelo com regularização L1 e L2\n",
    "def create_l1_l2_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(8, activation='relu', kernel_regularizer=l1_l2(l1=0.01, l2=0.01)),\n",
    "        Dense(1, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura modelo base 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Função para criar um modelo com 7 camadas ocultas\n",
    "def create_7_layer_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(512, activation='relu', input_shape=(input_shape,)), # Camada de entrada\n",
    "        Dense(256, activation='relu'),  # Primeira camada oculta\n",
    "        Dense(128, activation='relu'),  # Segunda camada oculta\n",
    "        Dense(64, activation='relu'),   # Terceira camada oculta\n",
    "        Dense(32, activation='relu'),   # Quarta camada oculta\n",
    "        Dense(16, activation='relu'),   # Quinta camada oculta\n",
    "        Dense(8, activation='relu'),    # Sexta camada oculta\n",
    "        Dense(4, activation='relu'),    # Sétima camada oculta\n",
    "        Dense(1, activation='linear')   # Camada de saída\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Função para criar um modelo com 3 camadas ocultas\n",
    "def create_3_layer_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),  # Camada de entrada\n",
    "        Dense(32, activation='relu'),  # Primeira camada oculta\n",
    "        Dense(16, activation='relu'),  # Segunda camada oculta\n",
    "        Dense(8, activation='relu'),   # Terceira camada oculta\n",
    "        Dense(1, activation='linear')  # Camada de saída\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrutura modelo base 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),  # Camada de entrada\n",
    "        Dense(32, activation='relu'),  # Primeira camada oculta\n",
    "        Dense(16, activation='relu'),  # Segunda camada oculta\n",
    "        Dense(8, activation='relu'),   # Terceira camada oculta\n",
    "        Dense(1, activation='linear')  # Camada de saída\n",
    "    ])\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilize a base de dados notebooks_ruidosos.csv que contém 3 colunas com valores aleatórios que devem atrapalhar o aprendizado do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1 - Separe os dados em X_train, X_temp, y_train, y_temp, com 50% para o treinamento normalize com fit transform os dados de treinamento (um normalizador para X e outro para y), normalize os dados temporários com o transform e em seguida separe X e y temp em X e y de validação e teste, 50% para cada um. (a coluna valor será o y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos), em seguida crie uma função que gere um modelo baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3 - De acordo com as peculiaridades de aplicação da aplicação do dropout, crie uma função que gere um modelo baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos) e da aplicação do dropout, crie uma função que gere um modelo  baseado na estrutura do modelo de 7 camadas ocultas que melhor se adeque para a base de dados (basicamente mesclar os dois modelos anteriores), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos), em seguida crie uma função que gere um modelo baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5 - De acordo com as peculiaridades de aplicação da aplicação do dropout, crie uma função que gere um modelo baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (de acordo com a teoria), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7 - De acordo com as peculiaridades de aplicação da regularização de kernel L1, L2 e Elastic Net (L1 e L2 Juntos) e da aplicação do dropout, crie uma função que gere um modelo  baseado na estrutura do modelo de 3 camadas ocultas que melhor se adeque para a base de dados (basicamente mesclar os dois modelos anteriores), em seguida crie e treine por 50 épocas um modelo gerado com esta função;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 8 - Crie e treine por 50 épocas um modelo gerado com a função create_7_layer_model e outro com a função create_3_layer_model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9 - Faça previsão com os 8 modelos nos dados de teste exibindo um gráfico com o MSE de cada um deles;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 10 - Selecione o melhor modelo e exiba as métricas (da função metricas_regressao) com os valores de teste e os valores de treinamento. Comparando as métricas, o modelo apresenta características de overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
